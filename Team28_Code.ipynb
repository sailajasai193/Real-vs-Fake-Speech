{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cMlLFZZHa3H2",
        "H6gVMTOy1Gyi",
        "CP-RjGU33ZK_",
        "Tw7EWqmCa42-",
        "wqgC-myf6Cjt",
        "LXsfZq4e2QNm"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Loading modules"
      ],
      "metadata": {
        "id": "cMlLFZZHa3H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve\n",
        "from scipy.optimize import brentq\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "DdZuO7Mx00Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the CSV"
      ],
      "metadata": {
        "id": "H6gVMTOy1Gyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using google colab, importing CSV"
      ],
      "metadata": {
        "id": "jJoJ7whP1pQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Using google colab\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# train_data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "# dev_data = pd.read_csv('/content/drive/MyDrive/dev.csv')\n",
        "# eval_data = pd.read_csv('/content/drive/MyDrive/eval.csv')\n",
        "\n",
        "# #checking for Duplicates\n",
        "# unique_row_count_train = len(train_data.drop_duplicates())\n",
        "# unique_row_count_val = len(dev_data.drop_duplicates())\n",
        "# unique_row_count_test = len(eval_data.drop_duplicates())\n",
        "\n",
        "# print(\"Count of unique rows:\", unique_row_count_train)\n",
        "# print(\"Count of unique rows:\", unique_row_count_val)\n",
        "# print(\"Count of unique rows:\", unique_row_count_test)\n",
        "\n",
        "# # Actual length of Data frame\n",
        "# row_count_train = len(train_data)\n",
        "# row_count_val = len(dev_data)\n",
        "# row_count_test = len(eval_data)\n",
        "\n",
        "# print('Row Count',row_count_train)\n",
        "# print('Row Count',row_count_val)\n",
        "# print('Row Count',row_count_test)\n",
        "\n",
        "# X_train = train_data.iloc[:, 2:].values\n",
        "# y_train = train_data['label'].values\n",
        "\n",
        "# X_val = dev_data.iloc[:, 2:].values\n",
        "# y_val = dev_data['label'].values\n",
        "\n",
        "# X_test = eval_data.iloc[:, 2:].values\n",
        "# y_test = eval_data['label'].values\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_val_scaled = scaler.transform(X_val)\n",
        "# X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "yhLnlec61L2p"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating DataFrames with features"
      ],
      "metadata": {
        "id": "tk6G98Qx1j5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####### Creating DataFrames with features labeled from Feature_0 to Feature_1023 ######\n",
        "\n",
        "# X_val_df = pd.DataFrame(X_val, columns=[f'{i}' for i in range(1024)])\n",
        "# X_test_df = pd.DataFrame(X_test, columns=[f'{i}' for i in range(1024)])\n",
        "\n",
        "# y_val_df = pd.DataFrame(y_val, columns=['label'])\n",
        "# y_test_df = pd.DataFrame(y_test, columns=['label'])\n",
        "\n",
        "# label_distribution_train = train_data['label'].value_counts()\n",
        "# print(\"Distribution of data for the label attribute in train data set:\")\n",
        "# print(label_distribution_train)\n",
        "# label_percentage_train = (label_distribution_train / len(train_data)) * 100\n",
        "# print(\"Percentage distribution of data for the label attribute in train data set:\")\n",
        "# print(label_percentage_train, '\\n\\n\\n')\n",
        "\n",
        "# label_distribution_val = dev_data['label'].value_counts()\n",
        "# print(\"Distribution of data for the label attribute in validation data set:\")\n",
        "# print(label_distribution_val)\n",
        "# label_percentage_val = (label_distribution_val / len(dev_data)) * 100\n",
        "# print(\"Percentage distribution of data for the label attribute in val data set:\")\n",
        "# print(label_percentage_val, '\\n\\n\\n')\n",
        "\n",
        "# label_distribution_test = eval_data['label'].value_counts()\n",
        "# print(\"Distribution of data for the label attribute in test data set:\")\n",
        "# print(label_distribution_test)\n",
        "# label_percentage_test = (label_distribution_test / len(eval_data)) * 100\n",
        "# print(\"Percentage distribution of data for the label attribute in test data set:\")\n",
        "# print(label_percentage_test, '\\n\\n\\n')"
      ],
      "metadata": {
        "id": "vgwUQkW21emj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sampling Data"
      ],
      "metadata": {
        "id": "04RCp4-T12hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 1a. Random Oversampling\n",
        "# ros = RandomOverSampler(random_state=42)\n",
        "# X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "# # 1a. SMOTE Oversampling\n",
        "# smote = SMOTE(random_state=42)\n",
        "# X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# # 1a. ADASYN Oversampling\n",
        "# adasyn = ADASYN(random_state=42)\n",
        "# X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)\n",
        "\n",
        "# # 1b. Random Undersampling\n",
        "# rus = RandomUnderSampler(random_state=42)\n",
        "# X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "SrTKJTo111xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Visualisation"
      ],
      "metadata": {
        "id": "CP-RjGU33ZK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming you have train_data, dev_data, and eval_data DataFrames\n",
        "# with columns labeled from 0 to 1023 and a column named 'label'\n",
        "\n",
        "# Function to calculate central tendency and dispersion for each feature\n",
        "def calculate_stats(data):\n",
        "    stats = {}\n",
        "    for feature in range(1024):\n",
        "        feature_name = str(feature)\n",
        "        min_val = data[feature_name].min()\n",
        "        max_val = data[feature_name].max()\n",
        "        range_val = max_val - min_val\n",
        "        quartiles = data[feature_name].quantile([0.25, 0.5, 0.75]).to_dict()\n",
        "        iqr = quartiles[0.75] - quartiles[0.25]\n",
        "        five_number_summary = {\n",
        "            'min': min_val,\n",
        "            'Q1': quartiles[0.25],\n",
        "            'median': quartiles[0.5],\n",
        "            'Q3': quartiles[0.75],\n",
        "            'max': max_val\n",
        "        }\n",
        "        std_dev = data[feature_name].std()\n",
        "\n",
        "        # Simple outlier detection using IQR\n",
        "        Q1, Q3 = quartiles[0.25], quartiles[0.75]\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - (1.5 * IQR)\n",
        "        upper_bound = Q3 + (1.5 * IQR)\n",
        "        outliers = data[feature_name][(data[feature_name] < lower_bound) | (data[feature_name] > upper_bound)].count()\n",
        "\n",
        "        stats[feature_name] = {\n",
        "            'min': min_val,\n",
        "            'max': max_val,\n",
        "            'range': range_val,\n",
        "            'quartiles': quartiles,\n",
        "            'IQR': iqr,\n",
        "            'five_number_summary': five_number_summary,\n",
        "            'std_dev': std_dev,\n",
        "            'outliers': outliers\n",
        "        }\n",
        "    return stats\n",
        "\n",
        "# Calculate for each dataset\n",
        "train_stats = calculate_stats(train_data.drop('label', axis=1))\n",
        "dev_stats = calculate_stats(dev_data.drop('label', axis=1))\n",
        "eval_stats = calculate_stats(eval_data.drop('label', axis=1))\n",
        "\n",
        "# Function to plot distributions of min, max, and outliers\n",
        "def plot_distributions(stats, title):\n",
        "    min_values = [stats[feature]['min'] for feature in stats]\n",
        "    max_values = [stats[feature]['max'] for feature in stats]\n",
        "    outliers = [stats[feature]['outliers'] for feature in stats]\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 6))\n",
        "\n",
        "    sns.histplot(min_values, ax=axes[0], kde=True)\n",
        "    axes[0].set_title(f\"Distribution of Min Values - {title}\")\n",
        "    axes[0].set_label(f\"Distribution of Min Values - {title}\")\n",
        "\n",
        "    sns.histplot(max_values, ax=axes[1], kde=True)\n",
        "    axes[1].set_title(f\"Distribution of Max Values - {title}\")\n",
        "\n",
        "    sns.histplot(outliers, ax=axes[2], kde=True)\n",
        "    axes[2].set_title(f\"Distribution of Outliers - {title}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot distributions for each dataset\n",
        "plot_distributions(train_stats, \"Train Data\")\n",
        "plot_distributions(dev_stats, \"Dev Data\")\n",
        "plot_distributions(eval_stats, \"Eval Data\")\n"
      ],
      "metadata": {
        "id": "-18qKhC43cyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks"
      ],
      "metadata": {
        "id": "NMpG_lZ3ayJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NN without Data Handling"
      ],
      "metadata": {
        "id": "Tw7EWqmCa42-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc, classification_report\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
        "    # Convert predictions to binary (0 or 1)\n",
        "    y_pred_binary = np.array([1 if p >= 0.5 else 0 for p in y_pred])\n",
        "    y_true = np.array(y_true).flatten()\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred_binary)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "                yticklabels=['Actual 0', 'Actual 1'])\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.ylabel('Actual Values')\n",
        "    plt.xlabel('Predicted Values')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'Confusion_Matrix_{model_name}.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate and print metrics\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"True Positives: {tp}, False Positives: {fp}\")\n",
        "    print(f\"True Negatives: {tn}, False Negatives: {fn}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "def plot_predicted_vs_actual(y_true, y_pred, model_name):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Convert to numpy arrays if they aren't already\n",
        "    y_true_array = np.array(y_true).flatten() if hasattr(y_true, 'flatten') else np.array(y_true).flatten()\n",
        "    y_pred_array = np.array(y_pred).flatten() if hasattr(y_pred, 'flatten') else np.array(y_pred).flatten()\n",
        "\n",
        "    # Get indices for sorting by true values\n",
        "    indices = np.argsort(y_true_array)\n",
        "\n",
        "    # Plot\n",
        "    plt.scatter(range(len(y_true_array)), y_true_array[indices], color='blue', label='Actual', alpha=0.5, s=10)\n",
        "    plt.scatter(range(len(y_pred_array)), y_pred_array[indices], color='red', label='Predicted', alpha=0.5, s=10)\n",
        "\n",
        "    plt.title(f'Predicted vs Actual Values - {model_name}')\n",
        "    plt.xlabel('Sample Index (sorted by actual value)')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'predicted_vs_actual_{model_name}.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Also plot a density scatter plot\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.hist2d(y_true_array, y_pred_array, bins=20, cmap='viridis')\n",
        "    plt.colorbar(label='Count')\n",
        "    plt.plot([0, 1], [0, 1], 'r--')\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title(f'Density Plot of Predicted vs Actual - {model_name}')\n",
        "    plt.tight_layout()\n",
        "    #plt.savefig(f'Density_scatter_{model_name}.png')\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curve(y_true, y_pred, model_name):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curve - {model_name}')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'ROC_curve_{model_name}.png')\n",
        "    plt.show()\n",
        "\n",
        "    return roc_auc\n",
        "\n",
        "def plot_precision_recall_curve(y_true, y_pred, model_name):\n",
        "    # Convert to numpy arrays if they aren't already\n",
        "    y_true_array = np.array(y_true).flatten() if hasattr(y_true, 'flatten') else np.array(y_true).flatten()\n",
        "    y_pred_array = np.array(y_pred).flatten() if hasattr(y_pred, 'flatten') else np.array(y_pred).flatten()\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true_array, y_pred_array)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    # Calculate baseline (positive rate) correctly with numpy array\n",
        "    baseline = np.mean(y_true_array)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (area = {pr_auc:.4f})')\n",
        "    plt.axhline(y=baseline, color='red', linestyle='--',\n",
        "                label=f'Baseline (ratio = {baseline:.4f})')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(f'Precision-Recall Curve - {model_name}')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'precision_recall_curve_{model_name}.png')\n",
        "    plt.show()\n",
        "\n",
        "    return pr_auc\n",
        "\n",
        "def plot_prediction_distribution(y_pred, model_name):\n",
        "    # plt.figure(figsize=(10, 6))\n",
        "    # sns.histplot(y_pred, bins=50, kde=True)\n",
        "    # plt.axvline(x=0.5, color='red', linestyle='--', label='Decision Threshold')\n",
        "    # plt.title(f'Distribution of Predictions - {model_name}')\n",
        "    # plt.xlabel('Predicted Probability')\n",
        "    # plt.ylabel('Count')\n",
        "    # plt.legend()\n",
        "    # plt.grid(True, alpha=0.3)\n",
        "    # plt.tight_layout()\n",
        "    # plt.show()\n",
        "\n",
        "def plot_learning_curves(train_losses, val_losses, train_accs, val_accs, model_name):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    ax1.plot(train_losses, label='Training Loss')\n",
        "    ax1.plot(val_losses, label='Validation Loss')\n",
        "    ax1.set_title(f'{model_name} - Loss Curves')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy plot\n",
        "    ax2.plot(train_accs, label='Training Accuracy')\n",
        "    ax2.plot(val_accs, label='Validation Accuracy')\n",
        "    ax2.set_title(f'{model_name} - Accuracy Curves')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'learning_curves_{model_name}.png')\n",
        "    plt.show()\n",
        "\n",
        "def compare_models_roc(all_y_true, all_y_pred, model_names):\n",
        "    \"\"\"\n",
        "    Compare ROC curves of multiple models on the same plot\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    for i, (y_true, y_pred, name) in enumerate(zip(all_y_true, all_y_pred, model_names)):\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.4f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves Comparison')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('roc_curves_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "def plot_feature_importance(model, input_dim, model_name):\n",
        "    \"\"\"\n",
        "    Plot feature importance for single layer perceptron\n",
        "    \"\"\"\n",
        "    if hasattr(model, 'linear'):\n",
        "        # Get weights from the model\n",
        "        weights = model.linear.weight.data.cpu().numpy().flatten()\n",
        "\n",
        "        # Plot top 20 features by magnitude\n",
        "        indices = np.argsort(np.abs(weights))[-20:]\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.barh(np.arange(len(indices)), weights[indices])\n",
        "        plt.yticks(np.arange(len(indices)), [f'Feature {i}' for i in indices])\n",
        "        plt.title(f'Top 20 Feature Importance - {model_name}')\n",
        "        plt.xlabel('Weight Magnitude')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'feature_importance_{model_name}.png')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Feature importance visualization not applicable for {model_name}\")\n",
        "\n",
        "# Updated training function to track accuracy as well\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device='cpu'):\n",
        "    model = model.to(device)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accs.append(epoch_acc)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_epoch_loss = val_loss / len(val_loader.dataset)\n",
        "        val_epoch_acc = val_correct / val_total\n",
        "        val_losses.append(val_epoch_loss)\n",
        "        val_accs.append(val_epoch_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "              f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, '\n",
        "              f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}')\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs\n",
        "\n",
        "# Example of usage with all the visualization functions\n",
        "# Hyperparameters\n",
        "input_dim = 1024\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Prepare data\n",
        "train_loader, val_loader, test_loader = prepare_data(\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test, batch_size)\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Lists to store results for comparison\n",
        "all_model_names = [\"Single Layer Perceptron\", \"Multi-Layer Perceptron\", \"Deep Neural Network\", \"Artificial Neural Network\"]\n",
        "all_test_y_true = []\n",
        "all_test_y_pred = []\n",
        "all_roc_aucs = []\n",
        "all_pr_aucs = []\n",
        "all_test_accs = []\n",
        "\n",
        "# 1. Train and evaluate Single Layer Perceptron\n",
        "print(\"\\n=== Training Single Layer Perceptron ===\")\n",
        "slp_model = SingleLayerPerceptron(input_dim)\n",
        "slp_optimizer = optim.Adam(slp_model.parameters(), lr=learning_rate)\n",
        "slp_train_losses, slp_val_losses, slp_train_accs, slp_val_accs = train_model(\n",
        "    slp_model, train_loader, val_loader, criterion, slp_optimizer, num_epochs, device)\n",
        "\n",
        "# Evaluate on test set\n",
        "slp_test_loss, slp_test_acc, slp_preds, slp_targets = evaluate_model(\n",
        "    slp_model, test_loader, criterion, device)\n",
        "all_test_y_true.append(slp_targets)\n",
        "all_test_y_pred.append(slp_preds)\n",
        "all_test_accs.append(slp_test_acc)\n",
        "\n",
        "# Visualizations for Single Layer Perceptron\n",
        "model_name = \"Single Layer Perceptron\"\n",
        "plot_learning_curves(slp_train_losses, slp_val_losses, slp_train_accs, slp_val_accs, model_name)\n",
        "plot_confusion_matrix(slp_targets, slp_preds, model_name)\n",
        "plot_predicted_vs_actual(slp_targets, slp_preds, model_name)\n",
        "slp_roc_auc = plot_roc_curve(slp_targets, slp_preds, model_name)\n",
        "slp_pr_auc = plot_precision_recall_curve(slp_targets, slp_preds, model_name)\n",
        "plot_prediction_distribution(slp_preds, model_name)\n",
        "plot_feature_importance(slp_model, input_dim, model_name)\n",
        "all_roc_aucs.append(slp_roc_auc)\n",
        "all_pr_aucs.append(slp_pr_auc)\n",
        "\n",
        "# 2. Train and evaluate Multi-Layer Perceptron\n",
        "print(\"\\n=== Training Multi-Layer Perceptron ===\")\n",
        "mlp_model = MultiLayerPerceptron(input_dim)\n",
        "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=learning_rate)\n",
        "mlp_train_losses, mlp_val_losses, mlp_train_accs, mlp_val_accs = train_model(\n",
        "    mlp_model, train_loader, val_loader, criterion, mlp_optimizer, num_epochs, device)\n",
        "\n",
        "# Evaluate on test set\n",
        "mlp_test_loss, mlp_test_acc, mlp_preds, mlp_targets = evaluate_model(\n",
        "    mlp_model, test_loader, criterion, device)\n",
        "all_test_y_true.append(mlp_targets)\n",
        "all_test_y_pred.append(mlp_preds)\n",
        "all_test_accs.append(mlp_test_acc)\n",
        "\n",
        "# Visualizations for Multi-Layer Perceptron\n",
        "model_name = \"Multi-Layer Perceptron\"\n",
        "plot_learning_curves(mlp_train_losses, mlp_val_losses, mlp_train_accs, mlp_val_accs, model_name)\n",
        "plot_confusion_matrix(mlp_targets, mlp_preds, model_name)\n",
        "plot_predicted_vs_actual(mlp_targets, mlp_preds, model_name)\n",
        "mlp_roc_auc = plot_roc_curve(mlp_targets, mlp_preds, model_name)\n",
        "mlp_pr_auc = plot_precision_recall_curve(mlp_targets, mlp_preds, model_name)\n",
        "plot_prediction_distribution(mlp_preds, model_name)\n",
        "all_roc_aucs.append(mlp_roc_auc)\n",
        "all_pr_aucs.append(mlp_pr_auc)\n",
        "\n",
        "# 3. Train and evaluate Deep Neural Network\n",
        "print(\"\\n=== Training Deep Neural Network ===\")\n",
        "dnn_model = DeepNeuralNetwork(input_dim)\n",
        "dnn_optimizer = optim.Adam(dnn_model.parameters(), lr=learning_rate)\n",
        "dnn_train_losses, dnn_val_losses, dnn_train_accs, dnn_val_accs = train_model(\n",
        "    dnn_model, train_loader, val_loader, criterion, dnn_optimizer, num_epochs, device)\n",
        "\n",
        "# Evaluate on test set\n",
        "dnn_test_loss, dnn_test_acc, dnn_preds, dnn_targets = evaluate_model(\n",
        "    dnn_model, test_loader, criterion, device)\n",
        "all_test_y_true.append(dnn_targets)\n",
        "all_test_y_pred.append(dnn_preds)\n",
        "all_test_accs.append(dnn_test_acc)\n",
        "\n",
        "# Visualizations for Deep Neural Network\n",
        "model_name = \"Deep Neural Network\"\n",
        "plot_learning_curves(dnn_train_losses, dnn_val_losses, dnn_train_accs, dnn_val_accs, model_name)\n",
        "plot_confusion_matrix(dnn_targets, dnn_preds, model_name)\n",
        "plot_predicted_vs_actual(dnn_targets, dnn_preds, model_name)\n",
        "dnn_roc_auc = plot_roc_curve(dnn_targets, dnn_preds, model_name)\n",
        "dnn_pr_auc = plot_precision_recall_curve(dnn_targets, dnn_preds, model_name)\n",
        "plot_prediction_distribution(dnn_preds, model_name)\n",
        "all_roc_aucs.append(dnn_roc_auc)\n",
        "all_pr_aucs.append(dnn_pr_auc)\n",
        "\n",
        "# 4. Train and evaluate Artificial Neural Network\n",
        "print(\"\\n=== Training Artificial Neural Network ===\")\n",
        "ann_model = ArtificialNeuralNetwork(input_dim)\n",
        "ann_optimizer = optim.Adam(ann_model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "ann_train_losses, ann_val_losses, ann_train_accs, ann_val_accs = train_model(\n",
        "    ann_model, train_loader, val_loader, criterion, ann_optimizer, num_epochs, device)\n",
        "\n",
        "# Evaluate on test set\n",
        "ann_test_loss, ann_test_acc, ann_preds, ann_targets = evaluate_model(\n",
        "    ann_model, test_loader, criterion, device)\n",
        "all_test_y_true.append(ann_targets)\n",
        "all_test_y_pred.append(ann_preds)\n",
        "all_test_accs.append(ann_test_acc)\n",
        "\n",
        "# Visualizations for Artificial Neural Network\n",
        "model_name = \"Artificial Neural Network\"\n",
        "plot_learning_curves(ann_train_losses, ann_val_losses, ann_train_accs, ann_val_accs, model_name)\n",
        "plot_confusion_matrix(ann_targets, ann_preds, model_name)\n",
        "plot_predicted_vs_actual(ann_targets, ann_preds, model_name)\n",
        "ann_roc_auc = plot_roc_curve(ann_targets, ann_preds, model_name)\n",
        "ann_pr_auc = plot_precision_recall_curve(ann_targets, ann_preds, model_name)\n",
        "plot_prediction_distribution(ann_preds, model_name)\n",
        "all_roc_aucs.append(ann_roc_auc)\n",
        "all_pr_aucs.append(ann_pr_auc)\n",
        "\n",
        "# Compare all models\n",
        "print(\"\\n=== Model Comparison ===\")\n",
        "for name, acc, roc, pr in zip(all_model_names, all_test_accs, all_roc_aucs, all_pr_aucs):\n",
        "    print(f\"{name}: Test Acc={acc:.4f}, ROC AUC={roc:.4f}, PR AUC={pr:.4f}\")\n",
        "\n",
        "# Plot comparative ROC curves\n",
        "compare_models_roc(all_test_y_true, all_test_y_pred, all_model_names)\n",
        "\n",
        "# Plot accuracy comparison\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(all_model_names, all_test_accs)\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.savefig('accuracy_comparison_NN_withoutDataHandling.png')\n",
        "plt.show()\n",
        "\n",
        "# Plot AUC comparison\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# ROC AUC comparison\n",
        "ax1.bar(all_model_names, all_roc_aucs)\n",
        "ax1.set_title('ROC AUC Comparison')\n",
        "ax1.set_xlabel('Model')\n",
        "ax1.set_ylabel('ROC AUC')\n",
        "ax1.set_ylim(0, 1)\n",
        "ax1.grid(axis='y')\n",
        "\n",
        "# PR AUC comparison\n",
        "ax2.bar(all_model_names, all_pr_aucs)\n",
        "ax2.set_title('Precision-Recall AUC Comparison')\n",
        "ax2.set_xlabel('Model')\n",
        "ax2.set_ylabel('PR AUC')\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.grid(axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('auc_comparison_NN_withoutDataHandling.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mxStd7Kra1IP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NN with Data Handling\n"
      ],
      "metadata": {
        "id": "wqgC-myf6Cjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sampling the DataSet and Using PCA"
      ],
      "metadata": {
        "id": "rlTkvjBa6GY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "\n",
        "# For sampling strategies\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# ----------------------------\n",
        "# ASSUMPTIONS:\n",
        "# Your datasets should already be defined and preprocessed\n",
        "# X_train, y_train, X_val, y_val, X_test, y_test\n",
        "# ----------------------------\n",
        "\n",
        "# Define class weights (you can adjust based on your imbalance needs)\n",
        "class_weights = {0: 0.1, 1: 0.9}\n",
        "\n",
        "# --- Function to Create Neural Network Models ---\n",
        "def create_models(input_dim):\n",
        "    models = {}\n",
        "\n",
        "    # 1. Single-Layer Neural Network\n",
        "    model_single = Sequential([\n",
        "        Dense(1, activation='sigmoid', input_shape=(input_dim,))\n",
        "    ])\n",
        "    model_single.compile(optimizer=Adam(0.001),\n",
        "                         loss='binary_crossentropy',\n",
        "                         metrics=['accuracy'])\n",
        "    models['Single-Layer'] = model_single\n",
        "\n",
        "    # 2. Multi-Layer Neural Network (2 layers)\n",
        "    model_multi = Sequential([\n",
        "        Dense(512, activation='relu', input_shape=(input_dim,)),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model_multi.compile(optimizer=Adam(0.001),\n",
        "                        loss='binary_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "    models['Multi-Layer'] = model_multi\n",
        "\n",
        "    # 3. Deep Neural Network (4 layers)\n",
        "    model_deep = Sequential([\n",
        "        Dense(512, activation='relu', input_shape=(input_dim,)),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model_deep.compile(optimizer=Adam(0.001),\n",
        "                        loss='binary_crossentropy',\n",
        "                        metrics=['accuracy'])\n",
        "    models['Deep'] = model_deep\n",
        "\n",
        "    # 4. Regularized ANN (with dropout and L2)\n",
        "    model_reg = Sequential([\n",
        "        Dense(512, activation='relu', kernel_regularizer=l2(0.001), input_shape=(input_dim,)),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model_reg.compile(optimizer=Adam(0.001),\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "    models['Regularized'] = model_reg\n",
        "\n",
        "    return models\n",
        "\n",
        "# --- Plotting helper functions ---\n",
        "def plot_history(history, title):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title(f'{title} - Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "    plt.title(f'{title} - Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(cm, title):\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(title)\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curve(fpr, tpr, roc_auc, title):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0,1], [0,1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# --- Define Sampling Strategies and PCA Dimensions ---\n",
        "# For oversampling, we use SMOTE; for undersampling, use RandomUnderSampler.\n",
        "sampling_strategies = {\n",
        "    'SMOTE': SMOTE(random_state=42),\n",
        "    'Undersampling': RandomUnderSampler(random_state=42)\n",
        "}\n",
        "\n",
        "pca_dimensions = [50, 100, 200]\n",
        "\n",
        "# ----------------------------\n",
        "# Loop over each sampling strategy and PCA dimension\n",
        "for sample_method, sampler in sampling_strategies.items():\n",
        "    for n_components in pca_dimensions:\n",
        "        print(f\"\\n=== Running {sample_method} with PCA (n_components = {n_components}) ===\\n\")\n",
        "\n",
        "        # 1. Apply sampling only to the training set\n",
        "        X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
        "        print(f\"Resampled training set shape: {X_train_res.shape}, Class distribution: {np.bincount(y_train_res)}\")\n",
        "\n",
        "        # 2. Apply PCA on the resampled training data and transform validation and test sets\n",
        "        pca = PCA(n_components=n_components, random_state=42)\n",
        "        X_train_pca = pca.fit_transform(X_train_res)\n",
        "        X_val_pca   = pca.transform(X_val)\n",
        "        X_test_pca  = pca.transform(X_test)\n",
        "        print(f\"PCA-transformed train shape: {X_train_pca.shape}\")\n",
        "\n",
        "        # 3. Create neural network models adjusted for the new input dimension (n_components)\n",
        "        models = create_models(input_dim=n_components)\n",
        "        histories = {}  # To store training history for each model\n",
        "\n",
        "        # 4. Train each model on the PCA-transformed, resampled training data\n",
        "        for model_name, model in models.items():\n",
        "            print(f\"Training model: {model_name}\")\n",
        "            history = model.fit(\n",
        "                X_train_pca, y_train_res,\n",
        "                validation_data=(X_val_pca, y_val),\n",
        "                epochs=30,\n",
        "                batch_size=32,\n",
        "                class_weight=class_weights,\n",
        "                verbose=0\n",
        "            )\n",
        "            histories[model_name] = history\n",
        "            # Plot training history for this model\n",
        "            plot_history(history, f\"{model_name} ({sample_method}, PCA={n_components})\")\n",
        "\n",
        "        # 5. Combined Validation Accuracy Plot for all models in this experiment\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        for model_name, history in histories.items():\n",
        "            plt.plot(history.history['val_accuracy'], label=f'{model_name}')\n",
        "        plt.title(f'Model Comparison - Val Accuracy ({sample_method}, PCA={n_components})')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        # 6. Evaluate each model on the test data (transformed by PCA)\n",
        "        for model_name, model in models.items():\n",
        "            print(f\"Evaluating model: {model_name} ({sample_method}, PCA={n_components})\")\n",
        "            # Predict probabilities and then binarize predictions using a threshold of 0.5\n",
        "            y_pred_prob = model.predict(X_test_pca)\n",
        "            y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "            # Generate and plot Confusion Matrix\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "            plot_confusion_matrix(cm, f'{model_name} - Confusion Matrix ({sample_method}, PCA={n_components})')\n",
        "\n",
        "            # Print Classification Report\n",
        "            print(f'\\n{model_name} - Classification Report ({sample_method}, PCA={n_components}):')\n",
        "            print(classification_report(y_test, y_pred))\n",
        "\n",
        "            # Plot ROC Curve\n",
        "            fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            plot_roc_curve(fpr, tpr, roc_auc, f'{model_name} - ROC Curve ({sample_method}, PCA={n_components})')\n"
      ],
      "metadata": {
        "id": "-e_ZT8Ax6WfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adam Optimisor & Cross Entropy Loss Function for NN"
      ],
      "metadata": {
        "id": "LXsfZq4e2QNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterating through multiple models for Hyper-parameter Tuning & Threshold tuning\n",
        "\n",
        "Changing the class weights and checking for the results...\n",
        "\n",
        "\n",
        "These are multiple codes....."
      ],
      "metadata": {
        "id": "flsivAao2Wcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Working with existing variables: X_train, y_train, X_val, y_val, X_test, y_test\n",
        "# Assuming these are already defined with binary classes (0 and 1)\n",
        "# and there are 1024 features\n",
        "\n",
        "# Get input dimension from the data\n",
        "input_dim = 1024  # As specified\n",
        "n_classes = 2  # Binary classification with classes 0 and 1\n",
        "\n",
        "# Store original labels for metric calculation\n",
        "y_train_orig = y_train.copy()\n",
        "y_val_orig = y_val.copy()\n",
        "y_test_orig = y_test.copy()\n",
        "\n",
        "# Calculate class weights for weighted cross-entropy\n",
        "def calculate_class_weights(y):\n",
        "    class_counts = np.bincount(y.astype(int))\n",
        "    total = len(y)\n",
        "\n",
        "    # Inverse frequency weighting\n",
        "    class_weights = total / (len(class_counts) * class_counts)\n",
        "\n",
        "    # Convert to dictionary format for Keras\n",
        "    weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "    return weights_dict, class_weights\n",
        "\n",
        "weights_dict, class_weights_array = calculate_class_weights(y_train)\n",
        "print(\"Class weights:\", weights_dict)\n",
        "\n",
        "# Define a custom weighted binary cross-entropy loss function\n",
        "def weighted_binary_crossentropy(class_weights):\n",
        "    def loss(y_true, y_pred):\n",
        "        # Standard binary crossentropy calculation\n",
        "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "\n",
        "        # Create weight vector based on class values in y_true\n",
        "        weights = y_true * class_weights[1] + (1 - y_true) * class_weights[0]\n",
        "\n",
        "        # Apply weights to the bce loss\n",
        "        weighted_bce = weights * bce\n",
        "\n",
        "        return tf.reduce_mean(weighted_bce)\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Define a function to create and compile a neural network\n",
        "def create_neural_network(model_type, input_dim, class_weights_array):\n",
        "    model = Sequential()\n",
        "\n",
        "    if model_type == \"Single Layer\":\n",
        "        model.add(Dense(units=1, input_dim=input_dim, activation='sigmoid'))\n",
        "\n",
        "    elif model_type == \"Multi-layer\":\n",
        "        model.add(Dense(units=64, input_dim=input_dim, activation='relu'))\n",
        "        model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "    elif model_type == \"Deep NN\":\n",
        "        model.add(Dense(units=128, input_dim=input_dim, activation='relu'))\n",
        "        model.add(Dense(units=64, activation='relu'))\n",
        "        model.add(Dense(units=32, activation='relu'))\n",
        "        model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "    elif model_type == \"Artificial NN\":\n",
        "        model.add(Dense(units=256, input_dim=input_dim, activation='relu'))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(units=128, activation='relu'))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(units=64, activation='relu'))\n",
        "        model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "    # Custom weighted binary cross-entropy loss function\n",
        "    weighted_loss = weighted_binary_crossentropy(class_weights_array)\n",
        "\n",
        "    # Compile the model with our custom weighted loss\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                 loss=weighted_loss,\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function for training and evaluation\n",
        "def train_and_evaluate(model_type, X_train, y_train, X_val, y_val, X_test, y_test, class_weights_array):\n",
        "    # Create model\n",
        "    model = create_neural_network(model_type, input_dim, class_weights_array)\n",
        "\n",
        "    # Set up early stopping\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=20,\n",
        "        batch_size=32,\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate on test set\n",
        "    y_pred_prob = model.predict(X_test)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test_orig, y_pred)\n",
        "    precision = precision_score(y_test_orig, y_pred)\n",
        "    recall = recall_score(y_test_orig, y_pred)\n",
        "    f1 = f1_score(y_test_orig, y_pred)\n",
        "\n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(y_test_orig, y_pred)\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "    return model, history, metrics, cm\n",
        "\n",
        "# List of models to train\n",
        "model_types = [\"Single Layer\", \"Multi-layer\", \"Deep NN\", \"Artificial NN\"]\n",
        "model_results = {}\n",
        "\n",
        "# Train each model\n",
        "for model_type in model_types:\n",
        "    print(f\"\\nTraining {model_type} Neural Network...\")\n",
        "    model, history, metrics, cm = train_and_evaluate(\n",
        "        model_type, X_train, y_train, X_val, y_val, X_test, y_test, class_weights_array\n",
        "    )\n",
        "    model_results[model_type] = {\n",
        "        'model': model,\n",
        "        'history': history,\n",
        "        'metrics': metrics,\n",
        "        'confusion_matrix': cm\n",
        "    }\n",
        "    print(f\"{model_type} Metrics:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "# Plotting functions\n",
        "def plot_training_history(histories, model_types):\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Plot training & validation accuracy\n",
        "    plt.subplot(2, 1, 1)\n",
        "    for model_type in model_types:\n",
        "        history = histories[model_type]['history']\n",
        "        plt.plot(history.history['accuracy'], label=f'{model_type} Training')\n",
        "        plt.plot(history.history['val_accuracy'], label=f'{model_type} Validation')\n",
        "\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(loc='lower right')\n",
        "\n",
        "    # Plot training & validation loss\n",
        "    plt.subplot(2, 1, 2)\n",
        "    for model_type in model_types:\n",
        "        history = histories[model_type]['history']\n",
        "        plt.plot(history.history['loss'], label=f'{model_type} Training')\n",
        "        plt.plot(history.history['val_loss'], label=f'{model_type} Validation')\n",
        "\n",
        "    plt.title('Model Loss (Weighted Cross-Entropy)')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrices(model_results, model_types):\n",
        "    n = len(model_types)\n",
        "    fig, axes = plt.subplots(1, n, figsize=(n*5, 5))\n",
        "\n",
        "    if n == 1:\n",
        "        axes = [axes]  # Handle the case of a single plot\n",
        "\n",
        "    for i, model_type in enumerate(model_types):\n",
        "        cm = model_results[model_type]['confusion_matrix']\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
        "        axes[i].set_title(f'{model_type} Confusion Matrix')\n",
        "        axes[i].set_xlabel('Predicted labels')\n",
        "        axes[i].set_ylabel('True labels')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrices.png')\n",
        "    plt.show()\n",
        "\n",
        "def plot_metrics_comparison(model_results, model_types):\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "    # Prepare data for plotting\n",
        "    df_metrics = pd.DataFrame({\n",
        "        metric: [model_results[model_type]['metrics'][metric] for model_type in model_types]\n",
        "        for metric in metrics\n",
        "    }, index=model_types)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    df_metrics.plot(kind='bar', rot=0)\n",
        "    plt.title('Performance Comparison Across Models')\n",
        "    plt.ylabel('Score')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend(title='Metric')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('metrics_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "def plot_individual_model_metrics(model_results):\n",
        "    # Plot separate metrics for each model\n",
        "    for model_type in model_results:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        metrics = model_results[model_type]['metrics']\n",
        "        plt.bar(metrics.keys(), metrics.values())\n",
        "        plt.title(f'{model_type} Performance Metrics')\n",
        "        plt.ylabel('Score')\n",
        "        plt.ylim(0, 1)\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{model_type.replace(\" \", \"_\").lower()}_metrics.png')\n",
        "        plt.show()\n",
        "\n",
        "# Generate plots\n",
        "plot_training_history(model_results, model_types)\n",
        "plot_confusion_matrices(model_results, model_types)\n",
        "plot_metrics_comparison(model_results, model_types)\n",
        "plot_individual_model_metrics(model_results)\n",
        "\n",
        "# Summary of results\n",
        "print(\"\\n===== RESULTS SUMMARY =====\")\n",
        "for model_type in model_types:\n",
        "    metrics = model_results[model_type]['metrics']\n",
        "    print(f\"\\n{model_type} Neural Network:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "# Create a recommendation based on the results\n",
        "best_model = max(model_types, key=lambda x: model_results[x]['metrics']['f1'])\n",
        "print(f\"\\nRecommended model: {best_model} (based on F1 score)\")\n",
        "print(f\"F1 Score: {model_results[best_model]['metrics']['f1']:.4f}\")"
      ],
      "metadata": {
        "id": "Nek_X6Wp2f8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Define class weights (adjust based on class imbalance)\n",
        "class_weights = {0: 0.1, 1: 0.9}\n",
        "\n",
        "# Initialize history storage\n",
        "histories = {}\n",
        "\n",
        "# 1. Single-Layer Neural Network\n",
        "model_single = Sequential([\n",
        "    Dense(1, activation='sigmoid', input_shape=(1024,))\n",
        "])\n",
        "model_single.compile(optimizer=Adam(0.001),\n",
        "                    loss='binary_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "histories['Single-Layer'] = model_single.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# 2. Multi-Layer Neural Network (2 layers)\n",
        "model_multi = Sequential([\n",
        "    Dense(512, activation='relu', input_shape=(1024,)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_multi.compile(optimizer=Adam(0.001),\n",
        "                   loss='binary_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "histories['Multi-Layer'] = model_multi.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# 3. Deep Neural Network (4 layers)\n",
        "model_deep = Sequential([\n",
        "    Dense(512, activation='relu', input_shape=(1024,)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_deep.compile(optimizer=Adam(0.001),\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "histories['Deep'] = model_deep.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# 4. Regularized ANN (with dropout and L2)\n",
        "model_reg = Sequential([\n",
        "    Dense(512, activation='relu', kernel_regularizer=l2(0.001), input_shape=(1024,)),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_reg.compile(optimizer=Adam(0.001),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "histories['Regularized'] = model_reg.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Individual model plots\n",
        "def plot_history(history, title):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title(f'{title} - Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "    plt.title(f'{title} - Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "for name, history in histories.items():\n",
        "    plot_history(history, name)\n",
        "\n",
        "# Comparison plot\n",
        "plt.figure(figsize=(14, 6))\n",
        "for model_name, history in histories.items():\n",
        "    plt.plot(history.history['val_accuracy'], label=f'{model_name} Val Accuracy')\n",
        "plt.title('Model Comparison - Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    # Predict probabilities and classes\n",
        "    y_probs = model.predict(X_test, verbose=0)\n",
        "    y_pred = (y_probs > 0.5).astype(int)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'{model.name} Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "    # ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.title(f'{model.name} ROC Curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "    # Classification Report\n",
        "    print(f\"Classification Report for {model.name}:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))\n",
        "\n",
        "# Load saved models and evaluate\n",
        "models = {\n",
        "    'Single-Layer': model_single,\n",
        "    'Multi-Layer': model_multi,\n",
        "    'Deep': model_deep,\n",
        "    'Regularized': model_reg\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model._name = name  # Set model name for labeling\n",
        "    evaluate_model(model, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "3Unth9jn2tGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM (Yash)"
      ],
      "metadata": {
        "id": "yLoJxHaX0TB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn import svm\n",
        "\n",
        "# Assume X and y are your 2D dataset and labels\n",
        "# For demonstration, let's generate a synthetic dataset\n",
        "#from sklearn.datasets import make_classification\n",
        "#X, y = make_classification(n_samples=200, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.9], flip_y=0, random_state=4)\n",
        "\n",
        "# Apply SVM with a linear kernel for simplicity\n",
        "svm_model = svm.SVC(kernel='linear')\n",
        "\n",
        "# Train the model\n",
        "svm_model.fit(X_train_pca, y_train)\n",
        "\n",
        "# Create a grid of points to visualize the decision boundary\n",
        "h = .02  # step size in the mesh\n",
        "x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
        "y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "\n",
        "# Predict probabilities or decision function values for the grid\n",
        "Z = svm_model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Reshape Z to match the grid dimensions\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plotting\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot data points\n",
        "ax.scatter(X_train_pca[y_train == 0, 0], X_train_pca[y_train == 0, 1], np.zeros(len(X_train_pca[y_train == 0])), color='orange', label='Class 0')\n",
        "ax.scatter(X_train_pca[y_train == 1, 0], X_train_pca[y_train == 1, 1], np.zeros(len(X_train_pca[y_train == 1])), color='blue', label='Class 1')\n",
        "\n",
        "# Plot the decision boundary as a surface\n",
        "ax.plot_surface(xx, yy, Z, alpha=0.5, cmap='coolwarm')\n",
        "\n",
        "ax.set_title('SVM Decision Boundary in 3D')\n",
        "ax.set_xlabel('Feature 1')\n",
        "ax.set_ylabel('Feature 2')\n",
        "ax.set_zlabel('Decision Function Value')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AnfDsVms5jw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style and figure size for better visualization\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "\n",
        "def evaluate_model(model, X, y, set_name):\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Calculate metrics (setting zero_division=0 to handle cases where precision/recall might be undefined)\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    try:\n",
        "        precision = precision_score(y, y_pred, zero_division=0)\n",
        "        recall = recall_score(y, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y, y_pred, zero_division=0)\n",
        "    except:\n",
        "        precision, recall, f1 = 0, 0, 0\n",
        "\n",
        "    print(f\"\\n--- {set_name} Metrics ---\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    return y_pred, accuracy, precision, recall, f1\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, set_name, save=True):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "    plt.title(f'Confusion Matrix - {set_name}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    if save:\n",
        "        plt.savefig(f'confusion_matrix_{set_name.lower().replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate and print additional metrics from confusion matrix\n",
        "    if cm.size == 4:  # Binary classification\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        print(f\"True Negatives: {tn}\")\n",
        "        print(f\"False Positives: {fp}\")\n",
        "        print(f\"False Negatives: {fn}\")\n",
        "        print(f\"True Positives: {tp}\")\n",
        "\n",
        "def plot_roc_curve(model, X, y, set_name, save=True):\n",
        "    try:\n",
        "        # Get probability estimates\n",
        "        y_prob = model.decision_function(X)\n",
        "\n",
        "        # Calculate ROC curve and AUC\n",
        "        fpr, tpr, _ = roc_curve(y, y_prob)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        # Plot ROC curve\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'Receiver Operating Characteristic (ROC) - {set_name}')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        if save:\n",
        "            plt.savefig(f'roc_curve_{set_name.lower().replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        return roc_auc\n",
        "    except:\n",
        "        print(f\"Could not generate ROC curve for {set_name} - the model might not support decision_function or the data is too imbalanced\")\n",
        "        return None\n",
        "\n",
        "def plot_precision_recall_curve(model, X, y, set_name, save=True):\n",
        "    try:\n",
        "        # Get probability estimates\n",
        "        y_prob = model.decision_function(X)\n",
        "\n",
        "        # Calculate precision-recall curve\n",
        "        precision, recall, _ = precision_recall_curve(y, y_prob)\n",
        "\n",
        "        # Plot precision-recall curve\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(recall, precision, color='green', lw=2)\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.title(f'Precision-Recall Curve - {set_name}')\n",
        "        if save:\n",
        "            plt.savefig(f'precision_recall_curve_{set_name.lower().replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    except:\n",
        "        print(f\"Could not generate Precision-Recall curve for {set_name} - the model might not support decision_function or the data is too imbalanced\")\n",
        "\n",
        "def plot_decision_boundary(model, X, y, set_name, save=True):\n",
        "    if X.shape[1] != 2:\n",
        "        print(f\"Cannot plot decision boundary for {set_name} - requires exactly 2 features\")\n",
        "        return\n",
        "\n",
        "    # Create mesh grid\n",
        "    h = 0.02  # step size in the mesh\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Get predictions on the mesh grid\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot the decision boundary and scatter points\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdBu)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.RdBu)\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(f'Decision Boundary - {set_name}')\n",
        "    if save:\n",
        "        plt.savefig(f'decision_boundary_{set_name.lower().replace(\" \", \"_\")}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def run_grid_search_for_kernel(X_train, y_train, X_val, y_val, kernel_type):\n",
        "    print(f\"\\nPerforming grid search for {kernel_type} kernel...\")\n",
        "\n",
        "    # Define parameter grid based on kernel type\n",
        "    if kernel_type == 'linear':\n",
        "        param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
        "    elif kernel_type == 'poly':\n",
        "        param_grid = {'C': [0.1, 1, 10, 100], 'degree': [2, 3, 4, 5], 'gamma': ['scale', 'auto', 0.01, 0.1, 1]}\n",
        "    else:  # rbf\n",
        "        param_grid = {'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]}\n",
        "\n",
        "    # Initialize SVM model with the specific kernel\n",
        "    base_model = SVC(kernel=kernel_type, probability=True, class_weight='balanced')\n",
        "\n",
        "    # Initialize GridSearchCV\n",
        "    grid = GridSearchCV(base_model,\n",
        "                      param_grid,\n",
        "                      cv=5,\n",
        "                      scoring='f1',\n",
        "                      verbose=1,\n",
        "                      n_jobs=-1)\n",
        "\n",
        "    # Fit GridSearchCV\n",
        "    grid.fit(X_train, y_train)\n",
        "\n",
        "    # Display best parameters and score\n",
        "    print(f\"\\nBest parameters for {kernel_type} kernel: {grid.best_params_}\")\n",
        "    print(f\"Best cross-validation score: {grid.best_score_:.4f}\")\n",
        "\n",
        "    # Evaluate best model on validation set\n",
        "    best_model = grid.best_estimator_\n",
        "    y_val_pred = best_model.predict(X_val)\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    val_precision = precision_score(y_val, y_val_pred, zero_division=0)\n",
        "    val_recall = recall_score(y_val, y_val_pred, zero_division=0)\n",
        "    val_f1 = f1_score(y_val, y_val_pred, zero_division=0)\n",
        "\n",
        "    print(f\"\\nValidation set performance:\")\n",
        "    print(f\"Accuracy: {val_accuracy:.4f}\")\n",
        "    print(f\"Precision: {val_precision:.4f}\")\n",
        "    print(f\"Recall: {val_recall:.4f}\")\n",
        "    print(f\"F1 Score: {val_f1:.4f}\")\n",
        "\n",
        "    return best_model, grid.best_params_, val_f1\n",
        "\n",
        "def plot_hyperparameter_comparison(X_train, y_train, X_val, y_val, kernel_type, best_params):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Define hyperparameters to test based on kernel type\n",
        "    if kernel_type == 'linear':\n",
        "        C_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "        train_scores = []\n",
        "        val_scores = []\n",
        "\n",
        "        for C in C_values:\n",
        "            model = SVC(kernel='linear', C=C, class_weight='balanced')\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            train_pred = model.predict(X_train)\n",
        "            val_pred = model.predict(X_val)\n",
        "\n",
        "            train_f1 = f1_score(y_train, train_pred, zero_division=0)\n",
        "            val_f1 = f1_score(y_val, val_pred, zero_division=0)\n",
        "\n",
        "            train_scores.append(train_f1)\n",
        "            val_scores.append(val_f1)\n",
        "\n",
        "        plt.plot(C_values, train_scores, 'o-', label='Training F1 Score')\n",
        "        plt.plot(C_values, val_scores, 's-', label='Validation F1 Score')\n",
        "        plt.axvline(x=best_params['C'], color='r', linestyle='--',\n",
        "                    label=f'Best C: {best_params[\"C\"]}')\n",
        "        plt.xscale('log')\n",
        "        plt.xlabel('C (Regularization parameter)')\n",
        "        plt.ylabel('F1 Score')\n",
        "        plt.title(f'Impact of C on Linear SVM Performance')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.savefig('linear_svm_hyperparameter_comparison.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    elif kernel_type == 'poly':\n",
        "        C_values = [0.1, 1, 10, 100]\n",
        "        degrees = [2, 3, 4, 5]\n",
        "\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Test different C values with selected optimal degree\n",
        "        best_degree = best_params['degree']\n",
        "        train_scores_c = []\n",
        "        val_scores_c = []\n",
        "        for C in C_values:\n",
        "            model = SVC(kernel='poly', C=C, degree=best_degree, gamma=best_params['gamma'],\n",
        "                       class_weight='balanced')\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            train_pred = model.predict(X_train)\n",
        "            val_pred = model.predict(X_val)\n",
        "\n",
        "            train_f1 = f1_score(y_train, train_pred, zero_division=0)\n",
        "            val_f1 = f1_score(y_val, val_pred, zero_division=0)\n",
        "\n",
        "            train_scores_c.append(train_f1)\n",
        "            val_scores_c.append(val_f1)\n",
        "\n",
        "        ax[0].plot(C_values, train_scores_c, 'o-', label='Training F1 Score')\n",
        "        ax[0].plot(C_values, val_scores_c, 's-', label='Validation F1 Score')\n",
        "        ax[0].axvline(x=best_params['C'], color='r', linestyle='--',\n",
        "                     label=f'Best C: {best_params[\"C\"]}')\n",
        "        ax[0].set_xscale('log')\n",
        "        ax[0].set_xlabel('C (Regularization parameter)')\n",
        "        ax[0].set_ylabel('F1 Score')\n",
        "        ax[0].set_title(f'Impact of C on Polynomial SVM (degree={best_degree})')\n",
        "        ax[0].legend()\n",
        "        ax[0].grid(True)\n",
        "\n",
        "        # Test different degree values with selected optimal C\n",
        "        best_C = best_params['C']\n",
        "        train_scores_d = []\n",
        "        val_scores_d = []\n",
        "        for degree in degrees:\n",
        "            model = SVC(kernel='poly', C=best_C, degree=degree, gamma=best_params['gamma'],\n",
        "                       class_weight='balanced')\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            train_pred = model.predict(X_train)\n",
        "            val_pred = model.predict(X_val)\n",
        "\n",
        "            train_f1 = f1_score(y_train, train_pred, zero_division=0)\n",
        "            val_f1 = f1_score(y_val, val_pred, zero_division=0)\n",
        "\n",
        "            train_scores_d.append(train_f1)\n",
        "            val_scores_d.append(val_f1)\n",
        "\n",
        "        ax[1].plot(degrees, train_scores_d, 'o-', label='Training F1 Score')\n",
        "        ax[1].plot(degrees, val_scores_d, 's-', label='Validation F1 Score')\n",
        "        ax[1].axvline(x=best_params['degree'], color='r', linestyle='--',\n",
        "                     label=f'Best degree: {best_params[\"degree\"]}')\n",
        "        ax[1].set_xlabel('Polynomial Degree')\n",
        "        ax[1].set_ylabel('F1 Score')\n",
        "        ax[1].set_title(f'Impact of Degree on Polynomial SVM (C={best_C})')\n",
        "        ax[1].legend()\n",
        "        ax[1].grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('poly_svm_hyperparameter_comparison.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    elif kernel_type == 'rbf':\n",
        "        C_values = [0.1, 1, 10, 100]\n",
        "        if 'scale' in best_params['gamma'] or 'auto' in best_params['gamma']:\n",
        "            gamma_values = [0.001, 0.01, 0.1, 1, 10]\n",
        "        else:\n",
        "            gamma_values = [0.001, 0.01, 0.1, 1, 10]\n",
        "\n",
        "        # Create meshgrid of hyperparameters\n",
        "        C_mesh, gamma_mesh = np.meshgrid(C_values, gamma_values)\n",
        "        scores = np.zeros_like(C_mesh, dtype=float)\n",
        "\n",
        "        # Evaluate each combination\n",
        "        for i, gamma in enumerate(gamma_values):\n",
        "            for j, C in enumerate(C_values):\n",
        "                model = SVC(kernel='rbf', C=C, gamma=gamma, class_weight='balanced')\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "                val_pred = model.predict(X_val)\n",
        "                val_f1 = f1_score(y_val, val_pred, zero_division=0)\n",
        "\n",
        "                scores[i, j] = val_f1\n",
        "\n",
        "        # Plot heatmap\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(scores, annot=True, fmt='.3f', cmap='viridis',\n",
        "                    xticklabels=C_values, yticklabels=gamma_values)\n",
        "        plt.xlabel('C (Regularization parameter)')\n",
        "        plt.ylabel('gamma')\n",
        "        plt.title('RBF SVM: Validation F1 Score for Different C and gamma Values')\n",
        "\n",
        "        # Try to mark the best parameters\n",
        "        if isinstance(best_params['gamma'], (int, float)):\n",
        "            try:\n",
        "                best_gamma_idx = gamma_values.index(best_params['gamma'])\n",
        "                best_C_idx = C_values.index(best_params['C'])\n",
        "                plt.plot(best_C_idx + 0.5, best_gamma_idx + 0.5, 'ro', markersize=12,\n",
        "                        markeredgecolor='white', markeredgewidth=2)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        plt.savefig('rbf_svm_hyperparameter_comparison.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "def compare_kernels(X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    # Find best hyperparameters for each kernel\n",
        "    best_models = {}\n",
        "    best_params = {}\n",
        "    val_f1_scores = {}\n",
        "\n",
        "    # Get best parameters for each kernel type\n",
        "    for kernel in ['linear', 'poly', 'rbf']:\n",
        "        print(f\"\\n{'-'*50}\")\n",
        "        print(f\"Finding best hyperparameters for {kernel} kernel\")\n",
        "        print(f\"{'-'*50}\")\n",
        "\n",
        "        best_model, params, val_f1 = run_grid_search_for_kernel(X_train, y_train, X_val, y_val, kernel)\n",
        "        best_models[kernel] = best_model\n",
        "        best_params[kernel] = params\n",
        "        val_f1_scores[kernel] = val_f1\n",
        "\n",
        "    # Evaluate performance for each kernel with its best hyperparameters\n",
        "    results = {}\n",
        "\n",
        "    for kernel in ['linear', 'poly', 'rbf']:\n",
        "        print(f\"\\n{'-'*50}\")\n",
        "        print(f\"Evaluating SVM with {kernel} kernel and optimal hyperparameters\")\n",
        "        print(f\"Best parameters: {best_params[kernel]}\")\n",
        "        print(f\"{'-'*50}\")\n",
        "\n",
        "        model = best_models[kernel]\n",
        "\n",
        "        # Evaluate on each dataset\n",
        "        _, train_acc, train_prec, train_rec, train_f1 = evaluate_model(model, X_train, y_train, f\"{kernel.upper()} Training Set\")\n",
        "        _, val_acc, val_prec, val_rec, val_f1 = evaluate_model(model, X_val, y_val, f\"{kernel.upper()} Validation Set\")\n",
        "        test_pred, test_acc, test_prec, test_rec, test_f1 = evaluate_model(model, X_test, y_test, f\"{kernel.upper()} Test Set\")\n",
        "\n",
        "        # Store results\n",
        "        results[kernel] = {\n",
        "            'train': {'accuracy': train_acc, 'precision': train_prec, 'recall': train_rec, 'f1': train_f1},\n",
        "            'val': {'accuracy': val_acc, 'precision': val_prec, 'recall': val_rec, 'f1': val_f1},\n",
        "            'test': {'accuracy': test_acc, 'precision': test_prec, 'recall': test_rec, 'f1': test_f1}\n",
        "        }\n",
        "\n",
        "        # Plot confusion matrix for test set\n",
        "        plot_confusion_matrix(y_test, test_pred, f\"{kernel.upper()} SVM Test Set\")\n",
        "\n",
        "        # Try to plot ROC curve and PR curve (may fail if highly imbalanced)\n",
        "        try:\n",
        "            plot_roc_curve(model, X_test, y_test, f\"{kernel.upper()} SVM\")\n",
        "            plot_precision_recall_curve(model, X_test, y_test, f\"{kernel.upper()} SVM\")\n",
        "        except:\n",
        "            print(f\"Could not generate ROC or PR curves for {kernel} kernel - likely due to highly imbalanced data\")\n",
        "\n",
        "        # Try to plot decision boundary if 2D\n",
        "        if X_test.shape[1] == 2:\n",
        "            plot_decision_boundary(model, X_test, y_test, f\"{kernel.upper()} SVM\")\n",
        "\n",
        "        # Plot hyperparameter comparison\n",
        "        plot_hyperparameter_comparison(X_train, y_train, X_val, y_val, kernel, best_params[kernel])\n",
        "\n",
        "    # Compare F1 scores across kernels\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    x = np.arange(3)\n",
        "    width = 0.25\n",
        "\n",
        "    # Extract metrics for plotting\n",
        "    train_f1 = [results[k]['train']['f1'] for k in ['linear', 'poly', 'rbf']]\n",
        "    val_f1 = [results[k]['val']['f1'] for k in ['linear', 'poly', 'rbf']]\n",
        "    test_f1 = [results[k]['test']['f1'] for k in ['linear', 'poly', 'rbf']]\n",
        "\n",
        "    # Plot bars\n",
        "    plt.bar(x - width, train_f1, width, label='Training')\n",
        "    plt.bar(x, val_f1, width, label='Validation')\n",
        "    plt.bar(x + width, test_f1, width, label='Test')\n",
        "\n",
        "    plt.xlabel('Kernel')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.title('F1 Score Comparison Across Different Kernels')\n",
        "    plt.xticks(x, ['Linear', 'Polynomial', 'RBF'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend()\n",
        "    plt.grid(True, axis='y')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('kernel_comparison_f1.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Compare accuracy across kernels\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Extract metrics for plotting\n",
        "    train_acc = [results[k]['train']['accuracy'] for k in ['linear', 'poly', 'rbf']]\n",
        "    val_acc = [results[k]['val']['accuracy'] for k in ['linear', 'poly', 'rbf']]\n",
        "    test_acc = [results[k]['test']['accuracy'] for k in ['linear', 'poly', 'rbf']]\n",
        "\n",
        "    # Plot bars\n",
        "    plt.bar(x - width, train_acc, width, label='Training')\n",
        "    plt.bar(x, val_acc, width, label='Validation')\n",
        "    plt.bar(x + width, test_acc, width, label='Test')\n",
        "\n",
        "    plt.xlabel('Kernel')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy Comparison Across Different Kernels')\n",
        "    plt.xticks(x, ['Linear', 'Polynomial', 'RBF'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend()\n",
        "    plt.grid(True, axis='y')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('kernel_comparison_accuracy.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Determine best overall model based on validation F1 score\n",
        "    best_kernel = max(val_f1_scores, key=val_f1_scores.get)\n",
        "    print(f\"\\nBest overall kernel based on validation F1 score: {best_kernel.upper()}\")\n",
        "    print(f\"Best hyperparameters: {best_params[best_kernel]}\")\n",
        "\n",
        "    return results, best_models[best_kernel], best_kernel, best_params"
      ],
      "metadata": {
        "id": "aW9WlUYq0YKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tried Running AASIST"
      ],
      "metadata": {
        "id": "Kg3DZxDq7Lr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oops....I couldnt run it..."
      ],
      "metadata": {
        "id": "9l5HKOGI7m0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.config.list_physical_devices('GPU'))\n"
      ],
      "metadata": {
        "id": "dF9lw5uh7TpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "uQajoJyc7WmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s /content/drive/MyDrive/aasist /content/aasist"
      ],
      "metadata": {
        "id": "S5YSGbds7YUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/clovaai/aasist.git\n",
        "%cd aasist\n"
      ],
      "metadata": {
        "id": "OBMKzLf67ZZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "mCdipVb67a5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy==1.23.5 torch==2.0.1 tensorflow==2.12.0\n"
      ],
      "metadata": {
        "id": "UjeZm2aV7dD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=MODEL_ID', 'aasist.pth')\n",
        "\n",
        "# !wget https://github.com/clovaai/aasist/releases/download/v1.0/model_weights.pth\n"
      ],
      "metadata": {
        "id": "FUvIzoOm7fJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from models import AASIST\n",
        "\n",
        "model = AASIST()\n",
        "model.load_state_dict(torch.load('model_weights.pth'))\n",
        "model.to('cuda').eval()\n"
      ],
      "metadata": {
        "id": "gUB2WVNY7jRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN (Bholendra Tripathi )"
      ],
      "metadata": {
        "id": "bWztaiKj7sk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "# Drop non-numeric and irrelevant columns\n",
        "df = df.drop(columns=['id', 'filename', 'path'], errors='ignore')\n",
        "\n",
        "# Separate features and target\n",
        "y = df['label']\n",
        "X = df.drop(columns=['label'])\n",
        "\n",
        "# Keep only numeric columns\n",
        "X = X.select_dtypes(include='number')\n",
        "\n",
        "# Train on the entire dataset\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X, y)\n",
        "\n",
        "# Predict on the same full dataset\n",
        "y_pred = knn.predict(X)\n",
        "\n",
        "# Evaluation\n",
        "print(\"KNN Evaluation on Entire Dataset:\")\n",
        "print(\"Accuracy: \", round(accuracy_score(y, y_pred), 4))\n",
        "print(\"Precision:\", round(precision_score(y, y_pred, zero_division=0), 4))\n",
        "print(\"Recall:   \", round(recall_score(y, y_pred), 4))\n",
        "print(\"F1 Score: \", round(f1_score(y, y_pred), 4))\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y, y_pred)\n",
        "print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
        "plt.title('Confusion Matrix (All Samples)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7StQxWnQ7rQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM (Bholendra Tripathi)"
      ],
      "metadata": {
        "id": "pX72fNRY_LnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM without Data modification"
      ],
      "metadata": {
        "id": "wBmLO0jM_hes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Drop non-numeric columns (except for 'label')\n",
        "numeric_df = df.select_dtypes(include=[\"number\"])\n",
        "X = numeric_df.drop(columns=[\"label\"], errors='ignore')\n",
        "y = df[\"label\"]  # Use the original label column\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define kernels\n",
        "kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
        "\n",
        "# Train and evaluate SVM for each kernel\n",
        "for kernel in kernels:\n",
        "    print(f\"\\nSVM with {kernel} kernel:\")\n",
        "    model = SVC(kernel=kernel)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, zero_division=1)\n",
        "    recall = recall_score(y_test, y_pred, zero_division=1)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=1)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "DcgUNeJc_kkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM with modified Data"
      ],
      "metadata": {
        "id": "Tqfp6elP_chz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, roc_curve, auc, log_loss, hinge_loss\n",
        ")\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from scipy.special import expit  # sigmoid\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Drop non-numeric columns (except for 'label')\n",
        "numeric_df = df.select_dtypes(include=[\"number\"])\n",
        "X = numeric_df.drop(columns=[\"label\"], errors='ignore')\n",
        "y = df[\"label\"]\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Dictionary to store results\n",
        "results_dict = {}\n",
        "\n",
        "# Define \"kernels\" via loss functions\n",
        "losses = {\n",
        "    \"hinge (SVM)\": \"hinge\",\n",
        "    \"logistic\": \"log_loss\",\n",
        "    \"modified_huber\": \"modified_huber\",\n",
        "    \"squared_hinge\": \"squared_hinge\"\n",
        "}\n",
        "\n",
        "def train_plot_evaluate(X_train, X_test, y_train, y_test, technique):\n",
        "    kernel_scores = []\n",
        "\n",
        "    for label, loss_func in losses.items():\n",
        "        print(f\"\\n[{technique}] Training with loss: {label}\")\n",
        "        clf = SGDClassifier(loss=loss_func, max_iter=1, tol=None, warm_start=True,\n",
        "                            learning_rate='optimal', random_state=42)\n",
        "\n",
        "        train_loss = []\n",
        "        train_acc = []\n",
        "\n",
        "        for epoch in range(50):  # Simulate epochs\n",
        "            clf.fit(X_train, y_train)\n",
        "            pred_train = clf.predict(X_train)\n",
        "            acc = accuracy_score(y_train, pred_train)\n",
        "\n",
        "            if loss_func in ['log_loss', 'modified_huber']:\n",
        "                probs = expit(clf.decision_function(X_train))\n",
        "                loss = log_loss(y_train, probs)\n",
        "            else:\n",
        "                scores = clf.decision_function(X_train)\n",
        "                loss = hinge_loss(y_train, scores)\n",
        "\n",
        "            train_loss.append(loss)\n",
        "            train_acc.append(acc)\n",
        "\n",
        "        # Plot Loss vs Epoch\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_loss, label='Loss')\n",
        "        plt.title(f\"Loss vs Epoch [{label}]\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.grid()\n",
        "\n",
        "        # Plot Accuracy vs Epoch\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(train_acc, label='Accuracy', color='green')\n",
        "        plt.title(f\"Accuracy vs Epoch [{label}]\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.grid()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Final Evaluation\n",
        "        y_pred = clf.predict(X_test)\n",
        "        y_score = clf.decision_function(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, zero_division=1)\n",
        "        recall = recall_score(y_test, y_pred, zero_division=1)\n",
        "        f1 = f1_score(y_test, y_pred, zero_division=1)\n",
        "\n",
        "        kernel_scores.append((label, accuracy, precision, recall, f1))\n",
        "\n",
        "        # Confusion Matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title(f\"Confusion Matrix [{label}]\")\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"Actual\")\n",
        "        plt.show()\n",
        "\n",
        "        # ROC Curve\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f\"{label} AUC={roc_auc:.2f}\")\n",
        "\n",
        "    # ROC Summary Plot\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(f\"ROC Curve [{technique}]\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "    results_dict[technique] = kernel_scores\n",
        "\n",
        "# ==== 1. OVERSAMPLING ====\n",
        "print(\"\\n===== OVERSAMPLING =====\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "X_train_os, y_train_os = SMOTE().fit_resample(X_train, y_train)\n",
        "train_plot_evaluate(X_train_os, X_test, y_train_os, y_test, \"Oversampling\")\n",
        "\n",
        "# ==== 2. UNDER-SAMPLING ====\n",
        "print(\"\\n===== UNDER-SAMPLING =====\")\n",
        "X_train_us, y_train_us = RandomUnderSampler().fit_resample(X_train, y_train)\n",
        "train_plot_evaluate(X_train_us, X_test, y_train_us, y_test, \"Under-sampling\")\n",
        "\n",
        "# ==== 3. CLASS WEIGHT ====\n",
        "print(\"\\n===== CLASS WEIGHT BIASING =====\")\n",
        "train_plot_evaluate(X_train, X_test, y_train, y_test, \"ClassWeight\")\n",
        "\n",
        "# ==== 4. PCA ====\n",
        "for n_comp in [50, 100, 200]:\n",
        "    if n_comp >= X.shape[1]:\n",
        "        continue\n",
        "    print(f\"\\n===== PCA ({n_comp}) =====\")\n",
        "    pca = PCA(n_components=n_comp)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "    train_plot_evaluate(X_train_pca, X_test_pca, y_train, y_test, f\"PCA_{n_comp}\")\n",
        "\n",
        "# ==== BAR PLOTS ====\n",
        "for technique, values in results_dict.items():\n",
        "    df_bar = pd.DataFrame(values, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
        "    df_bar.set_index(\"Model\", inplace=True)\n",
        "    df_bar.plot(kind=\"bar\", figsize=(10, 6), ylim=(0, 1), title=f\"{technique} Performance\")\n",
        "    plt.grid(True, axis='y')\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "o0WWFYuQ_afh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# Drop non-numeric columns (except for 'label')\n",
        "numeric_df = df.select_dtypes(include=[\"number\"])\n",
        "X = numeric_df.drop(columns=[\"label\"], errors='ignore')\n",
        "y = df[\"label\"]\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, zero_division=1)\n",
        "    recall = recall_score(y_test, y_pred, zero_division=1)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=1)\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Define kernels\n",
        "kernels = [\"linear\"]\n",
        "\n",
        "# ---- 1. Oversampling using SMOTE ----\n",
        "print(\"\\n------ 1. Oversampling using SMOTE ------\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "for kernel in kernels:\n",
        "    print(f\"\\nSVM with {kernel} kernel:\")\n",
        "    model = SVC(kernel=kernel)\n",
        "    model.fit(X_train_res, y_train_res)\n",
        "    results = evaluate(model, X_test, y_test)\n",
        "    print(f\"Accuracy: {results[0]:.4f}, Precision: {results[1]:.4f}, Recall: {results[2]:.4f}, F1: {results[3]:.4f}\")\n",
        "\n",
        "# ---- 2. Under-sampling ----\n",
        "print(\"\\n------ 2. Under-sampling ------\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "undersample = RandomUnderSampler(random_state=42)\n",
        "X_train_res, y_train_res = undersample.fit_resample(X_train, y_train)\n",
        "\n",
        "for kernel in kernels:\n",
        "    print(f\"\\nSVM with {kernel} kernel:\")\n",
        "    model = SVC(kernel=kernel)\n",
        "    model.fit(X_train_res, y_train_res)\n",
        "    results = evaluate(model, X_test, y_test)\n",
        "    print(f\"Accuracy: {results[0]:.4f}, Precision: {results[1]:.4f}, Recall: {results[2]:.4f}, F1: {results[3]:.4f}\")\n",
        "\n",
        "# ---- 3. Class Weight Biasing ----\n",
        "print(\"\\n------ 3. Class Weight Biasing ------\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "for kernel in kernels:\n",
        "    print(f\"\\nSVM with {kernel} kernel and class_weight='balanced':\")\n",
        "    model = SVC(kernel=kernel, class_weight='balanced')\n",
        "    model.fit(X_train, y_train)\n",
        "    results = evaluate(model, X_test, y_test)\n",
        "    print(f\"Accuracy: {results[0]:.4f}, Precision: {results[1]:.4f}, Recall: {results[2]:.4f}, F1: {results[3]:.4f}\")\n",
        "\n",
        "# ---- 4. PCA ----\n",
        "print(\"\\n------ 4. PCA (n_components = 50, 100, 200) ------\")\n",
        "pca_components = [50, 100, 200]\n",
        "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "for n_comp in pca_components:\n",
        "    print(f\"\\n-- PCA with {n_comp} components --\")\n",
        "    try:\n",
        "        pca = PCA(n_components=n_comp)\n",
        "        X_train_pca = pca.fit_transform(X_train_full)\n",
        "        X_test_pca = pca.transform(X_test_full)\n",
        "    except ValueError:\n",
        "        print(f\"Skipped: PCA({n_comp}) > feature count\")\n",
        "        continue\n",
        "\n",
        "    for kernel in kernels:\n",
        "        print(f\"\\nSVM with {kernel} kernel on PCA-reduced data:\")\n",
        "        model = SVC(kernel=kernel)\n",
        "        model.fit(X_train_pca, y_train_full)\n",
        "        results = evaluate(model, X_test_pca, y_test_full)\n",
        "        print(f\"Accuracy: {results[0]:.4f}, Precision: {results[1]:.4f}, Recall: {results[2]:.4f}, F1: {results[3]:.4f}\")\n"
      ],
      "metadata": {
        "id": "Z7df6Pqc_q4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FADWo5V7_KbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdaBoost (Sailaja)"
      ],
      "metadata": {
        "id": "mtjyfNcnAFiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix, roc_curve, auc,precision_recall_curve\n",
        "\n",
        "\n",
        "# In[6]:\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "val_df = pd.read_csv(\"dev.csv\")\n",
        "test_df = pd.read_csv(\"eval.csv\")\n",
        "\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "X_train = train_df.iloc[:, 2:].values\n",
        "y_train = train_df[\"label\"].values\n",
        "\n",
        "X_val = val_df.iloc[:, 2:].values\n",
        "y_val = val_df[\"label\"].values\n",
        "\n",
        "X_test = test_df.iloc[:, 2:].values\n",
        "y_test = test_df[\"label\"].values\n",
        "\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "model = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=3),  # directly inside\n",
        "    n_estimators=50,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "\n",
        "y_train_pred = model.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "train_recall = recall_score(y_train, y_train_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred)\n",
        "y_train_probs = model.predict_proba(X_train)[:, 1]\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "train_eer = compute_eer(y_train, y_train_probs)\n",
        "\n",
        "print(\"\\nTraining Results:\")\n",
        "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"F1 Score: {train_f1:.4f}\")\n",
        "print(f\"Recall Score: {train_recall:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_train, y_train_pred))\n",
        "print(f\"Equal Error Rate: {train_eer:.4f}\")\n",
        "\n",
        "\n",
        "# In[10]:\n",
        "\n",
        "\n",
        "#####VALIDATION SET\n",
        "y_val_pred = model.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_recall=recall_score(y_val,y_val_pred)\n",
        "val_f1 = f1_score(y_val, y_val_pred)\n",
        "y_probs = model.predict_proba(X_val)[:, 1]\n",
        "def compute_eer(y_true, y_scores):\n",
        "  fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "  fnr = 1 - tpr\n",
        "  eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "  eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "  return eer\n",
        "print(\"Validation Results:\")\n",
        "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"F1 Score: {val_f1:.4f}\")\n",
        "print(f\"Recall Score: {val_recall:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_val, y_val_pred))\n",
        "eer = compute_eer(y_val, y_probs)\n",
        "print(f\"Equal Error Rate: {eer:.4f}\")\n",
        "\n",
        "\n",
        "# In[11]:\n",
        "\n",
        "\n",
        "y_test_pred = model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_f1 = f1_score(y_test, y_test_pred)\n",
        "test_recall=recall_score(y_test,y_test_pred)\n",
        "y_probs_test = model.predict_proba(X_test)[:, 1]\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "\n",
        "print(\"\\nTest (Evaluation) Results:\")\n",
        "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"F1 Score: {test_f1:.4f}\")\n",
        "print(f\"Recall Score: {test_recall:.4f}\")\n",
        "eer_test = compute_eer(y_test, y_probs_test)\n",
        "print(f\"Equal Error Rate: {eer_test:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_test_pred))\n",
        "\n",
        "\n",
        "# In[12]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    threshold = thresholds[eer_index]\n",
        "    return eer, fpr, tpr, threshold\n",
        "\n",
        "def plot_graphs_only(model, X, y, set_name, row, fig):\n",
        "    y_pred = model.predict(X)\n",
        "    y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    eer, fpr, tpr, eer_threshold = compute_eer(y, y_probs)\n",
        "    precision, recall, _ = precision_recall_curve(y, y_probs)\n",
        "\n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = fig.add_subplot(3, 3, 3*row + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1, cbar=False)\n",
        "    ax1.set_title(f\"{set_name} - Confusion Matrix\")\n",
        "    ax1.set_xlabel(\"Predicted\")\n",
        "    ax1.set_ylabel(\"Actual\")\n",
        "\n",
        "    # Plot 2: ROC Curve\n",
        "    ax2 = fig.add_subplot(3, 3, 3*row + 2)\n",
        "    ax2.plot(fpr, tpr, label=\"ROC Curve\")\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
        "    ax2.axvline(eer, color='red', linestyle='--', label=f\"EER = {eer:.4f}\")\n",
        "    ax2.set_title(f\"{set_name} - ROC Curve\")\n",
        "    ax2.set_xlabel(\"False Positive Rate\")\n",
        "    ax2.set_ylabel(\"True Positive Rate\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 3: Precision-Recall Curve\n",
        "    ax3 = fig.add_subplot(3, 3, 3*row + 3)\n",
        "    ax3.plot(recall, precision, label=\"PR Curve\")\n",
        "    ax3.set_title(f\"{set_name} - Precision-Recall Curve\")\n",
        "    ax3.set_xlabel(\"Recall\")\n",
        "    ax3.set_ylabel(\"Precision\")\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "plot_graphs_only(model, X_train, y_train, \"Training (Oversampled)\", 0, fig)\n",
        "plot_graphs_only(model, X_val, y_val, \"Validation\", 1, fig)\n",
        "plot_graphs_only(model, X_test, y_test, \"Test\", 2, fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[15]:\n",
        "\n",
        "\n",
        "########################SMOTING\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "OVERtrain_balanced_df = pd.DataFrame(X_train_resampled)\n",
        "OVERtrain_balanced_df['label'] = y_train_resampled\n",
        "\n",
        "# Check new label distribution\n",
        "print(OVERtrain_balanced_df['label'].value_counts())\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Define AdaBoost with DecisionTree base and max_depth=3\n",
        "over_xg_boost = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=3),\n",
        "    n_estimators=50,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "over_xg_boost.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make predictions\n",
        "over_y_train_pred = over_xg_boost.predict(X_train_resampled)\n",
        "over_y_val_pred = over_xg_boost.predict(X_val)\n",
        "over_y_test_pred = over_xg_boost.predict(X_test)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "def evaluate(X_eval, y_true, y_pred, dataset_name):\n",
        "    from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix, roc_curve\n",
        "    import numpy as np\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    y_score = over_xg_boost.predict_proba(X_eval)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    eer = fpr[np.nanargmin(np.absolute((1 - tpr) - fpr))]\n",
        "\n",
        "    print(f\"{dataset_name} Results:\")\n",
        "    print(\"Accuracy:\", round(acc, 4))\n",
        "    print(\"F1 Score:\", round(f1, 4))\n",
        "    print(\"Recall:\", round(rec, 4))\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    print(\"Equal Error Rate (EER):\", round(eer, 4))\n",
        "    print()\n",
        "\n",
        "\n",
        "evaluate(X_train_resampled, y_train_resampled, over_y_train_pred, \"Training\")\n",
        "evaluate(X_val, y_val, over_y_val_pred, \"Validation\")\n",
        "evaluate(X_test, y_test, over_y_test_pred, \"Test\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    threshold = thresholds[eer_index]\n",
        "    return eer, fpr, tpr, threshold\n",
        "\n",
        "def plot_graphs_only(model, X, y, set_name, row, fig):\n",
        "    y_pred = model.predict(X)\n",
        "    y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    eer, fpr, tpr, eer_threshold = compute_eer(y, y_probs)\n",
        "    precision, recall, _ = precision_recall_curve(y, y_probs)\n",
        "\n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = fig.add_subplot(3, 3, 3*row + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1, cbar=False)\n",
        "    ax1.set_title(f\"{set_name} - Confusion Matrix\")\n",
        "    ax1.set_xlabel(\"Predicted\")\n",
        "    ax1.set_ylabel(\"Actual\")\n",
        "\n",
        "    # Plot 2: ROC Curve\n",
        "    ax2 = fig.add_subplot(3, 3, 3*row + 2)\n",
        "    ax2.plot(fpr, tpr, label=\"ROC Curve\")\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
        "    ax2.axvline(eer, color='red', linestyle='--', label=f\"EER = {eer:.4f}\")\n",
        "    ax2.set_title(f\"{set_name} - ROC Curve\")\n",
        "    ax2.set_xlabel(\"False Positive Rate\")\n",
        "    ax2.set_ylabel(\"True Positive Rate\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 3: Precision-Recall Curve\n",
        "    ax3 = fig.add_subplot(3, 3, 3*row + 3)\n",
        "    ax3.plot(recall, precision, label=\"PR Curve\")\n",
        "    ax3.set_title(f\"{set_name} - Precision-Recall Curve\")\n",
        "    ax3.set_xlabel(\"Recall\")\n",
        "    ax3.set_ylabel(\"Precision\")\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "plot_graphs_only(over_xg_boost, X_train, y_train, \"Training (Oversampled)\", 0, fig)\n",
        "plot_graphs_only(over_xg_boost, X_val, y_val, \"Validation\", 1, fig)\n",
        "plot_graphs_only(over_xg_boost, X_test, y_test, \"Test\", 2, fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[20]:\n",
        "\n",
        "\n",
        "##################UNDERSAMPLING\n",
        "print(train_df['label'].value_counts())\n",
        "class_0 = train_df[train_df['label'] == 0]\n",
        "class_1 = train_df[train_df['label'] == 1]\n",
        "class_0_undersampled = class_0.sample(n=len(class_1), random_state=42)\n",
        "train_balanced_df = pd.concat([class_0_undersampled, class_1], axis=0)\n",
        "train_balanced_df = train_balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(\"Balanced Training Set:\")\n",
        "print(train_balanced_df['label'].value_counts())\n",
        "\n",
        "\n",
        "# In[21]:\n",
        "\n",
        "\n",
        "X_train_undersampling = train_balanced_df.iloc[:, 2:].values\n",
        "y_train_undersampling = train_balanced_df[\"label\"].values\n",
        "under_xg_boost =AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=3),\n",
        "    n_estimators=50,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "under_xg_boost.fit(X_train_undersampling, y_train_undersampling)\n",
        "\n",
        "\n",
        "# In[22]:\n",
        "\n",
        "\n",
        "under_y_train_pred = under_xg_boost.predict(X_train_undersampling)\n",
        "under_y_val_pred = under_xg_boost.predict(X_val)\n",
        "under_y_test_pred = under_xg_boost.predict(X_test)\n",
        "\n",
        "\n",
        "# In[13]:\n",
        "\n",
        "\n",
        "def evaluate_under(y_true, y_pred, X_data, dataset_name):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fpr, tpr, _ = roc_curve(y_true, under_xg_boost.predict_proba(X_data)[:, 1])\n",
        "    eer = fpr[np.nanargmin(np.absolute((1 - tpr) - fpr))]\n",
        "\n",
        "    print(f\"{dataset_name} Results:\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n {cm}\")\n",
        "    print(f\"Equal Error Rate (EER): {eer:.4f}\\n\")\n",
        "\n",
        "evaluate_under(y_train_undersampling, under_y_train_pred, X_train_undersampling, \"Training\")\n",
        "evaluate_under(y_val, under_y_val_pred, X_val, \"Validation\")\n",
        "evaluate_under(y_test, under_y_test_pred, X_test, \"Test (Evaluation)\")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    threshold = thresholds[eer_index]\n",
        "    return eer, fpr, tpr, threshold\n",
        "\n",
        "def plot_graphs_only(model, X, y, set_name, row, fig):\n",
        "    y_pred = model.predict(X)\n",
        "    y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    eer, fpr, tpr, eer_threshold = compute_eer(y, y_probs)\n",
        "    precision, recall, _ = precision_recall_curve(y, y_probs)\n",
        "\n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = fig.add_subplot(3, 3, 3*row + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1, cbar=False)\n",
        "    ax1.set_title(f\"{set_name} - Confusion Matrix\")\n",
        "    ax1.set_xlabel(\"Predicted\")\n",
        "    ax1.set_ylabel(\"Actual\")\n",
        "\n",
        "    # Plot 2: ROC Curve\n",
        "    ax2 = fig.add_subplot(3, 3, 3*row + 2)\n",
        "    ax2.plot(fpr, tpr, label=\"ROC Curve\")\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
        "    ax2.axvline(eer, color='red', linestyle='--', label=f\"EER = {eer:.4f}\")\n",
        "    ax2.set_title(f\"{set_name} - ROC Curve\")\n",
        "    ax2.set_xlabel(\"False Positive Rate\")\n",
        "    ax2.set_ylabel(\"True Positive Rate\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 3: Precision-Recall Curve\n",
        "    ax3 = fig.add_subplot(3, 3, 3*row + 3)\n",
        "    ax3.plot(recall, precision, label=\"PR Curve\")\n",
        "    ax3.set_title(f\"{set_name} - Precision-Recall Curve\")\n",
        "    ax3.set_xlabel(\"Recall\")\n",
        "    ax3.set_ylabel(\"Precision\")\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "plot_graphs_only(under_xg_boost, X_train_undersampling, y_train_undersampling, \"Training (Oversampled)\", 0, fig)\n",
        "plot_graphs_only(under_xg_boost, X_val, y_val, \"Validation\", 1, fig)\n",
        "plot_graphs_only(under_xg_boost, X_test, y_test, \"Test\", 2, fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[23]:\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "pca = PCA(n_components=50)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_val_pca = pca.transform(X_val_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "\n",
        "model_pca = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=2),\n",
        "    n_estimators=100,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "\n",
        "train_preds= model_pca.predict(X_train_pca)\n",
        "val_preds = model_pca.predict(X_val_pca)\n",
        "test_preds = model_pca.predict(X_test_pca)\n",
        "\n",
        "\n",
        "# In[26]:\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score, recall_score, confusion_matrix, roc_curve\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_pca(y_true, y_pred, X_data, dataset_name):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, model_pca.predict_proba(X_data)[:, 1])\n",
        "    eer = fpr[np.nanargmin(np.absolute((1 - tpr) - fpr))]\n",
        "\n",
        "    print(f\"{dataset_name} Results:\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n {cm}\")\n",
        "    print(f\"Equal Error Rate (EER): {eer:.4f}\\n\")\n",
        "evaluate_pca(y_train, train_preds, X_train_pca, \"Training\")\n",
        "evaluate_pca(y_val, val_preds, X_val_pca, \"Validation\")\n",
        "evaluate_pca(y_test, test_preds, X_test_pca, \"Test (Evaluation)\")\n",
        "\n",
        "\n",
        "# In[27]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def plot_all_graphs(model, X_sets, y_sets, set_names):\n",
        "    num_sets = len(X_sets)\n",
        "    fig, axes = plt.subplots(nrows=num_sets, ncols=3, figsize=(18, 4 * num_sets))\n",
        "\n",
        "    for i, (X, y, name) in enumerate(zip(X_sets, y_sets, set_names)):\n",
        "        y_pred = model.predict(X)\n",
        "        y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "        # 1. Actual Labels\n",
        "        sns.countplot(x=y, hue=y, palette='cool', ax=axes[i, 0], legend=False)\n",
        "        axes[i, 0].set_title(f'{name} - Actual Labels')\n",
        "        axes[i, 0].set_xlabel('Class')\n",
        "        axes[i, 0].set_ylabel('Count')\n",
        "\n",
        "        # 2. Predicted Labels\n",
        "        sns.countplot(x=y_pred, hue=y_pred, palette='magma', ax=axes[i, 1], legend=False)\n",
        "        axes[i, 1].set_title(f'{name} - Predicted Labels')\n",
        "        axes[i, 1].set_xlabel('Class')\n",
        "        axes[i, 1].set_ylabel('Count')\n",
        "\n",
        "        # 3. Scatter Plot\n",
        "        sns.scatterplot(x=np.arange(len(y_probs)), y=y_probs, hue=y, palette='Set2', alpha=0.7, ax=axes[i, 2])\n",
        "        axes[i, 2].set_title(f'{name} - Probabilities vs True Labels')\n",
        "        axes[i, 2].set_xlabel('Sample Index')\n",
        "        axes[i, 2].set_ylabel('Predicted Probability')\n",
        "        axes[i, 2].legend(title='True Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example call\n",
        "plot_all_graphs(\n",
        "    model,\n",
        "    [X_train, X_val, X_test],\n",
        "    [y_train, y_val, y_test],\n",
        "    [\"Training\", \"Validation\", \"Test\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[28]:\n",
        "\n",
        "\n",
        "#############SOMTING\n",
        "plot_all_graphs(\n",
        "    over_xg_boost,\n",
        "    [X_train_resampled, X_val, X_test],\n",
        "    [y_train_resampled, y_val, y_test],\n",
        "    [\"Training\", \"Validation\", \"Test\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[29]:\n",
        "\n",
        "\n",
        "########UNDESAMPLING\n",
        "plot_all_graphs(\n",
        "    under_xg_boost,\n",
        "    [X_train_undersampling, X_val, X_test],\n",
        "    [y_train_undersampling, y_val, y_test],\n",
        "    [\"Training\", \"Validation\", \"Test\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[30]:\n",
        "\n",
        "\n",
        "##########PCA\n",
        "plot_all_graphs(\n",
        "    model_pca,\n",
        "    [X_train_pca, X_val_pca, X_test_pca],\n",
        "    [y_train, y_val, y_test],\n",
        "    [\"Training\", \"Validation\", \"Test\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[15]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrices(y_tr, y_tr_pred, y_val, y_val_pred, y_test, y_test_pred):\n",
        "    titles = [\"Training Set\", \"Validation Set\", \"Test Set\"]\n",
        "    datasets = [(y_tr, y_tr_pred), (y_val, y_val_pred), (y_test, y_test_pred)]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    for ax, (y_true, y_pred), title in zip(axes, datasets, titles):\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False, ax=ax)\n",
        "        ax.set_title(f\"Confusion Matrix - {title}\")\n",
        "        ax.set_xlabel(\"Predicted\")\n",
        "        ax.set_ylabel(\"Actual\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# In[32]:\n",
        "\n",
        "\n",
        "########MODEL\n",
        "plot_confusion_matrices(y_train, y_train_pred, y_val, y_val_pred, y_test, y_test_pred)\n",
        "\n",
        "\n",
        "# In[33]:\n",
        "\n",
        "\n",
        "plot_confusion_matrices(y_train_resampled, over_y_train_pred, y_val, over_y_val_pred, y_test, over_y_test_pred)\n",
        "\n",
        "\n",
        "# In[16]:\n",
        "\n",
        "\n",
        "plot_confusion_matrices(y_train_undersampling, under_y_train_pred, y_val, under_y_val_pred, y_test, under_y_test_pred)\n",
        "\n",
        "\n",
        "# In[35]:\n",
        "\n",
        "\n",
        "plot_confusion_matrices(y_train, train_preds, y_val, val_preds, y_test, test_preds)\n",
        "\n",
        "\n",
        "# In[36]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Techniques and their test accuracies\n",
        "techniques = [\"Original\", \"Oversampling\", \"Undersampling\", \"PCA\"]\n",
        "accuracies = [0.82, 0.851, 0.78, 0.80]\n",
        "\n",
        "x = np.arange(len(techniques))\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(x, accuracies, width=0.5, color=[\"skyblue\", \"lightgreen\", \"salmon\", \"orange\"])\n",
        "\n",
        "# Labeling\n",
        "plt.ylabel(\"Test Accuracy\")\n",
        "plt.title(\"Test Accuracy vs Data Technique\")\n",
        "plt.xticks(x, techniques)\n",
        "plt.ylim(0.7, 0.9)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[17]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the confusion matrix for each case\n",
        "confusion_matrices = {\n",
        "    \"Original Training\": [[22579, 221], [2260, 320]],\n",
        "    \"Original Validation\": [[21986, 310], [2389, 159]],\n",
        "    \"Original Test\": [[63882, 0], [7355, 0]],\n",
        "\n",
        "    \"Oversampling Training\": [[13708, 9092], [2563, 20237]],\n",
        "    \"Oversampling Validation\": [[13488, 8808], [761, 1787]],\n",
        "    \"Oversampling Test\": [[63716, 166], [7353, 2]],\n",
        "\n",
        "    \"Undersampling Training\": [[1724, 856], [585, 1995]],\n",
        "    \"Undersampling Validation\": [[14299, 7997], [789, 1759]],\n",
        "    \"Undersampling Test\": [[63346, 536], [7257, 98]],\n",
        "\n",
        "    \"PCA Reduction Training\": [[22715, 85], [2543, 37]],\n",
        "    \"PCA Reduction validation\": [[22231, 65], [2519, 29]],\n",
        "    \"PCA Reduction Test\": [[63882, 0], [7355, 0]]\n",
        "}\n",
        "\n",
        "\n",
        "# Function to calculate accuracy for label 0 and label 1\n",
        "def calculate_accuracies(conf_matrix):\n",
        "    # Label 0 accuracy = True Negatives / (True Negatives + False Positives)\n",
        "    accuracy_label_0 = conf_matrix[0][0] / (conf_matrix[0][0] + conf_matrix[0][1])\n",
        "    # Label 1 accuracy = True Positives / (True Positives + False Negatives)\n",
        "    accuracy_label_1 = conf_matrix[1][1] / (conf_matrix[1][0] + conf_matrix[1][1])\n",
        "    return accuracy_label_0, accuracy_label_1\n",
        "\n",
        "# Prepare data for plotting\n",
        "labels = list(confusion_matrices.keys())\n",
        "accuracies_label_0 = []\n",
        "accuracies_label_1 = []\n",
        "\n",
        "for label in labels:\n",
        "    conf_matrix = confusion_matrices[label]\n",
        "    acc_label_0, acc_label_1 = calculate_accuracies(conf_matrix)\n",
        "    accuracies_label_0.append(acc_label_0)\n",
        "    accuracies_label_1.append(acc_label_1)\n",
        "\n",
        "# Plotting the accuracies\n",
        "x = np.arange(len(labels))  # Label positions\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "width = 0.35  # Bar width\n",
        "\n",
        "# Set colors for the bars (one color for label 0, another for label 1)\n",
        "color_label_0 = 'skyblue'\n",
        "color_label_1 = 'orange'\n",
        "\n",
        "# Plotting bars for Label 0 and Label 1\n",
        "bars_label_0 = ax.bar(x - width/2, accuracies_label_0, width, label='Accuracy for Label 0', color=color_label_0)\n",
        "bars_label_1 = ax.bar(x + width/2, accuracies_label_1, width, label='Accuracy for Label 1', color=color_label_1)\n",
        "\n",
        "# Labeling and formatting\n",
        "ax.set_xlabel('Model Configuration')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Accuracy for Label 0 and Label 1 in Different Models')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "adaboost = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Updated param_grid for newer versions (use 'estimator__' prefix)\n",
        "param_grid = {\n",
        "    'estimator__max_depth': [3, 5, 6],\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.5],\n",
        "}\n",
        "\n",
        "# Grid search for best parameters\n",
        "grid_search = GridSearchCV(adaboost, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters found by GridSearchCV\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate on test set using best model\n",
        "best_adaboost = grid_search.best_estimator_\n",
        "y_pred = best_adaboost.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy with Best Model: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# In[23]:\n",
        "\n",
        "\n",
        "pip install --upgrade scikit-learn\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E0jYkhRNAI7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boost (Sailaja)"
      ],
      "metadata": {
        "id": "_lIwISC3AZQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix, roc_curve, auc,precision_recall_curve\n",
        "\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "val_df = pd.read_csv(\"dev.csv\")\n",
        "test_df = pd.read_csv(\"eval.csv\")\n",
        "\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "X_train = train_df.iloc[:, 2:].values\n",
        "y_train = train_df[\"label\"].values\n",
        "\n",
        "X_val = val_df.iloc[:, 2:].values\n",
        "y_val = val_df[\"label\"].values\n",
        "\n",
        "X_test = test_df.iloc[:, 2:].values\n",
        "y_test = test_df[\"label\"].values\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "gb_original = GradientBoostingClassifier(n_estimators=40, learning_rate=0.1, max_depth=4, random_state=42)\n",
        "gb_original.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "\n",
        "y_train_pred = gb_original.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "train_recall = recall_score(y_train, y_train_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred)\n",
        "y_train_probs = gb_original.predict_proba(X_train)[:, 1]\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "train_eer = compute_eer(y_train, y_train_probs)\n",
        "\n",
        "print(\"\\nTraining Results:\")\n",
        "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"F1 Score: {train_f1:.4f}\")\n",
        "print(f\"Recall Score: {train_recall:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_train, y_train_pred))\n",
        "print(f\"Equal Error Rate: {train_eer:.4f}\")\n",
        "\n",
        "\n",
        "# In[6]:\n",
        "\n",
        "\n",
        "#####VALIDATION SET\n",
        "y_val_pred = gb_original.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_recall=recall_score(y_val,y_val_pred)\n",
        "val_f1 = f1_score(y_val, y_val_pred)\n",
        "y_probs = gb_original.predict_proba(X_val)[:, 1]\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "print(\"Validation Results:\")\n",
        "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"F1 Score: {val_f1:.4f}\")\n",
        "print(f\"Recall Score: {val_recall:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_val, y_val_pred))\n",
        "eer = compute_eer(y_val, y_probs)\n",
        "print(f\"Equal Error Rate: {eer:.4f}\")\n",
        "\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "y_test_pred = gb_original.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_f1 = f1_score(y_test, y_test_pred)\n",
        "test_recall=recall_score(y_test,y_test_pred)\n",
        "y_probs_test = gb_original.predict_proba(X_test)[:, 1]\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "\n",
        "print(\"\\nTest (Evaluation) Results:\")\n",
        "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"F1 Score: {test_f1:.4f}\")\n",
        "print(f\"Recall Score: {test_recall:.4f}\")\n",
        "eer_test = compute_eer(y_test, y_probs_test)\n",
        "print(f\"Equal Error Rate: {eer_test:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_test_pred))\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[33]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    threshold = thresholds[eer_index]\n",
        "    return eer, fpr, tpr, threshold\n",
        "\n",
        "def plot_graphs_only(model, X, y, set_name, row, fig):\n",
        "    y_pred = model.predict(X)\n",
        "    y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    eer, fpr, tpr, eer_threshold = compute_eer(y, y_probs)\n",
        "    precision, recall, _ = precision_recall_curve(y, y_probs)\n",
        "\n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = fig.add_subplot(3, 3, 3*row + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1, cbar=False)\n",
        "    ax1.set_title(f\"{set_name} - Confusion Matrix\")\n",
        "    ax1.set_xlabel(\"Predicted\")\n",
        "    ax1.set_ylabel(\"Actual\")\n",
        "\n",
        "    # Plot 2: ROC Curve\n",
        "    ax2 = fig.add_subplot(3, 3, 3*row + 2)\n",
        "    ax2.plot(fpr, tpr, label=\"ROC Curve\")\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
        "    ax2.axvline(eer, color='red', linestyle='--', label=f\"EER = {eer:.4f}\")\n",
        "    ax2.set_title(f\"{set_name} - ROC Curve\")\n",
        "    ax2.set_xlabel(\"False Positive Rate\")\n",
        "    ax2.set_ylabel(\"True Positive Rate\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 3: Precision-Recall Curve\n",
        "    ax3 = fig.add_subplot(3, 3, 3*row + 3)\n",
        "    ax3.plot(recall, precision, label=\"PR Curve\")\n",
        "    ax3.set_title(f\"{set_name} - Precision-Recall Curve\")\n",
        "    ax3.set_xlabel(\"Recall\")\n",
        "    ax3.set_ylabel(\"Precision\")\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "\n",
        "\n",
        "plot_graphs_only(gb_original, X_train, y_train, \"Training\", 0, fig)\n",
        "plot_graphs_only(gb_original, X_val, y_val, \"Validation\", 1, fig)\n",
        "plot_graphs_only(gb_original, X_test, y_test, \"Test\", 2, fig)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[42]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def plot_all_graphs(model, X_sets, y_sets, set_names):\n",
        "    num_sets = len(X_sets)\n",
        "    fig, axes = plt.subplots(nrows=num_sets, ncols=3, figsize=(18, 4 * num_sets))\n",
        "\n",
        "    for i, (X, y, name) in enumerate(zip(X_sets, y_sets, set_names)):\n",
        "        y_pred = model.predict(X)\n",
        "        y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "        # 1. Actual Labels\n",
        "        sns.countplot(x=y, hue=y, palette='cool', ax=axes[i, 0], legend=False)\n",
        "        axes[i, 0].set_title(f'{name} - Actual Labels')\n",
        "        axes[i, 0].set_xlabel('Class')\n",
        "        axes[i, 0].set_ylabel('Count')\n",
        "\n",
        "        # 2. Predicted Labels\n",
        "        sns.countplot(x=y_pred, hue=y_pred, palette='magma', ax=axes[i, 1], legend=False)\n",
        "        axes[i, 1].set_title(f'{name} - Predicted Labels')\n",
        "        axes[i, 1].set_xlabel('Class')\n",
        "        axes[i, 1].set_ylabel('Count')\n",
        "\n",
        "        # 3. Scatter Plot\n",
        "        sns.scatterplot(x=np.arange(len(y_probs)), y=y_probs, hue=y, palette='Set2', alpha=0.7, ax=axes[i, 2])\n",
        "        axes[i, 2].set_title(f'{name} - Probabilities vs True Labels')\n",
        "        axes[i, 2].set_xlabel('Sample Index')\n",
        "        axes[i, 2].set_ylabel('Predicted Probability')\n",
        "        axes[i, 2].legend(title='True Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example call\n",
        "plot_all_graphs(\n",
        "    gb_original,\n",
        "    [X_train, X_val, X_test],\n",
        "    [y_train, y_val, y_test],\n",
        "    [\"Training\", \"Validation\", \"Test\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[18]:\n",
        "\n",
        "\n",
        "#####UNDERSAMPLING\n",
        "print(train_df['label'].value_counts())\n",
        "class_0 = train_df[train_df['label'] == 0]\n",
        "class_1 = train_df[train_df['label'] == 1]\n",
        "class_0_undersampled = class_0.sample(n=len(class_1), random_state=42)\n",
        "train_balanced_df = pd.concat([class_0_undersampled, class_1], axis=0)\n",
        "train_balanced_df = train_balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(\"Balanced Training Set:\")\n",
        "print(train_balanced_df['label'].value_counts())\n",
        "\n",
        "\n",
        "# In[20]:\n",
        "\n",
        "\n",
        "X_train_undersampling = train_balanced_df.iloc[:, 2:].values\n",
        "y_train_undersampling = train_balanced_df[\"label\"].values\n",
        "gb_original_undersampling = GradientBoostingClassifier(n_estimators=40, learning_rate=0.1, max_depth=4, random_state=42)\n",
        "gb_original_undersampling.fit(X_train_undersampling, y_train_undersampling)\n",
        "\n",
        "\n",
        "# In[18]:\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, roc_curve\n",
        "import numpy as np\n",
        "\n",
        "under_y_train_pred = gb_original_undersampling.predict(X_train_undersampling)\n",
        "under_y_train_probs = gb_original_undersampling.predict_proba(X_train_undersampling)[:, 1]\n",
        "\n",
        "under_train_accuracy = accuracy_score(y_train_undersampling, under_y_train_pred)\n",
        "under_train_recall = recall_score(y_train_undersampling, under_y_train_pred)\n",
        "under_train_f1 = f1_score(y_train_undersampling, under_y_train_pred)\n",
        "\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "\n",
        "under_train_eer = compute_eer(y_train_undersampling, under_y_train_probs)\n",
        "\n",
        "under_conf_matrix = confusion_matrix(y_train_undersampling, under_y_train_pred)\n",
        "\n",
        "\n",
        "print(\"Under - Training Results on Undersampled Data:\")\n",
        "print(f\"Under - Accuracy: {under_train_accuracy:.4f}\")\n",
        "print(f\"Under - F1 Score: {under_train_f1:.4f}\")\n",
        "print(f\"Under - Recall Score: {under_train_recall:.4f}\")\n",
        "print(\"Under - Confusion Matrix:\")\n",
        "print(under_conf_matrix)\n",
        "print(f\"Under - Equal Error Rate: {under_train_eer:.4f}\")\n",
        "\n",
        "\n",
        "# In[19]:\n",
        "\n",
        "\n",
        "#######VALIDATION SET AFTER UNDERSAMPLING\n",
        "UNDERy_val_pred = gb_original_undersampling.predict(X_val)\n",
        "UNDERval_accuracy = accuracy_score(y_val, UNDERy_val_pred)\n",
        "UNDERval_recall=recall_score(y_val,UNDERy_val_pred)\n",
        "UNDERval_f1 = f1_score(y_val, UNDERy_val_pred)\n",
        "UNDERy_probs = gb_original_undersampling.predict_proba(X_val)[:, 1]\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "print(\"Validation Results:\")\n",
        "print(f\"Accuracy: {UNDERval_accuracy :.4f}\")\n",
        "print(f\"F1 Score: {UNDERval_f1:.4f}\")\n",
        "print(f\"Recall Score: {UNDERval_recall:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_val, UNDERy_val_pred))\n",
        "UNDEReer = compute_eer(y_val, y_probs)\n",
        "print(f\"Equal Error Rate: {UNDEReer:.4f}\")\n",
        "\n",
        "\n",
        "# In[20]:\n",
        "\n",
        "\n",
        "###TEST SET AFTER UNDERSAMPLING\n",
        "UNDERy_test_pred = gb_original_undersampling.predict(X_test)\n",
        "UNDERtest_accuracy = accuracy_score(y_test, UNDERy_test_pred)\n",
        "UNDERtest_f1 = f1_score(y_test, UNDERy_test_pred)\n",
        "UNDERtest_recall=recall_score(y_test,UNDERy_test_pred)\n",
        "UNDERy_probs_test = gb_original_undersampling.predict_proba(X_test)[:, 1]\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "\n",
        "print(\"\\nTest (Evaluation) Results:\")\n",
        "print(f\"Accuracy: {UNDERtest_accuracy:.4f}\")\n",
        "print(f\"F1 Score: {UNDERtest_f1:.4f}\")\n",
        "print(f\"Recall Score: {UNDERtest_recall:.4f}\")\n",
        "UNDEReer_test = compute_eer(y_test, UNDERy_probs_test)\n",
        "print(f\"Equal Error Rate: {UNDEReer_test:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, UNDERy_test_pred))\n",
        "\n",
        "\n",
        "# In[12]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Define confusion matrices\n",
        "conf_matrices = {\n",
        "    \"Training\": np.array([[2121, 459],\n",
        "                          [300,  2280]]),\n",
        "\n",
        "    \"Validation\": np.array([[15096, 7200],\n",
        "                            [768,   1780]]),\n",
        "\n",
        "    \"Test\": np.array([[63683, 199],\n",
        "                      [7316,   39]])\n",
        "}\n",
        "\n",
        "# Set up subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for ax, (title, matrix) in zip(axes, conf_matrices.items()):\n",
        "    sns.heatmap(matrix, annot=True, fmt='d', cmap='coolwarm', ax=ax)\n",
        "    ax.set_title(f\"{title} Set\")\n",
        "    ax.set_xlabel(\"Predicted Label\")\n",
        "    ax.set_ylabel(\"True Label\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[35]:\n",
        "\n",
        "\n",
        "# Create the figure\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "\n",
        "# Add all sets\n",
        "plot_graphs_only(gb_original_undersampling, X_train_undersampling, y_train_undersampling, \"Training\", 0, fig)\n",
        "plot_graphs_only(gb_original_undersampling, X_val, y_val, \"Validation\", 1, fig)\n",
        "plot_graphs_only(gb_original_undersampling, X_test, y_test, \"Test\", 2, fig)\n",
        "# Layout and show\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[43]:\n",
        "\n",
        "\n",
        "plot_all_graphs(\n",
        "    gb_original_undersampling,\n",
        "    [X_train_undersampling, X_val, X_test],\n",
        "    [y_train_undersampling, y_val, y_test],\n",
        "    [\"Training\", \"Validation\", \"Test\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[44]:\n",
        "\n",
        "\n",
        "####OVERSAMPLING\n",
        "\n",
        "print(\"Original Class Distribution:\")\n",
        "print(train_df['label'].value_counts())\n",
        "\n",
        "class_0 = train_df[train_df['label'] == 0]\n",
        "class_1 = train_df[train_df['label'] == 1]\n",
        "\n",
        "class_1_oversampled = class_1.sample(n=len(class_0), replace=True, random_state=42)\n",
        "\n",
        "OVERtrain_balanced_df = pd.concat([class_0, class_1_oversampled], axis=0)\n",
        "\n",
        "# Shuffle the combined dataset\n",
        "OVERtrain_balanced_df = OVERtrain_balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Show the new class distribution\n",
        "print(\"Balanced Training Set After Oversampling:\")\n",
        "print(OVERtrain_balanced_df['label'].value_counts())\n",
        "\n",
        "\n",
        "# In[45]:\n",
        "\n",
        "\n",
        "X_train_oversampling = OVERtrain_balanced_df.iloc[:, 2:].values\n",
        "y_train_oversampling = OVERtrain_balanced_df[\"label\"].values\n",
        "gb_oversampling = GradientBoostingClassifier(n_estimators=30, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_oversampling.fit(X_train_oversampling, y_train_oversampling)\n",
        "\n",
        "\n",
        "# In[46]:\n",
        "\n",
        "\n",
        "######VALIDATION METRICS AFTER OVERSAMPLING\n",
        "OVERy_val_pred = gb_oversampling.predict(X_val)\n",
        "OVERval_accuracy = accuracy_score(y_val, OVERy_val_pred)\n",
        "OVERval_recall=recall_score(y_val,OVERy_val_pred)\n",
        "OVERval_f1 = f1_score(y_val, OVERy_val_pred)\n",
        "OVERy_probs = gb_oversampling.predict_proba(X_val)[:, 1]\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "print(\"Validation Results:\")\n",
        "print(f\"Accuracy: {OVERval_accuracy :.4f}\")\n",
        "print(f\"F1 Score: {OVERval_f1:.4f}\")\n",
        "print(f\"Recall Score: {OVERval_recall:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_val, OVERy_val_pred))\n",
        "OVEReer = compute_eer(y_val, OVERy_probs)\n",
        "print(f\"Equal Error Rate: {OVEReer:.4f}\")\n",
        "\n",
        "\n",
        "# In[47]:\n",
        "\n",
        "\n",
        "###TEST SET AFTER OVERSAMPLING\n",
        "OVERy_test_pred = gb_oversampling.predict(X_test)\n",
        "OVERtest_accuracy = accuracy_score(y_test, OVERy_test_pred)\n",
        "OVERtest_f1 = f1_score(y_test,OVERy_test_pred)\n",
        "OVERtest_recall=recall_score(y_test,OVERy_test_pred)\n",
        "OVERy_probs_test = gb_oversampling.predict_proba(X_test)[:, 1]\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "\n",
        "print(\"\\nTest (Evaluation) Results:\")\n",
        "print(f\"Accuracy: {OVERtest_accuracy:.4f}\")\n",
        "print(f\"F1 Score: {OVERtest_f1:.4f}\")\n",
        "print(f\"Recall Score: {OVERtest_recall:.4f}\")\n",
        "OVEReer_test = compute_eer(y_test, OVERy_probs_test)\n",
        "print(f\"Equal Error Rate: {OVEReer_test:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, OVERy_test_pred))\n",
        "\n",
        "\n",
        "# In[48]:\n",
        "\n",
        "\n",
        "plot_all_graphs(\n",
        "    gb_oversampling,\n",
        "    [X_train_oversampling, X_val, X_test],\n",
        "    [y_train_oversampling, y_val, y_test],\n",
        "    [\"Training\", \"Validation\", \"Test\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[52]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    threshold = thresholds[eer_index]\n",
        "    return eer, fpr, tpr, threshold\n",
        "\n",
        "def plot_graphs_only(model, X, y, set_name, row, fig):\n",
        "    y_pred = model.predict(X)\n",
        "    y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    eer, fpr, tpr, eer_threshold = compute_eer(y, y_probs)\n",
        "    precision, recall, _ = precision_recall_curve(y, y_probs)\n",
        "\n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = fig.add_subplot(3, 3, 3*row + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1, cbar=False)\n",
        "    ax1.set_title(f\"{set_name} - Confusion Matrix\")\n",
        "    ax1.set_xlabel(\"Predicted\")\n",
        "    ax1.set_ylabel(\"Actual\")\n",
        "\n",
        "    # Plot 2: ROC Curve\n",
        "    ax2 = fig.add_subplot(3, 3, 3*row + 2)\n",
        "    ax2.plot(fpr, tpr, label=\"ROC Curve\")\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
        "    ax2.axvline(eer, color='red', linestyle='--', label=f\"EER = {eer:.4f}\")\n",
        "    ax2.set_title(f\"{set_name} - ROC Curve\")\n",
        "    ax2.set_xlabel(\"False Positive Rate\")\n",
        "    ax2.set_ylabel(\"True Positive Rate\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 3: Precision-Recall Curve\n",
        "    ax3 = fig.add_subplot(3, 3, 3*row + 3)\n",
        "    ax3.plot(recall, precision, label=\"PR Curve\")\n",
        "    ax3.set_title(f\"{set_name} - Precision-Recall Curve\")\n",
        "    ax3.set_xlabel(\"Recall\")\n",
        "    ax3.set_ylabel(\"Precision\")\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "plot_graphs_only(gb_oversampling, X_train_oversampling, y_train_oversampling, \"Training (Oversampled)\", 0, fig)\n",
        "plot_graphs_only(gb_oversampling, X_val, y_val, \"Validation\", 1, fig)\n",
        "plot_graphs_only(gb_oversampling, X_test, y_test, \"Test\", 2, fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "confusion_matrices = {\n",
        "    \"Original Training\": [[22800, 0], [2557, 23]],\n",
        "    \"Original Validation\": [[22295, 1], [2545, 3]],\n",
        "    \"Original Test\": [[63033, 849], [7125, 230]],\n",
        "\n",
        "    \"Undersampling Training\": [[2121, 459], [300, 2280]],\n",
        "    \"Undersampling Validation\": [[15096, 7200], [768, 1780]],\n",
        "    \"Undersampling Test\": [[63683, 199], [7316, 39]],\n",
        "\n",
        "    \"Oversampling Training\": [[16408, 6392], [3422, 19378]],\n",
        "    \"Oversampling Validation\": [[16054, 6242], [1031, 1517]],\n",
        "    \"Oversampling Test\": [[63877, 5], [7354, 1]],\n",
        "\n",
        "    \"Class Weighting Training\": [[15481, 7319], [598, 1982]],\n",
        "    \"Class Weighting Validation\": [[15263, 7033], [905, 1643]],\n",
        "    \"Class Weighting Test\": [[63815, 67], [7342, 13]]\n",
        "}\n",
        "\n",
        "\n",
        "def calculate_accuracies(conf_matrix):\n",
        "    # Label 0 accuracy = True Negatives / (True Negatives + False Positives)\n",
        "    accuracy_label_0 = conf_matrix[0][0] / (conf_matrix[0][0] + conf_matrix[0][1])\n",
        "    # Label 1 accuracy = True Positives / (True Positives + False Negatives)\n",
        "    accuracy_label_1 = conf_matrix[1][1] / (conf_matrix[1][0] + conf_matrix[1][1])\n",
        "    return accuracy_label_0, accuracy_label_1\n",
        "\n",
        "\n",
        "labels = list(confusion_matrices.keys())\n",
        "accuracies_label_0 = []\n",
        "accuracies_label_1 = []\n",
        "\n",
        "for label in labels:\n",
        "    conf_matrix = confusion_matrices[label]\n",
        "    acc_label_0, acc_label_1 = calculate_accuracies(conf_matrix)\n",
        "    accuracies_label_0.append(acc_label_0)\n",
        "    accuracies_label_1.append(acc_label_1)\n",
        "\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "width = 0.35\n",
        "\n",
        "\n",
        "color_label_0 = 'skyblue'\n",
        "color_label_1 = 'orange'\n",
        "\n",
        "\n",
        "bars_label_0 = ax.bar(x - width/2, accuracies_label_0, width, label='Accuracy for Label 0', color=color_label_0)\n",
        "bars_label_1 = ax.bar(x + width/2, accuracies_label_1, width, label='Accuracy for Label 1', color=color_label_1)\n",
        "\n",
        "\n",
        "ax.set_xlabel('Model Configuration')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Accuracy for Label 0 and Label 1 in Different Models')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrices(y_tr, y_tr_pred, y_val, y_val_pred, y_test, y_test_pred):\n",
        "    titles = [\"Training Set\", \"Validation Set\", \"Test Set\"]\n",
        "    datasets = [(y_tr, y_tr_pred), (y_val, y_val_pred), (y_test, y_test_pred)]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    for ax, (y_true, y_pred), title in zip(axes, datasets, titles):\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False, ax=ax)\n",
        "        ax.set_title(f\"Confusion Matrix - {title}\")\n",
        "        ax.set_xlabel(\"Predicted\")\n",
        "        ax.set_ylabel(\"Actual\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot_confusion_matrices(y_train, y_train_pred, y_val, y_val_pred, y_test, y_test_pred)\n",
        "\n",
        "\n",
        "# In[17]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Define confusion matrices\n",
        "conf_matrices = {\n",
        "    \"class Weighting Training\": np.array([[15481, 7319], [598, 1982]],),\n",
        "    \"class Weighting Validation\": np.array( [[15263, 7033], [905, 1643]]),\n",
        "    \"class Weighting Test\": np.array([[63815, 67], [7342, 13]])\n",
        "}\n",
        "\n",
        "# Plot each confusion matrix\n",
        "plt.figure(figsize=(18, 5))\n",
        "for i, (title, matrix) in enumerate(conf_matrices.items(), 1):\n",
        "    plt.subplot(1, 3, i)\n",
        "    sns.heatmap(matrix, annot=True, fmt='d', cmap='coolwarm', cbar=False,\n",
        "                xticklabels=['Pred 0', 'Pred 1'], yticklabels=['True 0', 'True 1'])\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\"Confusion Matrices After Class weighting\", fontsize=16, y=1.05)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[22]:\n",
        "\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix, roc_curve\n",
        "import numpy as np\n",
        "\n",
        "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "\n",
        "# Train the model with sample weights\n",
        "gb_weighted = GradientBoostingClassifier(n_estimators=30, learning_rate=0.1, max_depth=3, random_state=4)\n",
        "gb_weighted.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = gb_weighted.predict(X_train)\n",
        "y_val_pred = gb_weighted.predict(X_val)\n",
        "y_test_pred = gb_weighted.predict(X_test)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(y_true, y_pred, dataset_name):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fpr, tpr, _ = roc_curve(y_true, gb_weighted.predict_proba(X_val)[:,1])\n",
        "    eer = fpr[np.nanargmin(np.absolute((1 - tpr) - fpr))]\n",
        "\n",
        "    print(f\"{dataset_name} Results:\")\n",
        "    print(\"Accuracy:\", round(acc, 4))\n",
        "    print(\"F1 Score:\", round(f1, 4))\n",
        "    print(\"Recall:\", round(rec, 4))\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    print(\"Equal Error Rate (EER):\", round(eer, 4))\n",
        "    print()\n",
        "\n",
        "\n",
        "# In[23]:\n",
        "\n",
        "\n",
        "def evaluate(X_eval, y_true, y_pred, dataset_name):\n",
        "    from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix, roc_curve\n",
        "    import numpy as np\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    y_score = gb_weighted.predict_proba(X_eval)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
        "    eer = fpr[np.nanargmin(np.absolute((1 - tpr) - fpr))]\n",
        "\n",
        "    print(f\"{dataset_name} Results:\")\n",
        "    print(\"Accuracy:\", round(acc, 4))\n",
        "    print(\"F1 Score:\", round(f1, 4))\n",
        "    print(\"Recall:\", round(rec, 4))\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    print(\"Equal Error Rate (EER):\", round(eer, 4))\n",
        "    print()\n",
        "\n",
        "# Evaluate\n",
        "evaluate(X_train, y_train, y_train_pred, \"Training\")\n",
        "evaluate(X_val, y_val, y_val_pred, \"Validation\")\n",
        "evaluate(X_test, y_test, y_test_pred, \"Test\")\n",
        "\n",
        "\n",
        "# In[24]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    threshold = thresholds[eer_index]\n",
        "    return eer, fpr, tpr, threshold\n",
        "\n",
        "def plot_graphs_only(model, X, y, set_name, row, fig):\n",
        "    y_pred = model.predict(X)\n",
        "    y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    eer, fpr, tpr, eer_threshold = compute_eer(y, y_probs)\n",
        "    precision, recall, _ = precision_recall_curve(y, y_probs)\n",
        "\n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = fig.add_subplot(3, 3, 3*row + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1, cbar=False)\n",
        "    ax1.set_title(f\"{set_name} - Confusion Matrix\")\n",
        "    ax1.set_xlabel(\"Predicted\")\n",
        "    ax1.set_ylabel(\"Actual\")\n",
        "\n",
        "    # Plot 2: ROC Curve\n",
        "    ax2 = fig.add_subplot(3, 3, 3*row + 2)\n",
        "    ax2.plot(fpr, tpr, label=\"ROC Curve\")\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
        "    ax2.axvline(eer, color='red', linestyle='--', label=f\"EER = {eer:.4f}\")\n",
        "    ax2.set_title(f\"{set_name} - ROC Curve\")\n",
        "    ax2.set_xlabel(\"False Positive Rate\")\n",
        "    ax2.set_ylabel(\"True Positive Rate\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 3: Precision-Recall Curve\n",
        "    ax3 = fig.add_subplot(3, 3, 3*row + 3)\n",
        "    ax3.plot(recall, precision, label=\"PR Curve\")\n",
        "    ax3.set_title(f\"{set_name} - Precision-Recall Curve\")\n",
        "    ax3.set_xlabel(\"Recall\")\n",
        "    ax3.set_ylabel(\"Precision\")\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "plot_graphs_only(gb_weighted, X_train, y_train, \"Training (Oversampled)\", 0, fig)\n",
        "plot_graphs_only(gb_weighted, X_val, y_val, \"Validation\", 1, fig)\n",
        "plot_graphs_only(gb_weighted, X_test, y_test, \"Test\", 2, fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[26]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def plot_all_graphs(model, X_sets, y_sets, set_names):\n",
        "    num_sets = len(X_sets)\n",
        "    fig, axes = plt.subplots(nrows=num_sets, ncols=3, figsize=(18, 4 * num_sets))\n",
        "\n",
        "    for i, (X, y, name) in enumerate(zip(X_sets, y_sets, set_names)):\n",
        "        y_pred = model.predict(X)\n",
        "        y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "        # 1. Actual Labels\n",
        "        sns.countplot(x=y, hue=y, palette='cool', ax=axes[i, 0], legend=False)\n",
        "        axes[i, 0].set_title(f'{name} - Actual Labels')\n",
        "        axes[i, 0].set_xlabel('Class')\n",
        "        axes[i, 0].set_ylabel('Count')\n",
        "\n",
        "        # 2. Predicted Labels\n",
        "        sns.countplot(x=y_pred, hue=y_pred, palette='magma', ax=axes[i, 1], legend=False)\n",
        "        axes[i, 1].set_title(f'{name} - Predicted Labels')\n",
        "        axes[i, 1].set_xlabel('Class')\n",
        "        axes[i, 1].set_ylabel('Count')\n",
        "\n",
        "        # 3. Scatter Plot\n",
        "        sns.scatterplot(x=np.arange(len(y_probs)), y=y_probs, hue=y, palette='Set2', alpha=0.7, ax=axes[i, 2])\n",
        "        axes[i, 2].set_title(f'{name} - Probabilities vs True Labels')\n",
        "        axes[i, 2].set_xlabel('Sample Index')\n",
        "        axes[i, 2].set_ylabel('Predicted Probability')\n",
        "        axes[i, 2].legend(title='True Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_all_graphs(\n",
        "    gb_weighted,\n",
        "    [X_train, X_val, X_test],\n",
        "    [y_train, y_val, y_test],\n",
        "    [\"Training\", \"Validation\", \"Test\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AxZGENBbAdOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XG-Boost (Sailaja)"
      ],
      "metadata": {
        "id": "U2sI3pf1AqW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix, roc_curve, auc,precision_recall_curve\n",
        "\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "val_df = pd.read_csv(\"dev.csv\")\n",
        "test_df = pd.read_csv(\"eval.csv\")\n",
        "\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "X_train = train_df.iloc[:, 2:].values\n",
        "y_train = train_df[\"label\"].values\n",
        "\n",
        "X_val = val_df.iloc[:, 2:].values\n",
        "y_val = val_df[\"label\"].values\n",
        "\n",
        "X_test = test_df.iloc[:, 2:].values\n",
        "y_test = test_df[\"label\"].values\n",
        "\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix, roc_curve\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Define the model\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.1,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Fit the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 3: Predictions\n",
        "train_preds = xgb_model.predict(X_train)\n",
        "val_preds = xgb_model.predict(X_val)\n",
        "test_preds = xgb_model.predict(X_test)\n",
        "\n",
        "# Step 4: Evaluation function\n",
        "def evaluate_xgb(y_true, y_pred, X_data, dataset_name):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, xgb_model.predict_proba(X_data)[:, 1])\n",
        "    eer = fpr[np.nanargmin(np.abs((1 - tpr) - fpr))]\n",
        "\n",
        "    print(f\"{dataset_name} Results:\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "    print(f\"Equal Error Rate (EER): {eer:.4f}\\n\")\n",
        "\n",
        "# Step 5: Run evaluations\n",
        "evaluate_xgb(y_train, train_preds, X_train, \"Training\")\n",
        "evaluate_xgb(y_val, val_preds, X_val, \"Validation\")\n",
        "evaluate_xgb(y_test, test_preds, X_test, \"Test (Evaluation)\")\n",
        "\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    threshold = thresholds[eer_index]\n",
        "    return eer, fpr, tpr, threshold\n",
        "\n",
        "def plot_graphs_only(model, X, y, set_name, row, fig):\n",
        "    y_pred = model.predict(X)\n",
        "    y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    eer, fpr, tpr, eer_threshold = compute_eer(y, y_probs)\n",
        "    precision, recall, _ = precision_recall_curve(y, y_probs)\n",
        "\n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = fig.add_subplot(3, 3, 3*row + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1, cbar=False)\n",
        "    ax1.set_title(f\"{set_name} - Confusion Matrix\")\n",
        "    ax1.set_xlabel(\"Predicted\")\n",
        "    ax1.set_ylabel(\"Actual\")\n",
        "\n",
        "    # Plot 2: ROC Curve\n",
        "    ax2 = fig.add_subplot(3, 3, 3*row + 2)\n",
        "    ax2.plot(fpr, tpr, label=\"ROC Curve\")\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
        "    ax2.axvline(eer, color='red', linestyle='--', label=f\"EER = {eer:.4f}\")\n",
        "    ax2.set_title(f\"{set_name} - ROC Curve\")\n",
        "    ax2.set_xlabel(\"False Positive Rate\")\n",
        "    ax2.set_ylabel(\"True Positive Rate\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 3: Precision-Recall Curve\n",
        "    ax3 = fig.add_subplot(3, 3, 3*row + 3)\n",
        "    ax3.plot(recall, precision, label=\"PR Curve\")\n",
        "    ax3.set_title(f\"{set_name} - Precision-Recall Curve\")\n",
        "    ax3.set_xlabel(\"Recall\")\n",
        "    ax3.set_ylabel(\"Precision\")\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "plot_graphs_only(xgb_model, X_train, y_train, \"Training (Oversampled)\", 0, fig)\n",
        "plot_graphs_only(xgb_model, X_val, y_val, \"Validation\", 1, fig)\n",
        "plot_graphs_only(xgb_model, X_test, y_test, \"Test\", 2, fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[11]:\n",
        "\n",
        "\n",
        "##################UNDERSAMPLING\n",
        "print(train_df['label'].value_counts())\n",
        "class_0 = train_df[train_df['label'] == 0]\n",
        "class_1 = train_df[train_df['label'] == 1]\n",
        "class_0_undersampled = class_0.sample(n=len(class_1), random_state=42)\n",
        "train_balanced_df = pd.concat([class_0_undersampled, class_1], axis=0)\n",
        "train_balanced_df = train_balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(\"Balanced Training Set:\")\n",
        "print(train_balanced_df['label'].value_counts())\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "get_ipython().system('pip install --upgrade xgboost')\n",
        "\n",
        "\n",
        "# In[12]:\n",
        "\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix, roc_curve\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X_train_undersampling = train_balanced_df.iloc[:, 2:].values\n",
        "y_train_undersampling = train_balanced_df[\"label\"].values\n",
        "\n",
        "\n",
        "under_xg_boost = XGBClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.1,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='logloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "under_xg_boost.fit(X_train_undersampling, y_train_undersampling)\n",
        "\n",
        "\n",
        "train_preds = under_xg_boost.predict(X_train_undersampling)\n",
        "val_preds = under_xg_boost.predict(X_val)\n",
        "test_preds = under_xg_boost.predict(X_test)\n",
        "\n",
        "\n",
        "def evaluate_xgb_under(y_true, y_pred, X_data, dataset_name, model):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, model.predict_proba(X_data)[:, 1])\n",
        "    eer = fpr[np.nanargmin(np.abs((1 - tpr) - fpr))]\n",
        "\n",
        "    print(f\"{dataset_name} Results:\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "    print(f\"Equal Error Rate (EER): {eer:.4f}\\n\")\n",
        "\n",
        "\n",
        "evaluate_xgb_under(y_train_undersampling, train_preds, X_train_undersampling, \"Training (Undersampled)\", under_xg_boost)\n",
        "evaluate_xgb_under(y_val, val_preds, X_val, \"Validation\", under_xg_boost)\n",
        "evaluate_xgb_under(y_test, test_preds, X_test, \"Test\", under_xg_boost)\n",
        "\n",
        "\n",
        "# In[13]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    threshold = thresholds[eer_index]\n",
        "    return eer, fpr, tpr, threshold\n",
        "\n",
        "def plot_graphs_only(model, X, y, set_name, row, fig):\n",
        "    y_pred = model.predict(X)\n",
        "    y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    eer, fpr, tpr, eer_threshold = compute_eer(y, y_probs)\n",
        "    precision, recall, _ = precision_recall_curve(y, y_probs)\n",
        "\n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = fig.add_subplot(3, 3, 3*row + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1, cbar=False)\n",
        "    ax1.set_title(f\"{set_name} - Confusion Matrix\")\n",
        "    ax1.set_xlabel(\"Predicted\")\n",
        "    ax1.set_ylabel(\"Actual\")\n",
        "\n",
        "    # Plot 2: ROC Curve\n",
        "    ax2 = fig.add_subplot(3, 3, 3*row + 2)\n",
        "    ax2.plot(fpr, tpr, label=\"ROC Curve\")\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
        "    ax2.axvline(eer, color='red', linestyle='--', label=f\"EER = {eer:.4f}\")\n",
        "    ax2.set_title(f\"{set_name} - ROC Curve\")\n",
        "    ax2.set_xlabel(\"False Positive Rate\")\n",
        "    ax2.set_ylabel(\"True Positive Rate\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 3: Precision-Recall Curve\n",
        "    ax3 = fig.add_subplot(3, 3, 3*row + 3)\n",
        "    ax3.plot(recall, precision, label=\"PR Curve\")\n",
        "    ax3.set_title(f\"{set_name} - Precision-Recall Curve\")\n",
        "    ax3.set_xlabel(\"Recall\")\n",
        "    ax3.set_ylabel(\"Precision\")\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "plot_graphs_only(under_xg_boost, X_train_undersampling, y_train_undersampling, \"Training (Oversampled)\", 0, fig)\n",
        "plot_graphs_only(under_xg_boost, X_val, y_val, \"Validation\", 1, fig)\n",
        "plot_graphs_only(under_xg_boost, X_test, y_test, \"Test\", 2, fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[14]:\n",
        "\n",
        "\n",
        "########################SMOTING\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Convert to DataFrame (optional but useful for saving or inspecting)\n",
        "OVERtrain_balanced_df = pd.DataFrame(X_train_resampled)\n",
        "OVERtrain_balanced_df['label'] = y_train_resampled\n",
        "\n",
        "# Check new label distribution\n",
        "print(OVERtrain_balanced_df['label'].value_counts())\n",
        "\n",
        "\n",
        "# In[15]:\n",
        "\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "xgb_model_over = XGBClassifier(\n",
        "     n_estimators=50,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "xgb_model_over.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make predictions\n",
        "over_y_train_pred = xgb_model_over.predict(X_train_resampled)\n",
        "over_y_val_pred = xgb_model_over.predict(X_val)\n",
        "over_y_test_pred = xgb_model_over.predict(X_test)\n",
        "\n",
        "\n",
        "# In[16]:\n",
        "\n",
        "\n",
        "def evaluate(y_true, y_pred, X_data, dataset_name):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fpr, tpr, _ = roc_curve(y_true, xgb_model_over.predict_proba(X_data)[:, 1])\n",
        "    eer = fpr[np.nanargmin(np.absolute((1 - tpr) - fpr))]\n",
        "\n",
        "    print(f\"{dataset_name} Results:\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n {cm}\")\n",
        "    print(f\"Equal Error Rate (EER): {eer:.4f}\\n\")\n",
        "\n",
        "evaluate(y_train_resampled, over_y_train_pred, X_train_resampled, \"Training\")\n",
        "evaluate(y_val, over_y_val_pred, X_val, \"Validation\")\n",
        "evaluate(y_test, over_y_test_pred, X_test, \"Test (Evaluation)\")\n",
        "\n",
        "\n",
        "# In[18]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    threshold = thresholds[eer_index]\n",
        "    return eer, fpr, tpr, threshold\n",
        "\n",
        "def plot_graphs_only(model, X, y, set_name, row, fig):\n",
        "    y_pred = model.predict(X)\n",
        "    y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    eer, fpr, tpr, eer_threshold = compute_eer(y, y_probs)\n",
        "    precision, recall, _ = precision_recall_curve(y, y_probs)\n",
        "\n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = fig.add_subplot(3, 3, 3*row + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1, cbar=False)\n",
        "    ax1.set_title(f\"{set_name} - Confusion Matrix\")\n",
        "    ax1.set_xlabel(\"Predicted\")\n",
        "    ax1.set_ylabel(\"Actual\")\n",
        "\n",
        "    # Plot 2: ROC Curve\n",
        "    ax2 = fig.add_subplot(3, 3, 3*row + 2)\n",
        "    ax2.plot(fpr, tpr, label=\"ROC Curve\")\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
        "    ax2.axvline(eer, color='red', linestyle='--', label=f\"EER = {eer:.4f}\")\n",
        "    ax2.set_title(f\"{set_name} - ROC Curve\")\n",
        "    ax2.set_xlabel(\"False Positive Rate\")\n",
        "    ax2.set_ylabel(\"True Positive Rate\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 3: Precision-Recall Curve\n",
        "    ax3 = fig.add_subplot(3, 3, 3*row + 3)\n",
        "    ax3.plot(recall, precision, label=\"PR Curve\")\n",
        "    ax3.set_title(f\"{set_name} - Precision-Recall Curve\")\n",
        "    ax3.set_xlabel(\"Recall\")\n",
        "    ax3.set_ylabel(\"Precision\")\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "plot_graphs_only(xgb_model_over, X_train_resampled, y_train_resampled, \"Training (Oversampled)\", 0, fig)\n",
        "plot_graphs_only(xgb_model_over, X_val, y_val, \"Validation\", 1, fig)\n",
        "plot_graphs_only(xgb_model_over, X_test, y_test, \"Test\", 2, fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[19]:\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=50)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_val_pca = pca.transform(X_val_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "\n",
        "model_pca = XGBClassifier(\n",
        "     n_estimators=100,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "\n",
        "pca_train_preds= model_pca.predict(X_train_pca)\n",
        "pca_val_preds = model_pca.predict(X_val_pca)\n",
        "pca_test_preds = model_pca.predict(X_test_pca)\n",
        "\n",
        "\n",
        "# In[20]:\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score, recall_score, confusion_matrix, roc_curve\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_pca(y_true, y_pred, X_data, dataset_name):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, model_pca.predict_proba(X_data)[:, 1])\n",
        "    eer = fpr[np.nanargmin(np.absolute((1 - tpr) - fpr))]\n",
        "\n",
        "    print(f\"{dataset_name} Results:\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n {cm}\")\n",
        "    print(f\"Equal Error Rate (EER): {eer:.4f}\\n\")\n",
        "evaluate_pca(y_train, pca_train_preds, X_train_pca, \"Training\")\n",
        "evaluate_pca(y_val, pca_val_preds, X_val_pca, \"Validation\")\n",
        "evaluate_pca(y_test, pca_test_preds, X_test_pca, \"Test (Evaluation)\")\n",
        "\n",
        "\n",
        "# In[22]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    threshold = thresholds[eer_index]\n",
        "    return eer, fpr, tpr, threshold\n",
        "\n",
        "def plot_graphs_only(model, X, y, set_name, row, fig):\n",
        "    y_pred = model.predict(X)\n",
        "    y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    eer, fpr, tpr, eer_threshold = compute_eer(y, y_probs)\n",
        "    precision, recall, _ = precision_recall_curve(y, y_probs)\n",
        "\n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = fig.add_subplot(3, 3, 3*row + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1, cbar=False)\n",
        "    ax1.set_title(f\"{set_name} - Confusion Matrix\")\n",
        "    ax1.set_xlabel(\"Predicted\")\n",
        "    ax1.set_ylabel(\"Actual\")\n",
        "\n",
        "    # Plot 2: ROC Curve\n",
        "    ax2 = fig.add_subplot(3, 3, 3*row + 2)\n",
        "    ax2.plot(fpr, tpr, label=\"ROC Curve\")\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
        "    ax2.axvline(eer, color='red', linestyle='--', label=f\"EER = {eer:.4f}\")\n",
        "    ax2.set_title(f\"{set_name} - ROC Curve\")\n",
        "    ax2.set_xlabel(\"False Positive Rate\")\n",
        "    ax2.set_ylabel(\"True Positive Rate\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 3: Precision-Recall Curve\n",
        "    ax3 = fig.add_subplot(3, 3, 3*row + 3)\n",
        "    ax3.plot(recall, precision, label=\"PR Curve\")\n",
        "    ax3.set_title(f\"{set_name} - Precision-Recall Curve\")\n",
        "    ax3.set_xlabel(\"Recall\")\n",
        "    ax3.set_ylabel(\"Precision\")\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "plot_graphs_only(model_pca, X_train_pca, y_train, \"Training \", 0, fig)\n",
        "plot_graphs_only(model_pca, X_val_pca, y_val, \"Validation\", 1, fig)\n",
        "plot_graphs_only(model_pca, X_test_pca, y_test, \"Test\", 2, fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[27]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def plot_all_graphs(model, X_sets, y_sets, set_names):\n",
        "    num_sets = len(X_sets)\n",
        "    fig, axes = plt.subplots(nrows=num_sets, ncols=3, figsize=(18, 4 * num_sets))\n",
        "\n",
        "    for i, (X, y, name) in enumerate(zip(X_sets, y_sets, set_names)):\n",
        "        y_pred = model.predict(X)\n",
        "        y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "        # 1. Actual Labels\n",
        "        sns.countplot(x=y, hue=y, palette='cool', ax=axes[i, 0], legend=False)\n",
        "        axes[i, 0].set_title(f'{name} - Actual Labels')\n",
        "        axes[i, 0].set_xlabel('Class')\n",
        "        axes[i, 0].set_ylabel('Count')\n",
        "\n",
        "        # 2. Predicted Labels\n",
        "        sns.countplot(x=y_pred, hue=y_pred, palette='magma', ax=axes[i, 1], legend=False)\n",
        "        axes[i, 1].set_title(f'{name} - Predicted Labels')\n",
        "        axes[i, 1].set_xlabel('Class')\n",
        "        axes[i, 1].set_ylabel('Count')\n",
        "\n",
        "        # 3. Scatter Plot\n",
        "        sns.scatterplot(x=np.arange(len(y_probs)), y=y_probs, hue=y, palette='Set2', alpha=0.7, ax=axes[i, 2])\n",
        "        axes[i, 2].set_title(f'{name} - Probabilities vs True Labels')\n",
        "        axes[i, 2].set_xlabel('Sample Index')\n",
        "        axes[i, 2].set_ylabel('Predicted Probability')\n",
        "        axes[i, 2].legend(title='True Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example call\n",
        "plot_all_graphs(\n",
        "    xgb_model,\n",
        "    [X_train, X_val, X_test],\n",
        "    [y_train, y_val, y_test],\n",
        "    [\"Training\", \"Validation\", \"Test\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[28]:\n",
        "\n",
        "\n",
        "plot_all_graphs(\n",
        "    under_xg_boost,\n",
        "    [X_train_undersampling, X_val, X_test],\n",
        "    [y_train_undersampling, y_val, y_test],\n",
        "    [\"Training\", \"Validation\", \"Test\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[30]:\n",
        "\n",
        "\n",
        "plot_all_graphs(\n",
        "    xgb_model_over,\n",
        "    [X_train_resampled, X_val, X_test],\n",
        "    [y_train_resampled, y_val, y_test],\n",
        "    [\"Training\", \"Validation\", \"Test\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[34]:\n",
        "\n",
        "\n",
        "##########PCA\n",
        "plot_all_graphs(\n",
        "    model_pca,\n",
        "    [X_train_pca, X_val_pca, X_test_pca],\n",
        "    [y_train, y_val, y_test],\n",
        "    [\"Training\", \"Validation\", \"Test\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrices(y_tr, y_tr_pred, y_val, y_val_pred, y_test, y_test_pred):\n",
        "    titles = [\"Training Set\", \"Validation Set\", \"Test Set\"]\n",
        "    datasets = [(y_tr, y_tr_pred), (y_val, y_val_pred), (y_test, y_test_pred)]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    for ax, (y_true, y_pred), title in zip(axes, datasets, titles):\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False, ax=ax)\n",
        "        ax.set_title(f\"Confusion Matrix - {title}\")\n",
        "        ax.set_xlabel(\"Predicted\")\n",
        "        ax.set_ylabel(\"Actual\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "plot_confusion_matrices(y_train, train_preds, y_val, val_preds, y_test, test_preds)\n",
        "\n",
        "\n",
        "# In[40]:\n",
        "\n",
        "\n",
        "plot_confusion_matrices(y_train_resampled, over_y_train_pred, y_val, over_y_val_pred, y_test, over_y_test_pred)\n",
        "\n",
        "\n",
        "# In[41]:\n",
        "\n",
        "\n",
        "\n",
        "plot_confusion_matrices(y_train_undersampling, under_y_train_pred, y_val, under_y_val_pred, y_test, under_y_test_pred)\n",
        "\n",
        "\n",
        "# In[42]:\n",
        "\n",
        "\n",
        "plot_confusion_matrices(y_train, pca_train_preds, y_val, pca_val_preds, y_test, pca_test_preds)\n",
        "\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "confusion_matrices = {\n",
        "    \"Original Training\": [[22798, 2], [2391, 189]],\n",
        "    \"Original Validation\": [[22287, 9], [2532, 16]],\n",
        "    \"Original Test\": [[63882, 0], [7355, 0]],\n",
        "\n",
        "    \"Undersampling Training\": [[2343, 237], [140, 2440]],\n",
        "    \"Undersampling Validation\": [[15472, 6824], [745, 1803]],\n",
        "    \"Undersampling Test\": [[63375, 507], [7226, 129]],\n",
        "\n",
        "    \"Oversampling Training\": [[19174, 3626], [1519, 21281]],\n",
        "    \"Oversampling Validation\": [[18160, 4136], [1257, 1291]],\n",
        "    \"Oversampling Test\": [[63882, 0], [7355, 0]],\n",
        "\n",
        "    \"PCA Training\": [[22800, 0], [2563, 17]],\n",
        "    \"PCA Validation\": [[22296, 0], [2548, 0]],\n",
        "    \"PCA Test\": [[63882, 0], [7355, 0]]\n",
        "}\n",
        "\n",
        "\n",
        "# Function to calculate accuracy for label 0 and label 1\n",
        "def calculate_accuracies(conf_matrix):\n",
        "    # Label 0 accuracy = True Negatives / (True Negatives + False Positives)\n",
        "    accuracy_label_0 = conf_matrix[0][0] / (conf_matrix[0][0] + conf_matrix[0][1])\n",
        "    # Label 1 accuracy = True Positives / (True Positives + False Negatives)\n",
        "    accuracy_label_1 = conf_matrix[1][1] / (conf_matrix[1][0] + conf_matrix[1][1])\n",
        "    return accuracy_label_0, accuracy_label_1\n",
        "\n",
        "# Prepare data for plotting\n",
        "labels = list(confusion_matrices.keys())\n",
        "accuracies_label_0 = []\n",
        "accuracies_label_1 = []\n",
        "\n",
        "for label in labels:\n",
        "    conf_matrix = confusion_matrices[label]\n",
        "    acc_label_0, acc_label_1 = calculate_accuracies(conf_matrix)\n",
        "    accuracies_label_0.append(acc_label_0)\n",
        "    accuracies_label_1.append(acc_label_1)\n",
        "\n",
        "\n",
        "x = np.arange(len(labels))  # Label positions\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "width = 0.35  # Bar width\n",
        "\n",
        "\n",
        "color_label_0 = 'skyblue'\n",
        "color_label_1 = 'orange'\n",
        "\n",
        "# Plotting bars for Label 0 and Label 1\n",
        "bars_label_0 = ax.bar(x - width/2, accuracies_label_0, width, label='Accuracy for Label 0', color=color_label_0)\n",
        "bars_label_1 = ax.bar(x + width/2, accuracies_label_1, width, label='Accuracy for Label 1', color=color_label_1)\n",
        "\n",
        "# Labeling and formatting\n",
        "ax.set_xlabel('Model Configuration')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Accuracy for Label 0 and Label 1 in Different Models')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0dkBLx2MAuQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Classification (Sailaja)"
      ],
      "metadata": {
        "id": "M3g_A3vDA2wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix,recall_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "val_df = pd.read_csv(\"dev.csv\")\n",
        "test_df = pd.read_csv(\"eval.csv\")\n",
        "\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "X_train = train_df.iloc[:, 2:].values\n",
        "y_train = train_df[\"label\"].values\n",
        "\n",
        "X_val = val_df.iloc[:, 2:].values\n",
        "y_val = val_df[\"label\"].values\n",
        "\n",
        "X_test = test_df.iloc[:, 2:].values\n",
        "y_test = test_df[\"label\"].values\n",
        "\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "#######VALIDATION SET\n",
        "log_reg = LogisticRegression(max_iter=500)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_train_pred = log_reg.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "train_recall = recall_score(y_train, y_train_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred)\n",
        "y_probs_train = log_reg.predict_proba(X_train)[:, 1]\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "\n",
        "print(\"\\nTraining Results:\")\n",
        "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"F1 Score: {train_f1:.4f}\")\n",
        "print(f\"Recall Score: {train_recall:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_train, y_train_pred))\n",
        "print(f\"Equal Error Rate: {compute_eer(y_train, y_probs_train):.4f}\")\n",
        "\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "y_val_pred = log_reg.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_recall=recall_score(y_val,y_val_pred)\n",
        "val_f1 = f1_score(y_val, y_val_pred)\n",
        "y_probs = log_reg.predict_proba(X_val)[:, 1]\n",
        "\n",
        "print(\"Validation Results:\")\n",
        "print(f\"Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"F1 Score: {val_f1:.4f}\")\n",
        "print(f\"Recall Score: {val_recall:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_val, y_val_pred))\n",
        "eer = compute_eer(y_val, y_probs)\n",
        "print(f\"Equal Error Rate: {eer:.4f}\")\n",
        "\n",
        "\n",
        "# In[9]:\n",
        "\n",
        "\n",
        "###TEST SET\n",
        "y_test_pred = log_reg.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_f1 = f1_score(y_test, y_test_pred)\n",
        "test_recall=recall_score(y_test,y_test_pred)\n",
        "y_probs_test = log_reg.predict_proba(X_test)[:, 1]\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "\n",
        "print(\"\\nTest (Evaluation) Results:\")\n",
        "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"F1 Score: {test_f1:.4f}\")\n",
        "print(f\"Recall Score: {test_recall:.4f}\")\n",
        "eer_test = compute_eer(y_test, y_probs_test)\n",
        "print(f\"Equal Error Rate: {eer_test:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_test_pred))\n",
        "\n",
        "\n",
        "# In[10]:\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Confusion Matrix - Validation Set\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(confusion_matrix(y_val, y_val_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.title(\"Confusion Matrix (Validation Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred),  annot=True, fmt=\"d\", cmap=\"coolwarm\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[12]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    threshold = thresholds[eer_index]\n",
        "    return eer, fpr, tpr, threshold\n",
        "\n",
        "def plot_graphs_only(model, X, y, set_name, row, fig):\n",
        "    y_pred = model.predict(X)\n",
        "    y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    eer, fpr, tpr, eer_threshold = compute_eer(y, y_probs)\n",
        "    precision, recall, _ = precision_recall_curve(y, y_probs)\n",
        "\n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = fig.add_subplot(3, 3, 3*row + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1, cbar=False)\n",
        "    ax1.set_title(f\"{set_name} - Confusion Matrix\")\n",
        "    ax1.set_xlabel(\"Predicted\")\n",
        "    ax1.set_ylabel(\"Actual\")\n",
        "\n",
        "    # Plot 2: ROC Curve\n",
        "    ax2 = fig.add_subplot(3, 3, 3*row + 2)\n",
        "    ax2.plot(fpr, tpr, label=\"ROC Curve\")\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
        "    ax2.axvline(eer, color='red', linestyle='--', label=f\"EER = {eer:.4f}\")\n",
        "    ax2.set_title(f\"{set_name} - ROC Curve\")\n",
        "    ax2.set_xlabel(\"False Positive Rate\")\n",
        "    ax2.set_ylabel(\"True Positive Rate\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 3: Precision-Recall Curve\n",
        "    ax3 = fig.add_subplot(3, 3, 3*row + 3)\n",
        "    ax3.plot(recall, precision, label=\"PR Curve\")\n",
        "    ax3.set_title(f\"{set_name} - Precision-Recall Curve\")\n",
        "    ax3.set_xlabel(\"Recall\")\n",
        "    ax3.set_ylabel(\"Precision\")\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "plot_graphs_only(log_reg, X_train, y_train, \"Training (Oversampled)\", 0, fig)\n",
        "plot_graphs_only(log_reg, X_val, y_val, \"Validation\", 1, fig)\n",
        "plot_graphs_only(log_reg, X_test, y_test, \"Test\", 2, fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[29]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def plot_all_graphs(model, X_sets, y_sets, set_names):\n",
        "    num_sets = len(X_sets)\n",
        "    fig, axes = plt.subplots(nrows=num_sets, ncols=3, figsize=(18, 4 * num_sets))\n",
        "\n",
        "    for i, (X, y, name) in enumerate(zip(X_sets, y_sets, set_names)):\n",
        "        y_pred = model.predict(X)\n",
        "        y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "        # 1. Actual Labels\n",
        "        sns.countplot(x=y, hue=y, palette='cool', ax=axes[i, 0], legend=False)\n",
        "        axes[i, 0].set_title(f'{name} - Actual Labels')\n",
        "        axes[i, 0].set_xlabel('Class')\n",
        "        axes[i, 0].set_ylabel('Count')\n",
        "\n",
        "        # 2. Predicted Labels\n",
        "        sns.countplot(x=y_pred, hue=y_pred, palette='magma', ax=axes[i, 1], legend=False)\n",
        "        axes[i, 1].set_title(f'{name} - Predicted Labels')\n",
        "        axes[i, 1].set_xlabel('Class')\n",
        "        axes[i, 1].set_ylabel('Count')\n",
        "\n",
        "        # 3. Scatter Plot\n",
        "        sns.scatterplot(x=np.arange(len(y_probs)), y=y_probs, hue=y, palette='Set2', alpha=0.7, ax=axes[i, 2])\n",
        "        axes[i, 2].set_title(f'{name} - Probabilities vs True Labels')\n",
        "        axes[i, 2].set_xlabel('Sample Index')\n",
        "        axes[i, 2].set_ylabel('Predicted Probability')\n",
        "        axes[i, 2].legend(title='True Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example call\n",
        "plot_all_graphs(\n",
        "    log_reg,\n",
        "    [X_train, X_val, X_test],\n",
        "    [y_train, y_val, y_test],\n",
        "    [\"Training\", \"Validation\", \"Test\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[13]:\n",
        "\n",
        "\n",
        "#####UNDERSAMPLING\n",
        "print(train_df['label'].value_counts())\n",
        "class_0 = train_df[train_df['label'] == 0]\n",
        "class_1 = train_df[train_df['label'] == 1]\n",
        "class_0_undersampled = class_0.sample(n=len(class_1), random_state=42)\n",
        "train_balanced_df = pd.concat([class_0_undersampled, class_1], axis=0)\n",
        "train_balanced_df = train_balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(\"Balanced Training Set:\")\n",
        "print(train_balanced_df['label'].value_counts())\n",
        "\n",
        "\n",
        "# In[14]:\n",
        "\n",
        "\n",
        "X_train_undersampling = train_balanced_df.iloc[:, 2:].values\n",
        "y_train_undersampling = train_balanced_df[\"label\"].values\n",
        "UNDER_logistic = LogisticRegression(max_iter=500)\n",
        "UNDER_logistic.fit(X_train_undersampling, y_train_undersampling)\n",
        "\n",
        "\n",
        "# In[26]:\n",
        "\n",
        "\n",
        "# Predict on training set\n",
        "Under_y_train_pred = UNDER_logistic.predict(X_train_undersampling)\n",
        "Under_train_accuracy = accuracy_score(y_train_undersampling, Under_y_train_pred)\n",
        "Under_train_recall = recall_score(y_train_undersampling, Under_y_train_pred)\n",
        "Under_train_f1 = f1_score(y_train_undersampling, Under_y_train_pred)\n",
        "Under_y_train_probs = UNDER_logistic.predict_proba(X_train_undersampling)[:, 1]\n",
        "\n",
        "# EER function\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "\n",
        "# Print results\n",
        "print(\"Training Results after Undersampling:\")\n",
        "print(f\"Accuracy: {Under_train_accuracy:.4f}\")\n",
        "print(f\"F1 Score: {Under_train_f1:.4f}\")\n",
        "print(f\"Recall Score: {Under_train_recall:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_train_undersampling, Under_y_train_pred))\n",
        "print(f\"Equal Error Rate: {compute_eer(y_train_undersampling, Under_y_train_probs):.4f}\")\n",
        "\n",
        "\n",
        "# In[16]:\n",
        "\n",
        "\n",
        "#######VALIDATION SET AFTER UNDERSAMPLING\n",
        "UNDERy_val_pred = UNDER_logistic.predict(X_val)\n",
        "UNDERval_accuracy = accuracy_score(y_val, UNDERy_val_pred)\n",
        "UNDERval_recall=recall_score(y_val,UNDERy_val_pred)\n",
        "UNDERval_f1 = f1_score(y_val, UNDERy_val_pred)\n",
        "UNDERy_probs = UNDER_logistic.predict_proba(X_val)[:, 1]\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "print(\"Validation Results:\")\n",
        "print(f\"Accuracy: {UNDERval_accuracy :.4f}\")\n",
        "print(f\"F1 Score: {UNDERval_f1:.4f}\")\n",
        "print(f\"Recall Score: {UNDERval_recall:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_val, UNDERy_val_pred))\n",
        "UNDEReer = compute_eer(y_val, y_probs)\n",
        "print(f\"Equal Error Rate: {UNDEReer:.4f}\")\n",
        "\n",
        "\n",
        "# In[17]:\n",
        "\n",
        "\n",
        "###TEST SET AFTER UNDERSAMPLING\n",
        "UNDERy_test_pred = UNDER_logistic.predict(X_test)\n",
        "UNDERtest_accuracy = accuracy_score(y_test, UNDERy_test_pred)\n",
        "UNDERtest_f1 = f1_score(y_test, UNDERy_test_pred)\n",
        "UNDERtest_recall=recall_score(y_test,UNDERy_test_pred)\n",
        "UNDERy_probs_test = UNDER_logistic.predict_proba(X_test)[:, 1]\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "\n",
        "print(\"\\nTest (Evaluation) Results:\")\n",
        "print(f\"Accuracy: {UNDERtest_accuracy:.4f}\")\n",
        "print(f\"F1 Score: {UNDERtest_f1:.4f}\")\n",
        "print(f\"Recall Score: {UNDERtest_recall:.4f}\")\n",
        "UNDEReer_test = compute_eer(y_test, UNDERy_probs_test)\n",
        "print(f\"Equal Error Rate: {UNDEReer_test:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, UNDERy_test_pred))\n",
        "\n",
        "\n",
        "# In[18]:\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Confusion Matrix - Validation Set\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(confusion_matrix(y_val, UNDERy_val_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.title(\"Confusion Matrix (Validation Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(confusion_matrix(y_test, UNDERy_test_pred),  annot=True, fmt=\"d\", cmap=\"coolwarm\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[19]:\n",
        "\n",
        "\n",
        "####OVERSAMPLING\n",
        "\n",
        "print(\"Original Class Distribution:\")\n",
        "print(train_df['label'].value_counts())\n",
        "\n",
        "class_0 = train_df[train_df['label'] == 0]\n",
        "class_1 = train_df[train_df['label'] == 1]\n",
        "\n",
        "class_1_oversampled = class_1.sample(n=len(class_0), replace=True, random_state=42)\n",
        "\n",
        "OVERtrain_balanced_df = pd.concat([class_0, class_1_oversampled], axis=0)\n",
        "\n",
        "# Shuffle the combined dataset\n",
        "OVERtrain_balanced_df = OVERtrain_balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Show the new class distribution\n",
        "print(\"Balanced Training Set After Oversampling:\")\n",
        "print(OVERtrain_balanced_df['label'].value_counts())\n",
        "\n",
        "\n",
        "# In[20]:\n",
        "\n",
        "\n",
        "X_train_oversampling = OVERtrain_balanced_df.iloc[:, 2:].values\n",
        "y_train_oversampling = OVERtrain_balanced_df[\"label\"].values\n",
        "OVER_logistic = LogisticRegression(max_iter=500)\n",
        "OVER_logistic.fit(X_train_oversampling, y_train_oversampling)\n",
        "\n",
        "\n",
        "# In[21]:\n",
        "\n",
        "\n",
        "# Predict on training set\n",
        "over_y_train_pred = OVER_logistic.predict(X_train_oversampling)\n",
        "over_train_accuracy = accuracy_score(y_train_oversampling, over_y_train_pred)\n",
        "over_train_recall = recall_score(y_train_oversampling, over_y_train_pred)\n",
        "over_train_f1 = f1_score(y_train_oversampling, over_y_train_pred)\n",
        "over_y_train_probs = OVER_logistic.predict_proba(X_train_oversampling)[:, 1]\n",
        "\n",
        "# EER function\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "\n",
        "# Print results\n",
        "print(\"Training Results after Undersampling:\")\n",
        "print(f\"Accuracy: {over_train_accuracy:.4f}\")\n",
        "print(f\"F1 Score: {over_train_f1:.4f}\")\n",
        "print(f\"Recall Score: {over_train_recall:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_train_oversampling, over_y_train_pred))\n",
        "print(f\"Equal Error Rate: {compute_eer(y_train_oversampling, over_y_train_probs):.4f}\")\n",
        "\n",
        "\n",
        "# In[22]:\n",
        "\n",
        "\n",
        "######VALIDATION METRICS AFTER OVERSAMPLING\n",
        "OVERy_val_pred = OVER_logistic.predict(X_val)\n",
        "OVERval_accuracy = accuracy_score(y_val, OVERy_val_pred)\n",
        "OVERval_recall=recall_score(y_val,OVERy_val_pred)\n",
        "OVERval_f1 = f1_score(y_val, OVERy_val_pred)\n",
        "OVERy_probs = OVER_logistic.predict_proba(X_val)[:, 1]\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "print(\"Validation Results:\")\n",
        "print(f\"Accuracy: {OVERval_accuracy :.4f}\")\n",
        "print(f\"F1 Score: {OVERval_f1:.4f}\")\n",
        "print(f\"Recall Score: {OVERval_recall:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_val, OVERy_val_pred))\n",
        "OVEReer = compute_eer(y_val, OVERy_probs)\n",
        "print(f\"Equal Error Rate: {OVEReer:.4f}\")\n",
        "\n",
        "\n",
        "# In[23]:\n",
        "\n",
        "\n",
        "###TEST SET AFTER OVERSAMPLING\n",
        "OVERy_test_pred = OVER_logistic.predict(X_test)\n",
        "OVERtest_accuracy = accuracy_score(y_test, OVERy_test_pred)\n",
        "OVERtest_f1 = f1_score(y_test,OVERy_test_pred)\n",
        "OVERtest_recall=recall_score(y_test,OVERy_test_pred)\n",
        "OVERy_probs_test = OVER_logistic.predict_proba(X_test)[:, 1]\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    return eer\n",
        "\n",
        "print(\"\\nTest (Evaluation) Results:\")\n",
        "print(f\"Accuracy: {OVERtest_accuracy:.4f}\")\n",
        "print(f\"F1 Score: {OVERtest_f1:.4f}\")\n",
        "print(f\"Recall Score: {OVERtest_recall:.4f}\")\n",
        "OVEReer_test = compute_eer(y_test, OVERy_probs_test)\n",
        "print(f\"Equal Error Rate: {OVEReer_test:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, OVERy_test_pred))\n",
        "\n",
        "\n",
        "# In[24]:\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(confusion_matrix(y_val, OVERy_val_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.title(\"Confusion Matrix (Validation Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(confusion_matrix(y_test,OVERy_test_pred),  annot=True, fmt=\"d\", cmap=\"coolwarm\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[11]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    threshold = thresholds[eer_index]\n",
        "    return eer, fpr, tpr, threshold\n",
        "\n",
        "def plot_graphs_only(model, X, y, set_name, row, fig):\n",
        "    y_pred = model.predict(X)\n",
        "    y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    eer, fpr, tpr, eer_threshold = compute_eer(y, y_probs)\n",
        "    precision, recall, _ = precision_recall_curve(y, y_probs)\n",
        "\n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = fig.add_subplot(3, 3, 3*row + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1, cbar=False)\n",
        "    ax1.set_title(f\"{set_name} - Confusion Matrix\")\n",
        "    ax1.set_xlabel(\"Predicted\")\n",
        "    ax1.set_ylabel(\"Actual\")\n",
        "\n",
        "    # Plot 2: ROC Curve\n",
        "    ax2 = fig.add_subplot(3, 3, 3*row + 2)\n",
        "    ax2.plot(fpr, tpr, label=\"ROC Curve\")\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
        "    ax2.axvline(eer, color='red', linestyle='--', label=f\"EER = {eer:.4f}\")\n",
        "    ax2.set_title(f\"{set_name} - ROC Curve\")\n",
        "    ax2.set_xlabel(\"False Positive Rate\")\n",
        "    ax2.set_ylabel(\"True Positive Rate\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 3: Precision-Recall Curve\n",
        "    ax3 = fig.add_subplot(3, 3, 3*row + 3)\n",
        "    ax3.plot(recall, precision, label=\"PR Curve\")\n",
        "    ax3.set_title(f\"{set_name} - Precision-Recall Curve\")\n",
        "    ax3.set_xlabel(\"Recall\")\n",
        "    ax3.set_ylabel(\"Precision\")\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "plot_graphs_only(log_reg, X_train, y_train, \"Training (Oversampled)\", 0, fig)\n",
        "plot_graphs_only(log_reg, X_val, y_val, \"Validation\", 1, fig)\n",
        "plot_graphs_only(log_reg, X_test, y_test, \"Test\", 2, fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[27]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    threshold = thresholds[eer_index]\n",
        "    return eer, fpr, tpr, threshold\n",
        "\n",
        "def plot_graphs_only(model, X, y, set_name, row, fig):\n",
        "    y_pred = model.predict(X)\n",
        "    y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    eer, fpr, tpr, eer_threshold = compute_eer(y, y_probs)\n",
        "    precision, recall, _ = precision_recall_curve(y, y_probs)\n",
        "\n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = fig.add_subplot(3, 3, 3*row + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1, cbar=False)\n",
        "    ax1.set_title(f\"{set_name} - Confusion Matrix\")\n",
        "    ax1.set_xlabel(\"Predicted\")\n",
        "    ax1.set_ylabel(\"Actual\")\n",
        "\n",
        "    # Plot 2: ROC Curve\n",
        "    ax2 = fig.add_subplot(3, 3, 3*row + 2)\n",
        "    ax2.plot(fpr, tpr, label=\"ROC Curve\")\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
        "    ax2.axvline(eer, color='red', linestyle='--', label=f\"EER = {eer:.4f}\")\n",
        "    ax2.set_title(f\"{set_name} - ROC Curve\")\n",
        "    ax2.set_xlabel(\"False Positive Rate\")\n",
        "    ax2.set_ylabel(\"True Positive Rate\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 3: Precision-Recall Curve\n",
        "    ax3 = fig.add_subplot(3, 3, 3*row + 3)\n",
        "    ax3.plot(recall, precision, label=\"PR Curve\")\n",
        "    ax3.set_title(f\"{set_name} - Precision-Recall Curve\")\n",
        "    ax3.set_xlabel(\"Recall\")\n",
        "    ax3.set_ylabel(\"Precision\")\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "plot_graphs_only(UNDER_logistic, X_train_undersampling, y_train_undersampling, \"Training (Oversampled)\", 0, fig)\n",
        "plot_graphs_only(UNDER_logistic, X_val, y_val, \"Validation\", 1, fig)\n",
        "plot_graphs_only(UNDER_logistic, X_test, y_test, \"Test\", 2, fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[28]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    threshold = thresholds[eer_index]\n",
        "    return eer, fpr, tpr, threshold\n",
        "\n",
        "def plot_graphs_only(model, X, y, set_name, row, fig):\n",
        "    y_pred = model.predict(X)\n",
        "    y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    eer, fpr, tpr, eer_threshold = compute_eer(y, y_probs)\n",
        "    precision, recall, _ = precision_recall_curve(y, y_probs)\n",
        "\n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = fig.add_subplot(3, 3, 3*row + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1, cbar=False)\n",
        "    ax1.set_title(f\"{set_name} - Confusion Matrix\")\n",
        "    ax1.set_xlabel(\"Predicted\")\n",
        "    ax1.set_ylabel(\"Actual\")\n",
        "\n",
        "    # Plot 2: ROC Curve\n",
        "    ax2 = fig.add_subplot(3, 3, 3*row + 2)\n",
        "    ax2.plot(fpr, tpr, label=\"ROC Curve\")\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
        "    ax2.axvline(eer, color='red', linestyle='--', label=f\"EER = {eer:.4f}\")\n",
        "    ax2.set_title(f\"{set_name} - ROC Curve\")\n",
        "    ax2.set_xlabel(\"False Positive Rate\")\n",
        "    ax2.set_ylabel(\"True Positive Rate\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 3: Precision-Recall Curve\n",
        "    ax3 = fig.add_subplot(3, 3, 3*row + 3)\n",
        "    ax3.plot(recall, precision, label=\"PR Curve\")\n",
        "    ax3.set_title(f\"{set_name} - Precision-Recall Curve\")\n",
        "    ax3.set_xlabel(\"Recall\")\n",
        "    ax3.set_ylabel(\"Precision\")\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "plot_graphs_only(OVER_logistic, X_train_oversampling, y_train_oversampling, \"Training (Oversampled)\", 0, fig)\n",
        "plot_graphs_only(OVER_logistic, X_val, y_val, \"Validation\", 1, fig)\n",
        "plot_graphs_only(OVER_logistic, X_test, y_test, \"Test\", 2, fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[30]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def plot_all_graphs(model, X_sets, y_sets, set_names):\n",
        "    num_sets = len(X_sets)\n",
        "    fig, axes = plt.subplots(nrows=num_sets, ncols=3, figsize=(18, 4 * num_sets))\n",
        "\n",
        "    for i, (X, y, name) in enumerate(zip(X_sets, y_sets, set_names)):\n",
        "        y_pred = model.predict(X)\n",
        "        y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "        # 1. Actual Labels\n",
        "        sns.countplot(x=y, hue=y, palette='cool', ax=axes[i, 0], legend=False)\n",
        "        axes[i, 0].set_title(f'{name} - Actual Labels')\n",
        "        axes[i, 0].set_xlabel('Class')\n",
        "        axes[i, 0].set_ylabel('Count')\n",
        "\n",
        "        # 2. Predicted Labels\n",
        "        sns.countplot(x=y_pred, hue=y_pred, palette='magma', ax=axes[i, 1], legend=False)\n",
        "        axes[i, 1].set_title(f'{name} - Predicted Labels')\n",
        "        axes[i, 1].set_xlabel('Class')\n",
        "        axes[i, 1].set_ylabel('Count')\n",
        "\n",
        "        # 3. Scatter Plot\n",
        "        sns.scatterplot(x=np.arange(len(y_probs)), y=y_probs, hue=y, palette='Set2', alpha=0.7, ax=axes[i, 2])\n",
        "        axes[i, 2].set_title(f'{name} - Probabilities vs True Labels')\n",
        "        axes[i, 2].set_xlabel('Sample Index')\n",
        "        axes[i, 2].set_ylabel('Predicted Probability')\n",
        "        axes[i, 2].legend(title='True Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example call\n",
        "plot_all_graphs(\n",
        "    UNDER_logistic,\n",
        "    [X_train_undersampling, X_val, X_test],\n",
        "    [y_train_undersampling, y_val, y_test],\n",
        "    [\"Training\", \"Validation\", \"Test\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[31]:\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Confusion Matrix - Validation Set\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(confusion_matrix(y_val, UNDERy_val_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.title(\"Confusion Matrix (Validation Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(confusion_matrix(y_test, UNDERy_test_pred),  annot=True, fmt=\"d\", cmap=\"coolwarm\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[32]:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[33]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def plot_all_graphs(model, X_sets, y_sets, set_names):\n",
        "    num_sets = len(X_sets)\n",
        "    fig, axes = plt.subplots(nrows=num_sets, ncols=3, figsize=(18, 4 * num_sets))\n",
        "\n",
        "    for i, (X, y, name) in enumerate(zip(X_sets, y_sets, set_names)):\n",
        "        y_pred = model.predict(X)\n",
        "        y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "        # 1. Actual Labels\n",
        "        sns.countplot(x=y, hue=y, palette='cool', ax=axes[i, 0], legend=False)\n",
        "        axes[i, 0].set_title(f'{name} - Actual Labels')\n",
        "        axes[i, 0].set_xlabel('Class')\n",
        "        axes[i, 0].set_ylabel('Count')\n",
        "\n",
        "        # 2. Predicted Labels\n",
        "        sns.countplot(x=y_pred, hue=y_pred, palette='magma', ax=axes[i, 1], legend=False)\n",
        "        axes[i, 1].set_title(f'{name} - Predicted Labels')\n",
        "        axes[i, 1].set_xlabel('Class')\n",
        "        axes[i, 1].set_ylabel('Count')\n",
        "\n",
        "        # 3. Scatter Plot\n",
        "        sns.scatterplot(x=np.arange(len(y_probs)), y=y_probs, hue=y, palette='Set2', alpha=0.7, ax=axes[i, 2])\n",
        "        axes[i, 2].set_title(f'{name} - Probabilities vs True Labels')\n",
        "        axes[i, 2].set_xlabel('Sample Index')\n",
        "        axes[i, 2].set_ylabel('Predicted Probability')\n",
        "        axes[i, 2].legend(title='True Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example call\n",
        "plot_all_graphs(\n",
        "    OVER_logistic,\n",
        "    [X_train_oversampling, X_val, X_test],\n",
        "    [y_train_oversampling, y_val, y_test],\n",
        "    [\"Training\", \"Validation\", \"Test\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[36]:\n",
        "\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, confusion_matrix, roc_curve\n",
        "import numpy as np\n",
        "\n",
        "# sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "\n",
        "# # Train the model with sample weights\n",
        "# gb_weighted = GradientBoostingClassifier(n_estimators=30, learning_rate=0.1, max_depth=3, random_state=4)\n",
        "# gb_weighted.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train Logistic Regression with balanced class weights\n",
        "log_reg_balanced = LogisticRegression(class_weight='balanced', random_state=4, max_iter=1000)\n",
        "log_reg_balanced.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = log_reg_balanced.predict(X_train)\n",
        "y_val_pred = log_reg_balanced.predict(X_val)\n",
        "y_test_pred = log_reg_balanced.predict(X_test)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(y_true, y_pred, dataset_name):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fpr, tpr, _ = roc_curve(y_true, gb_weighted.predict_proba(X_val)[:,1])\n",
        "    eer = fpr[np.nanargmin(np.absolute((1 - tpr) - fpr))]\n",
        "\n",
        "    print(f\"{dataset_name} Results:\")\n",
        "    print(\"Accuracy:\", round(acc, 4))\n",
        "    print(\"F1 Score:\", round(f1, 4))\n",
        "    print(\"Recall:\", round(rec, 4))\n",
        "    print(\"Confusion Matrix:\\n\", cm)\n",
        "    print(\"Equal Error Rate (EER):\", round(eer, 4))\n",
        "    print()\n",
        "\n",
        "\n",
        "# In[37]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def plot_all_graphs(model, X_sets, y_sets, set_names):\n",
        "    num_sets = len(X_sets)\n",
        "    fig, axes = plt.subplots(nrows=num_sets, ncols=3, figsize=(18, 4 * num_sets))\n",
        "\n",
        "    for i, (X, y, name) in enumerate(zip(X_sets, y_sets, set_names)):\n",
        "        y_pred = model.predict(X)\n",
        "        y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "        # 1. Actual Labels\n",
        "        sns.countplot(x=y, hue=y, palette='cool', ax=axes[i, 0], legend=False)\n",
        "        axes[i, 0].set_title(f'{name} - Actual Labels')\n",
        "        axes[i, 0].set_xlabel('Class')\n",
        "        axes[i, 0].set_ylabel('Count')\n",
        "\n",
        "        # 2. Predicted Labels\n",
        "        sns.countplot(x=y_pred, hue=y_pred, palette='magma', ax=axes[i, 1], legend=False)\n",
        "        axes[i, 1].set_title(f'{name} - Predicted Labels')\n",
        "        axes[i, 1].set_xlabel('Class')\n",
        "        axes[i, 1].set_ylabel('Count')\n",
        "\n",
        "        # 3. Scatter Plot\n",
        "        sns.scatterplot(x=np.arange(len(y_probs)), y=y_probs, hue=y, palette='Set2', alpha=0.7, ax=axes[i, 2])\n",
        "        axes[i, 2].set_title(f'{name} - Probabilities vs True Labels')\n",
        "        axes[i, 2].set_xlabel('Sample Index')\n",
        "        axes[i, 2].set_ylabel('Predicted Probability')\n",
        "        axes[i, 2].legend(title='True Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_all_graphs(\n",
        "    log_reg_balanced,\n",
        "    [X_train, X_val, X_test],\n",
        "    [y_train, y_val, y_test],\n",
        "    [\"Training\", \"Validation\", \"Test\"]\n",
        ")\n",
        "\n",
        "\n",
        "# In[39]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    threshold = thresholds[eer_index]\n",
        "    return eer, fpr, tpr, threshold\n",
        "\n",
        "def plot_graphs_only(model, X, y, set_name, row, fig):\n",
        "    y_pred = model.predict(X)\n",
        "    y_probs = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    eer, fpr, tpr, eer_threshold = compute_eer(y, y_probs)\n",
        "    precision, recall, _ = precision_recall_curve(y, y_probs)\n",
        "\n",
        "    # Plot 1: Confusion Matrix\n",
        "    ax1 = fig.add_subplot(3, 3, 3*row + 1)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax1, cbar=False)\n",
        "    ax1.set_title(f\"{set_name} - Confusion Matrix\")\n",
        "    ax1.set_xlabel(\"Predicted\")\n",
        "    ax1.set_ylabel(\"Actual\")\n",
        "\n",
        "    # Plot 2: ROC Curve\n",
        "    ax2 = fig.add_subplot(3, 3, 3*row + 2)\n",
        "    ax2.plot(fpr, tpr, label=\"ROC Curve\")\n",
        "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
        "    ax2.axvline(eer, color='red', linestyle='--', label=f\"EER = {eer:.4f}\")\n",
        "    ax2.set_title(f\"{set_name} - ROC Curve\")\n",
        "    ax2.set_xlabel(\"False Positive Rate\")\n",
        "    ax2.set_ylabel(\"True Positive Rate\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    # Plot 3: Precision-Recall Curve\n",
        "    ax3 = fig.add_subplot(3, 3, 3*row + 3)\n",
        "    ax3.plot(recall, precision, label=\"PR Curve\")\n",
        "    ax3.set_title(f\"{set_name} - Precision-Recall Curve\")\n",
        "    ax3.set_xlabel(\"Recall\")\n",
        "    ax3.set_ylabel(\"Precision\")\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "plot_graphs_only(log_reg_balanced, X_train, y_train, \"Training (Oversampled)\", 0, fig)\n",
        "plot_graphs_only(log_reg_balanced, X_val, y_val, \"Validation\", 1, fig)\n",
        "plot_graphs_only(log_reg_balanced, X_test, y_test, \"Test\", 2, fig)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# In[40]:\n",
        "\n",
        "\n",
        "def evaluate_lgwt(y_true, y_pred, X_data, dataset_name):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    rec = recall_score(y_true, y_pred)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, log_reg_balanced.predict_proba(X_data)[:, 1])\n",
        "    eer = fpr[np.nanargmin(np.abs((1 - tpr) - fpr))]\n",
        "\n",
        "    print(f\"{dataset_name} Results:\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"Confusion Matrix:\\n{cm}\")\n",
        "    print(f\"Equal Error Rate (EER): {eer:.4f}\\n\")\n",
        "\n",
        "# Step 5: Run evaluations\n",
        "evaluate_lgwt(y_train, y_train_pred, X_train, \"Training\")\n",
        "evaluate_lgwt(y_val, y_val_pred, X_val, \"Validation\")\n",
        "evaluate_lgwt(y_test, y_test_pred, X_test, \"Test (Evaluation)\")\n",
        "\n",
        "\n",
        "# In[41]:\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrices(y_tr, y_tr_pred, y_val, y_val_pred, y_test, y_test_pred):\n",
        "    titles = [\"Training Set\", \"Validation Set\", \"Test Set\"]\n",
        "    datasets = [(y_tr, y_tr_pred), (y_val, y_val_pred), (y_test, y_test_pred)]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    for ax, (y_true, y_pred), title in zip(axes, datasets, titles):\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False, ax=ax)\n",
        "        ax.set_title(f\"Confusion Matrix - {title}\")\n",
        "        ax.set_xlabel(\"Predicted\")\n",
        "        ax.set_ylabel(\"Actual\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "plot_confusion_matrices(y_train, y_train_pred, y_val, y_val_pred, y_test, y_test_pred)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AsCHMVLzA7i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest (Abbas)"
      ],
      "metadata": {
        "id": "La4aUfGPE-DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest -  Random Search"
      ],
      "metadata": {
        "id": "VFvs3vJFGhm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import randint\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "U1guAmisGlx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "dev_df = pd.read_csv('dev.csv')\n",
        "eval_df = pd.read_csv('eval.csv')"
      ],
      "metadata": {
        "id": "2KFTg0XlGo2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_df.iloc[:, 2:].values\n",
        "y_train = train_df[\"label\"].values\n",
        "\n",
        "X_dev = dev_df.iloc[:, 2:].values\n",
        "y_dev = dev_df[\"label\"].values\n",
        "\n",
        "X_test = eval_df.iloc[:, 2:].values\n",
        "y_test = eval_df[\"label\"].values"
      ],
      "metadata": {
        "id": "gsRilhcfGqpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_dist = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['sqrt'],\n",
        "    'bootstrap': [True],\n",
        "    'class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,\n",
        "    verbose=1,\n",
        "    cv=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "\n",
        "random_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "K05vuwMIGsdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_rf = random_search.best_estimator_\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "y_dev_pred = best_rf.predict(X_dev)\n",
        "y_test_pred = best_rf.predict(X_test)"
      ],
      "metadata": {
        "id": "jtI8jpbpGuVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_cm(y_true, y_pred, title, cmap='viridis'):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap,\n",
        "                xticklabels=['Class 0', 'Class 1'],\n",
        "                yticklabels=['Class 0', 'Class 1'])\n",
        "    plt.title(f'Confusion Matrix ({title})')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "71ajLedLGyT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nValidation (Dev) Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_dev, y_dev_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_dev, y_dev_pred, zero_division=0))\n",
        "plot_cm(y_dev, y_dev_pred, \"Dev Set\", cmap='mako')\n",
        "\n",
        "print(\"\\nTest (Eval) Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_test_pred, zero_division=0))\n",
        "plot_cm(y_test, y_test_pred, \"Test Set\", cmap='rocket')"
      ],
      "metadata": {
        "id": "oHwArfFuGz2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Original"
      ],
      "metadata": {
        "id": "w8pUPKuSG4ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import randint\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "metadata": {
        "id": "sTRdhQCvG6n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSVs\n",
        "train_df = pd.read_csv('train.csv')\n",
        "train_df = pd.read_csv('train.csv')\n",
        "eval_df = pd.read_csv('eval.csv')"
      ],
      "metadata": {
        "id": "hKP3ier4G8MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_df.iloc[:, 2:].values\n",
        "y_train = train_df[\"label\"].values\n",
        "\n",
        "X_train = train_df.iloc[:, 2:].values\n",
        "y_train = train_df[\"label\"].values\n",
        "\n",
        "X_test = eval_df.iloc[:, 2:].values\n",
        "y_test = eval_df[\"label\"].values\n",
        "\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "O7CcKkLQG9cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(name, model, X, y_true):\n",
        "    print(f\"=== {name} Set Evaluation ===\")\n",
        "    y_pred = model.predict(X)\n",
        "    y_proba = model.predict_proba(X)[:, 1]\n",
        "\n",
        "\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Recall   :\", recall_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"F1 Score :\", f1_score(y_true, y_pred, average='weighted'))\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    print(\"Equal Error Rate (EER):\", round(eer, 4))\n",
        "    print(\"\\n\")\n",
        "\n",
        "evaluate_model(\"Train\", rf_model, X_train, y_train)\n",
        "evaluate_model(\"Dev\", rf_model, X_dev, y_dev)\n",
        "evaluate_model(\"Eval\", rf_model, X_test, y_test)"
      ],
      "metadata": {
        "id": "Nq0Iqh10HBTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest - Sampled"
      ],
      "metadata": {
        "id": "se-P5XzWHE7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import ADASYN\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import randint\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "jc7xMaMDHH5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "dev_df = pd.read_csv('dev.csv')\n",
        "eval_df = pd.read_csv('eval.csv')\n",
        "\n",
        "X_train = train_df.iloc[:, 2:].values\n",
        "y_train = train_df[\"label\"].values\n",
        "\n",
        "X_dev = dev_df.iloc[:, 2:].values\n",
        "y_dev = dev_df[\"label\"].values\n",
        "\n",
        "X_test = eval_df.iloc[:, 2:].values\n",
        "y_test = eval_df[\"label\"].values"
      ],
      "metadata": {
        "id": "ineF-LErHJi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#through randomized search method i have obtianed the best parameters and i have got the following.i am performing smote and undersampling on them.\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',\n",
        "    max_depth=None,\n",
        "    class_weight='balanced',\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_smote, y_smote)\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_idx = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
        "    return eer\n",
        "\n",
        "def evaluate(X, y_true, name):\n",
        "    y_pred = model.predict(X)\n",
        "    y_proba = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    err = 1 - acc\n",
        "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    eer = compute_eer(y_true, y_proba)\n",
        "\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(f\"Accuracy:           {acc:.4f}\")\n",
        "    print(f\"Error Rate:         {err:.4f}\")\n",
        "    print(f\"Weighted Precision: {precision:.4f}\")\n",
        "    print(f\"Weighted Recall:    {recall:.4f}\")\n",
        "    print(f\"Weighted F1 Score:  {f1:.4f}\")\n",
        "    print(f\"Equal Error Rate:   {eer:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "evaluate(X_smote, y_smote, \"Train\")\n",
        "evaluate(X_dev, y_dev, \"Dev\")\n",
        "evaluate(X_test, y_test, \"Test\")\n"
      ],
      "metadata": {
        "id": "YiVIwFrMHPjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def f1_per_class(X, y_true, name):\n",
        "    y_pred = model.predict(X)\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "\n",
        "    f1_0 = report['0']['f1-score']\n",
        "    f1_1 = report['1']['f1-score']\n",
        "\n",
        "    print(f\"\\n{name} Set F1 Scores:\")\n",
        "    print(f\"F1 Score (Label 0): {f1_0:.4f}\")\n",
        "    print(f\"F1 Score (Label 1): {f1_1:.4f}\")\n",
        "\n",
        "f1_per_class(X_smote, y_smote, \"Train\")\n",
        "f1_per_class(X_dev, y_dev, \"Dev\")\n",
        "f1_per_class(X_test, y_test, \"Test\")\n"
      ],
      "metadata": {
        "id": "5mPtPhFJHR0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_rus, y_rus = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',\n",
        "    max_depth=None,\n",
        "    class_weight='balanced',\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_rus, y_rus)\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_idx = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
        "    return eer\n",
        "\n",
        "def evaluate(X, y_true, name):\n",
        "    y_pred = model.predict(X)\n",
        "    y_proba = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    err = 1 - acc\n",
        "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    eer = compute_eer(y_true, y_proba)\n",
        "\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(f\"Accuracy:           {acc:.4f}\")\n",
        "    print(f\"Error Rate:         {err:.4f}\")\n",
        "    print(f\"Weighted Precision: {precision:.4f}\")\n",
        "    print(f\"Weighted Recall:    {recall:.4f}\")\n",
        "    print(f\"Weighted F1 Score:  {f1:.4f}\")\n",
        "    print(f\"Equal Error Rate:   {eer:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "evaluate(X_rus, y_rus, \"Train\")\n",
        "evaluate(X_dev, y_dev, \"Dev\")\n",
        "evaluate(X_test, y_test, \"Test\")\n"
      ],
      "metadata": {
        "id": "qpreQmYtHTnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def f1_per_class(X, y_true, name):\n",
        "    y_pred = model.predict(X)\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "\n",
        "    f1_0 = report['0']['f1-score']\n",
        "    f1_1 = report['1']['f1-score']\n",
        "\n",
        "    print(f\"\\n{name} Set F1 Scores:\")\n",
        "    print(f\"F1 Score (Label 0): {f1_0:.4f}\")\n",
        "    print(f\"F1 Score (Label 1): {f1_1:.4f}\")\n",
        "\n",
        "f1_per_class(X_rus, y_rus, \"Train\")\n",
        "f1_per_class(X_dev, y_dev, \"Dev\")\n",
        "f1_per_class(X_test, y_test, \"Test\")\n"
      ],
      "metadata": {
        "id": "kyRflpepHUeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_dev_scaled = scaler.transform(X_dev)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "pca = PCA(n_components=100, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_dev_pca = pca.transform(X_dev_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',\n",
        "    max_depth=None,\n",
        "    class_weight='balanced',\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train_pca, y_train)\n",
        "\n",
        "def compute_eer(y_true, y_scores):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    fnr = 1 - tpr\n",
        "    eer_idx = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
        "    return eer\n",
        "\n",
        "def evaluate(X, y_true, name):\n",
        "    y_pred = model.predict(X)\n",
        "    y_proba = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    err = 1 - acc\n",
        "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    eer = compute_eer(y_true, y_proba)\n",
        "\n",
        "    print(f\"\\n{name} Results (with PCA):\")\n",
        "    print(f\"Accuracy:           {acc:.4f}\")\n",
        "    print(f\"Error Rate:         {err:.4f}\")\n",
        "    print(f\"Weighted Precision: {precision:.4f}\")\n",
        "    print(f\"Weighted Recall:    {recall:.4f}\")\n",
        "    print(f\"Weighted F1 Score:  {f1:.4f}\")\n",
        "    print(f\"Equal Error Rate:   {eer:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "evaluate(X_train_pca, y_train, \"Train\")\n",
        "evaluate(X_dev_pca, y_dev, \"Dev\")\n",
        "evaluate(X_test_pca, y_test, \"Test\")\n"
      ],
      "metadata": {
        "id": "WrMpOxaLHWI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def f1_per_class(X, y_true, name):\n",
        "    y_pred = model.predict(X)\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "\n",
        "    f1_0 = report['0']['f1-score']\n",
        "    f1_1 = report['1']['f1-score']\n",
        "\n",
        "    print(f\"\\n{name} Set F1 Scores:\")\n",
        "    print(f\"F1 Score (Label 0): {f1_0:.4f}\")\n",
        "    print(f\"F1 Score (Label 1): {f1_1:.4f}\")\n",
        "\n",
        "f1_per_class(X_train_pca, y_train, \"Train\")\n",
        "f1_per_class(X_dev_pca, y_dev, \"Dev\")\n",
        "f1_per_class(X_test_pca, y_test, \"Test\")\n"
      ],
      "metadata": {
        "id": "10q9cqriHXlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes (Abbas)"
      ],
      "metadata": {
        "id": "2vhtlM4AFSWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_curve\n",
        "import numpy as np\n",
        "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler"
      ],
      "metadata": {
        "id": "SxYS7oc0GGr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "dev_df = pd.read_csv('dev.csv')\n",
        "eval_df = pd.read_csv('eval.csv')\n",
        "\n",
        "X_train = train_df.iloc[:, 2:].values\n",
        "y_train = train_df[\"label\"].values\n",
        "X_dev = dev_df.iloc[:, 2:].values\n",
        "y_dev = dev_df[\"label\"].values\n",
        "X_test = eval_df.iloc[:, 2:].values\n",
        "y_test = eval_df[\"label\"].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_dev_scaled = scaler.transform(X_dev)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#training\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "kXlboRE6GJvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(name, model, X, y_true):\n",
        "    print(f\"=== {name} Set Evaluation ===\")\n",
        "    y_pred = model.predict(X)\n",
        "    y_proba = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Recall   :\", recall_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"F1 Score :\", f1_score(y_true, y_pred, average='weighted'))\n",
        "\n",
        "    # EER\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    print(\"Equal Error Rate (EER):\", round(eer, 4))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Evaluations\n",
        "evaluate_model(\"Train\", nb_model, X_train_scaled, y_train)\n",
        "evaluate_model(\"Dev\", nb_model, X_dev_scaled, y_dev)\n",
        "evaluate_model(\"Eval\", nb_model, X_test_scaled, y_test)"
      ],
      "metadata": {
        "id": "TDJZd2P5GO1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_dev_pred = nb_model.predict(X_dev_scaled)\n",
        "y_test_pred = nb_model.predict(X_test_scaled)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_dev, y_dev_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False)\n",
        "plt.title(\"Confusion Matrix (Validation Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False)\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_3iDptj7GRYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=100)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_dev_pca = pca.transform(X_dev_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Training on pca data\n",
        "nb_model_1 = GaussianNB()\n",
        "nb_model_1.fit(X_train_pca, y_train)"
      ],
      "metadata": {
        "id": "QbgKbqcSGTO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(name, model, X, y_true):\n",
        "    print(f\"=== {name} Set Evaluation ===\")\n",
        "    y_pred = model.predict(X)\n",
        "    y_proba = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Recall   :\", recall_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"F1 Score :\", f1_score(y_true, y_pred, average='weighted'))\n",
        "\n",
        "    # EER\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    print(\"Equal Error Rate (EER):\", round(eer, 4))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Evaluations\n",
        "evaluate_model(\"Train\", nb_model_1, X_train_pca, y_train)\n",
        "evaluate_model(\"Dev\", nb_model_1, X_dev_pca, y_dev)\n",
        "evaluate_model(\"Eval\", nb_model_1, X_test_pca, y_test)"
      ],
      "metadata": {
        "id": "_R5MPpm6GUv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_dev_pred = nb_model_1.predict(X_dev_pca)\n",
        "y_test_pred = nb_model_1.predict(X_test_pca)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_dev, y_dev_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False)\n",
        "plt.title(\"Confusion Matrix (Validation Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False)\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bK0g0DUUGWf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "#Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
        "X_dev_scaled = scaler.transform(X_dev)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#PCA\n",
        "pca = PCA(n_components=100)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_dev_pca = pca.transform(X_dev_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "nb_model_2 = GaussianNB()\n",
        "nb_model_2.fit(X_train_pca, y_train_balanced)\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, accuracy_score,\n",
        "    precision_score, recall_score, f1_score, roc_curve\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(name, model, X, y_true):\n",
        "    print(f\"=== {name} Set Evaluation ===\")\n",
        "    y_pred = model.predict(X)\n",
        "    y_proba = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    # Metrics\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Recall   :\", recall_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"F1 Score :\", f1_score(y_true, y_pred, average='weighted'))\n",
        "\n",
        "    # EER\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fnr - fpr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    print(\"Equal Error Rate (EER):\", round(eer, 4))\n",
        "    print(\"\\n\")\n",
        "\n",
        "#Evaluations\n",
        "evaluate_model(\"Train\", nb_model_2, X_train_pca, y_train_balanced)\n",
        "evaluate_model(\"Dev\", nb_model_2, X_dev_pca, y_dev)\n",
        "evaluate_model(\"Eval\", nb_model_2, X_test_pca, y_test)"
      ],
      "metadata": {
        "id": "WSdym0o5GYGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_dev_pred = nb_model_2.predict(X_dev_pca)\n",
        "y_test_pred = nb_model_2.predict(X_test_pca)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_dev, y_dev_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False)\n",
        "plt.title(\"Confusion Matrix (Validation Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False)\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0xeWNXqXGZ-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying Random Undersampling\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "X_train_balanced, y_train_balanced = undersampler.fit_resample(X_train, y_train)\n",
        "\n",
        "#Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
        "X_dev_scaled = scaler.transform(X_dev)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=100)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_dev_pca = pca.transform(X_dev_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "\n",
        "nb_model_3 = GaussianNB()\n",
        "nb_model_3.fit(X_train_pca, y_train_balanced)\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, accuracy_score,\n",
        "    precision_score, recall_score, f1_score, roc_curve\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "#Evaluation\n",
        "def evaluate_model(name, model, X, y_true):\n",
        "    print(f\"\\n=== {name} Set Evaluation ===\")\n",
        "\n",
        "    y_pred = model.predict(X)\n",
        "    y_proba = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Recall   :\", recall_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"F1 Score :\", f1_score(y_true, y_pred, average='weighted'))\n",
        "\n",
        "    #EER\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
        "    fnr = 1 - tpr\n",
        "    eer_index = np.nanargmin(np.abs(fpr - fnr))\n",
        "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    print(\"Equal Error Rate (EER):\", round(eer, 4))\n",
        "\n",
        "#Evaluation\n",
        "evaluate_model(\"Train\", nb_model_3, X_train_pca, y_train_balanced)\n",
        "evaluate_model(\"Dev\", nb_model_3, X_dev_pca, y_dev)\n",
        "evaluate_model(\"Eval\", nb_model_3, X_test_pca, y_test)\n"
      ],
      "metadata": {
        "id": "lvw7GuiwGbpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_dev_pred = nb_model_3.predict(X_dev_pca)\n",
        "y_test_pred = nb_model_3.predict(X_test_pca)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_dev, y_dev_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False)\n",
        "plt.title(\"Confusion Matrix (Validation Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False)\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CPDkBvBIGdeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree (Abbas)"
      ],
      "metadata": {
        "id": "GtIaF1u6FW8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, accuracy_score,precision_score, recall_score, f1_score, roc_curve)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler"
      ],
      "metadata": {
        "id": "QZBnMYj7Fflk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "dev_df = pd.read_csv('dev.csv')\n",
        "eval_df = pd.read_csv('eval.csv')\n",
        "\n",
        "X_train = train_df.iloc[:, 2:].values\n",
        "y_train = train_df[\"label\"].values\n",
        "X_dev = dev_df.iloc[:, 2:].values\n",
        "y_dev = dev_df[\"label\"].values\n",
        "X_test = eval_df.iloc[:, 2:].values\n",
        "y_test = eval_df[\"label\"].values"
      ],
      "metadata": {
        "id": "-9hIvcXaBT4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_dev_scaled = scaler.transform(X_dev)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf = DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(name, model, X, y_true):\n",
        "    print(f\"\\n=== {name} Set Evaluation ===\")\n",
        "\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # EER\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_proba = model.predict_proba(X)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "        fnr = 1 - tpr\n",
        "        eer_index = np.nanargmin(np.abs(fpr - fnr))\n",
        "        eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    else:\n",
        "        eer = None\n",
        "\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Recall   :\", recall_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"F1 Score :\", f1_score(y_true, y_pred, average='weighted'))\n",
        "\n",
        "    if eer is not None:\n",
        "        print(\"Equal Error Rate (EER):\", round(eer, 4))\n",
        "    else:\n",
        "        print(\"Equal Error Rate (EER): Not available\")\n",
        "\n",
        "evaluate_model(\"Train\", clf, X_train_scaled, y_train)\n",
        "evaluate_model(\"Dev\", clf, X_dev_scaled, y_dev)\n",
        "evaluate_model(\"Eval\", clf, X_test_scaled, y_test)"
      ],
      "metadata": {
        "id": "dq1dvT2xFlKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_dev_scaled = scaler.transform(X_dev)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf = DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(name, model, X, y_true):\n",
        "    print(f\"\\n=== {name} Set Evaluation ===\")\n",
        "\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # EER\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_proba = model.predict_proba(X)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "        fnr = 1 - tpr\n",
        "        eer_index = np.nanargmin(np.abs(fpr - fnr))\n",
        "        eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    else:\n",
        "        eer = None\n",
        "\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
        "    print(\"Recall   :\", recall_score(y_true, y_pred))\n",
        "    print(\"F1 Score :\", f1_score(y_true, y_pred))\n",
        "\n",
        "    if eer is not None:\n",
        "        print(\"Equal Error Rate (EER):\", round(eer, 4))\n",
        "    else:\n",
        "        print(\"Equal Error Rate (EER): Not available\")\n",
        "\n",
        "evaluate_model(\"Train\", clf, X_train_scaled, y_train)\n",
        "evaluate_model(\"Dev\", clf, X_dev_scaled, y_dev)\n",
        "evaluate_model(\"Eval\", clf, X_test_scaled, y_test)"
      ],
      "metadata": {
        "id": "1X_qUbk4Fm5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_dev_pred = clf.predict(X_dev_scaled)\n",
        "y_test_pred = clf.predict(X_test_scaled)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_dev, y_dev_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False)\n",
        "plt.title(\"Confusion Matrix (Validation Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False)\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8I98_QFRFokU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA\n",
        "pca = PCA(n_components=100)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_dev_pca = pca.transform(X_dev_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "clf_1 = DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "clf_1.fit(X_train_pca, y_train)\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, accuracy_score,\n",
        "    precision_score, recall_score, f1_score, roc_curve\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "# === Evaluation Function ===\n",
        "def evaluate_model(name, model, X, y_true):\n",
        "    print(f\"\\n=== {name} Set Evaluation ===\")\n",
        "\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Predict probabilities only if supported\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_proba = model.predict_proba(X)[:, 1]  # Binary class 1 prob\n",
        "        fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
        "        fnr = 1 - tpr\n",
        "        eer_index = np.nanargmin(np.abs(fpr - fnr))\n",
        "        eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    else:\n",
        "        eer = None\n",
        "\n",
        "    # Print Metrics\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Recall   :\", recall_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"F1 Score :\", f1_score(y_true, y_pred, average='weighted'))\n",
        "\n",
        "    if eer is not None:\n",
        "        print(\"Equal Error Rate (EER):\", round(eer, 4))\n",
        "    else:\n",
        "        print(\"Equal Error Rate (EER): Not available\")\n",
        "\n",
        "# === Run for Train, Dev, Eval ===\n",
        "evaluate_model(\"Train\", clf_1, X_train_pca, y_train)\n",
        "evaluate_model(\"Dev\", clf_1, X_dev_pca, y_dev)\n",
        "evaluate_model(\"Eval\", clf_1, X_test_pca, y_test)\n"
      ],
      "metadata": {
        "id": "9_Lpv-sEFpSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA\n",
        "pca = PCA(n_components=100)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_dev_pca = pca.transform(X_dev_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "clf_1 = DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "clf_1.fit(X_train_pca, y_train)\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, accuracy_score,\n",
        "    precision_score, recall_score, f1_score, roc_curve\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "# === Evaluation Function ===\n",
        "def evaluate_model(name, model, X, y_true):\n",
        "    print(f\"\\n=== {name} Set Evaluation ===\")\n",
        "\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Predict probabilities only if supported\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_proba = model.predict_proba(X)[:, 1]  # Binary class 1 prob\n",
        "        fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
        "        fnr = 1 - tpr\n",
        "        eer_index = np.nanargmin(np.abs(fpr - fnr))\n",
        "        eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "    else:\n",
        "        eer = None\n",
        "\n",
        "    # Print Metrics\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
        "    print(\"Recall   :\", recall_score(y_true, y_pred))\n",
        "    print(\"F1 Score :\", f1_score(y_true, y_pred))\n",
        "\n",
        "    if eer is not None:\n",
        "        print(\"Equal Error Rate (EER):\", round(eer, 4))\n",
        "    else:\n",
        "        print(\"Equal Error Rate (EER): Not available\")\n",
        "\n",
        "# === Run for Train, Dev, Eval ===\n",
        "evaluate_model(\"Train\", clf_1, X_train_pca, y_train)\n",
        "evaluate_model(\"Dev\", clf_1, X_dev_pca, y_dev)\n",
        "evaluate_model(\"Eval\", clf_1, X_test_pca, y_test)\n"
      ],
      "metadata": {
        "id": "8tu6IoY6FsRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_dev_pred = clf_1.predict(X_dev_pca)\n",
        "y_test_pred = clf_1.predict(X_test_pca)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_dev, y_dev_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False)\n",
        "plt.title(\"Confusion Matrix (Validation Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False)\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sHRE0doRF2lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#smote & PCA100\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "pca = PCA(n_components=100)\n",
        "X_train_resampled_scaled = scaler.fit_transform(X_train_resampled)\n",
        "X_train_resampled_pca = pca.fit_transform(X_train_resampled_scaled)\n",
        "X_dev_pca = pca.transform(X_dev_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "clf_smote = DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "clf_smote.fit(X_train_resampled_pca, y_train_resampled)\n",
        "def evaluate_model(name, model, X, y_true):\n",
        "    print(f\"\\n=== {name} Set Evaluation ===\")\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # EER Calculation (binary only)\n",
        "    eer = None\n",
        "    if hasattr(model, \"predict_proba\") and len(np.unique(y_true)) == 2:\n",
        "        y_scores = model.predict_proba(X)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "        fnr = 1 - tpr\n",
        "        eer_index = np.nanargmin(np.abs(fpr - fnr))\n",
        "        eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "\n",
        "    # Metrics Output\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Recall   :\", recall_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"F1 Score :\", f1_score(y_true, y_pred, average='weighted'))\n",
        "    print(\"Equal Error Rate (EER):\", f\"{eer:.4f}\" if eer is not None else \"Not available\")\n",
        "\n",
        "# === 6. Run Evaluation on Train, Dev, and Test Sets ===\n",
        "evaluate_model(\"Train\", clf_smote, X_train_resampled_pca, y_train_resampled)\n",
        "evaluate_model(\"Dev\", clf_smote, X_dev_pca, y_dev)\n",
        "evaluate_model(\"Eval\", clf_smote, X_test_pca, y_test)"
      ],
      "metadata": {
        "id": "9ciBtAulF4dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_dev_pred = clf_smote.predict(X_dev_pca)\n",
        "y_test_pred = clf_smote.predict(X_test_pca)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_dev, y_dev_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False)\n",
        "plt.title(\"Confusion Matrix (Validation Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False)\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lehJ7xr9F6pG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#smote & PCA100 &metrics not weighted\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "pca = PCA(n_components=100)\n",
        "X_train_resampled_scaled = scaler.fit_transform(X_train_resampled)\n",
        "X_train_resampled_pca = pca.fit_transform(X_train_resampled_scaled)\n",
        "X_dev_pca = pca.transform(X_dev_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "clf_smote = DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "clf_smote.fit(X_train_resampled_pca, y_train_resampled)\n",
        "def evaluate_model(name, model, X, y_true):\n",
        "    print(f\"\\n=== {name} Set Evaluation ===\")\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # EER\n",
        "    eer = None\n",
        "    if hasattr(model, \"predict_proba\") and len(np.unique(y_true)) == 2:\n",
        "        y_scores = model.predict_proba(X)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "        fnr = 1 - tpr\n",
        "        eer_index = np.nanargmin(np.abs(fpr - fnr))\n",
        "        eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
        "    print(\"Recall   :\", recall_score(y_true, y_pred))\n",
        "    print(\"F1 Score :\", f1_score(y_true, y_pred))\n",
        "    print(\"Equal Error Rate (EER):\", f\"{eer:.4f}\" if eer is not None else \"Not available\")\n",
        "\n",
        "evaluate_model(\"Train\", clf_smote, X_train_resampled_pca, y_train_resampled)\n",
        "evaluate_model(\"Dev\", clf_smote, X_dev_pca, y_dev)\n",
        "evaluate_model(\"Eval\", clf_smote, X_test_pca, y_test)"
      ],
      "metadata": {
        "id": "-IhuqBU2F9oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Undersampling before PCA\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n",
        "\n",
        "X_train_under_scaled = scaler.fit_transform(X_train_under)\n",
        "X_train_under_pca = pca.fit_transform(X_train_under_scaled)\n",
        "X_dev_pca = pca.transform(X_dev_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "clf_under = DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "clf_under.fit(X_train_under_pca, y_train_under)\n",
        "\n",
        "def evaluate_model(name, model, X, y_true):\n",
        "    print(f\"\\n=== {name} Set Evaluation ===\")\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # EER\n",
        "    eer = None\n",
        "    if hasattr(model, \"predict_proba\") and len(np.unique(y_true)) == 2:\n",
        "        y_scores = model.predict_proba(X)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "        fnr = 1 - tpr\n",
        "        eer_index = np.nanargmin(np.abs(fpr - fnr))\n",
        "        eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
        "    print(\"Recall   :\", recall_score(y_true, y_pred))\n",
        "    print(\"F1 Score :\", f1_score(y_true, y_pred))\n",
        "    print(\"Equal Error Rate (EER):\", f\"{eer:.4f}\" if eer is not None else \"Not available\")\n",
        "\n",
        "evaluate_model(\"Train\", clf_under, X_train_under_pca, y_train_under)\n",
        "evaluate_model(\"Dev\", clf_under, X_dev_pca, y_dev)\n",
        "evaluate_model(\"Eval\", clf_under, X_test_pca, y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "cIsWM93WF-PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Undersampling before PCA\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n",
        "\n",
        "X_train_under_scaled = scaler.fit_transform(X_train_under)\n",
        "X_train_under_pca = pca.fit_transform(X_train_under_scaled)\n",
        "X_dev_pca = pca.transform(X_dev_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "clf_under = DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    class_weight='balanced',\n",
        "    random_state=42\n",
        ")\n",
        "clf_under.fit(X_train_under_pca, y_train_under)\n",
        "\n",
        "def evaluate_model(name, model, X, y_true):\n",
        "    print(f\"\\n=== {name} Set Evaluation ===\")\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # EER\n",
        "    eer = None\n",
        "    if hasattr(model, \"predict_proba\") and len(np.unique(y_true)) == 2:\n",
        "        y_scores = model.predict_proba(X)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "        fnr = 1 - tpr\n",
        "        eer_index = np.nanargmin(np.abs(fpr - fnr))\n",
        "        eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
        "\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred,average='weighted'))\n",
        "    print(\"Recall   :\", recall_score(y_true, y_pred,average='weighted'))\n",
        "    print(\"F1 Score :\", f1_score(y_true, y_pred,average='weighted'))\n",
        "    print(\"Equal Error Rate (EER):\", f\"{eer:.4f}\" if eer is not None else \"Not available\")\n",
        "\n",
        "evaluate_model(\"Train\", clf_under, X_train_under_pca, y_train_under)\n",
        "evaluate_model(\"Dev\", clf_under, X_dev_pca, y_dev)\n",
        "evaluate_model(\"Eval\", clf_under, X_test_pca, y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "gJPDTHUrGAmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_dev_pred = clf_under.predict(X_dev_pca)\n",
        "y_test_pred = clf_under.predict(X_test_pca)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_dev, y_dev_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False)\n",
        "plt.title(\"Confusion Matrix (Validation Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=False)\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VQM1OHsjGD1U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}